[["index.html", "Prefacio", " Prefacio Esta es el sitio web para la primera edición de Bioestadística, publicado en Julio de 2023. Este libro es el resultado de la preparación del material de los cursos de Bioestadística I y Bioestadística II dados en la Universidad de Carabobo, y los cuales tuve la suerte de dictar durante el primer periodo académico de la universidad en 2023. En este libro tú aprenderas "],["introducción..html", "1 Introducción.", " 1 Introducción. "],["teoría-de-probabilidades..html", "2 Teoría de Probabilidades.", " 2 Teoría de Probabilidades. "],["combinatoria..html", "3 Combinatoria.", " 3 Combinatoria. "],["otros-teoremas-de-probabilidades..html", "4 Otros Teoremas de Probabilidades.", " 4 Otros Teoremas de Probabilidades. "],["estadística-descriptiva..html", "5 Estadística Descriptiva.", " 5 Estadística Descriptiva. Un conjunto de datos se puede describir usando: Medidas de tendencia central: estas son valores que describen el centro alrededor del cual el conjunto de observaciones se distribuye. De esta forma, nos permite describir donde se localizan la mayoría de las observaciones. Son tres: Media: describe la localización media de las observaciones. Mediana: es el valor que distribuye las observaciones de tal forma que 50% de estas quedan por encima de ella, y el otro 50% por debajo. Moda: describe la posición de la (o las) observación (observaciones) más frecuente(s). Medidas de posición: vienen definidos por los cuantiles de una distribución. Un cuantil \\(C_p\\) se define como el valor que deja por debajo de si \\(p\\times100\\)% de las observaciones. Por ejemplo, el cuantil \\(C_{0{,}3}\\) es aquel valor que deja por debajo de si \\(30\\)% de las observaciones. Nos ayuda a describir la posición que una observación ocupa dentro del dominio sobre el cual se distribuyen los datos. Algunos ejemplos son: Los cuartiles: (\\(C_{0{,}25i}\\) con \\(i = 1,2,3\\)) que distribuyen las observaciones en \\(4\\) partes, Los deciles (\\(C_{0{,}1i}\\) con \\(i = 1,2,\\ldots,10\\)), que dividen la distribución en \\(10\\) partes, y Los percentiles (\\(C_{0{,}01i}\\) con \\(i = 1,2,\\ldots, 99, 100\\)), que dividen la distribución en \\(100\\) partes. Medidas de dispersión: estas son medidas de que tan variables son las observaciones. Sirven para describir la dispersión de las observaciones en su dominio y alrededor de su centro. Pueden ser: Rango o Recorrido: es la diferencia entre el valor máximo y el valor mínimo observado. Nos dice que tan amplio es el dominio ocupado por las observaciones, o que tan amplio es el intervalo sobre el cual se distribuyen todas las observaciones. Rango intercuartílico: es la diferencia entre el tercer cuartil y primer cuartil, y por lo tanto, describe la amplitud del intervalo que contiene a un \\(50\\)% de las observaciones. Varianza: es una medida de las diferencias cuadráticas promedio de las observaciones con respecto a la media (sumatoria de cuadrados promedio). Sirve como una medida de cuán variable es un conjunto de datos, dado que a mayor son las desviaciones de la media, más grande es la varianza. Desviación estándar: es una medida de la distancia promedio de las observaciones con respeto a la media. Al igual que antes para la varianza, la desviación estándar sirve como medida de variabilidad con respecto al centro, dado que a mayor la distancia de las observaciones a la media, mayor será la desviación estándar. Coeficiente de variación: es el valor proporcional de la desviación estándar con respecto a la media (Desviación estándar / Media). Esta sirve como medida de dispersión relativa, dado que permite comparar distribuciones basado en cuán distantes, en promedio, de la media están las observaciones, basados en el tamaño relativo de esta con respecto a la media. Medidas de forma: estas nos ayudan a describir la simetría y ensanchamiento de la distribución de las observaciones. Estas son: Asimetría: es un coeficiente cuyo valor nos permite decir si las observaciones se encuentran acumuladas a la derecha, o la izquierda de la distribución (a esto se le llama sesgo). Curtosis: esta describe que tan amplio es el pico de la distribución de observaciones, permitiéndonos decir si se trata de una colina amplía o de un pico estrecho. "],["datos-no-agrupados..html", "5.1 Datos no agrupados.", " 5.1 Datos no agrupados. Para comprender los conceptos de la estadística descriptiva en datos sin agrupar, usamos el conjunto de datos obtenidos del experimento que busca evaluar la reacción de saltamontes a estímulos visuales o acústicos, que midieron el tiempo de reacción a estos antes del vuelo: A estímulos acústicos: \\(86, 102, 103, 99, 108, 100, 118, 108, 109, 113, 114, 107, 107, 117, 120, 101, 126, 109, 106\\). A estímulos visuales: \\(72, 95, 73, 99, 71, 90, 102, 97, 71, 75, 80, 70, 100, 104, 81, 103, 101, 103, 77, 78, 89\\). 5.1.1 Medidas de Tendencia central. El promedio se define como: \\[\\bar{X} = \\frac{\\sum_{i=1}^k f_ix_i}{n}\\] dónde \\(x_i\\) es la \\(i\\)-ésima observación, y \\(f_i\\) es la frecuencia absoluta de la observación (cuantas veces se repite \\(x_i\\) entre el número de observaciones), y la sumatoria se hace sobre los \\(k\\) observaciones únicas (las observaciones repetidas no se toman en cuenta, ya se están tomando en cuenta al multiplicar por \\(f_i\\) el valor observado repetido). Para el tiempo de reacción a estímulos acústicos se calcularía entonces: \\[\\begin{aligned} \\bar{X}_a &amp;= \\frac{86+102+103+99+2\\times108+100+118+109+113+114+2\\times107+117+120+101+126+109+106}{19} \\\\ &amp;= 108{,}05 \\text{ segundos} \\end{aligned}\\] Procediendo de igual forma para el tiempo de reacción a estímulos visuales se obtiene \\(\\bar{X}_v=87{,}19\\) segundos (¡verifícalo!). La mediana para datos agrupados se consigue siguiendo los siguientes pasos: Se ordenan de menor a mayor las observaciones. Se asignan índices a los datos ordenados desde \\(1\\) a \\(n\\): al menor dato se le asigna el índice \\(1\\), al siguiente el \\(2\\), y así sucesivamente. Se calcula el índice de posición de la mediana como \\((n + 1)/2\\) si \\(n\\) es impar. Si \\(n\\) es par se calculan los índices de posición \\((n-1)/2\\) y \\((n+1)/2\\). La mediana es entonces: \\[M = \\begin{cases} x_{(n + 1)/2}, &amp; \\text{ si }n\\text{ es impar}. \\\\ \\frac{x_{(n-1)/2} + x_{(n+1)/2}}{2}, &amp; \\text{ si }n\\text{ es par.} \\end{cases}\\] Para el caso del tiempo de reacción a estímulos acústicos, tenemos que los datos ordenados son: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, 102_{(5)}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, 108_{(10)}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;114_{(15)}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] dónde el subscrito entre paréntesis corresponde al índice dado al dato. Cómo \\(n=19\\) es impar, se calcula el índice de posición \\((n+1)/2 = 10\\). Ubicamos en los datos ordenados la observación con el índice \\(10\\): \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, 102_{(5)}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, \\color{red}{108_{(10)}}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;114_{(15)}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] y este valor corresponde a la mediana, de forma que: \\[M =108 \\text{ segundos}\\] Se puede proceder de igual forma con el tiempo de reacción a estímulos visuales y obtener \\(M = 89\\text{ segundos}\\) (¡verifícalo!). La moda para datos no agrupados es la observación con la máxima frecuencia registrada (la frecuencia con la mayor magnitud): \\[Moda(\\{x_i\\}_{i=1}^n) = \\{x_i \\vert f_{x_i} = max(\\{f_{x_1}, f_{x_2}, \\ldots, f_{x_n}\\})\\}\\] (la notación \\(\\{x_i\\}_{i=1}^k\\) y \\(\\{x_1, x_2, \\ldots, x_n\\}\\) son equivalentes, es decir, \\(\\{x_i\\}_{i=1}^n = \\{x_1, x_2, \\ldots, x_n\\}\\)). Para calcular la moda se siguen los siguientes pasos: Se ordenan los datos de menor a mayor. Se calculan las frecuencias absolutas \\(f_{x_i}\\) contando el número de veces que la magnitud de una observación se repite. La moda es el valor que más se repita. Si hay más de un valor con la misma frecuencia absoluta, entonces la moda es un conjunto de todos los valores con la misma frecuencia absoluta. En el caso del tiempo de reacción a estímulos acústicos, los datos ordenados son: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(1)}, 100_{(1)}, 101_{(1)}, 102_{(1)}, 103_{(1)}, 106_{(1)}, 107_{(2)}, 108_{(2)}, \\\\ &amp;109_{(2)}, 113_{(1)}, 114_{(1)}, 117_{(1)}, 118_{(1)}, 120_{(1)}, 126_{(1)} \\end{aligned}\\] donde se muestra como subscrito en paréntesis la frecuencia absoluta \\(f_{x_i}\\). Como la \\(max(\\{f_{x_i}\\}) = 2\\), que es la frecuencia absoluta de \\(107, 108\\) y \\(109\\) todos en segundos, por lo que \\(Moda(\\{x_i\\}_{i=1}^n) = \\{107, 108, 109\\}\\) y los datos son multimodales. En el caso de estímulos visuales, \\(max(\\{f_{x_i}\\}) = 2\\) para \\(71\\) y \\(103\\) todos en segundos, por lo que \\(Moda(\\{x_i\\}_{i=1}^n) = \\{71, 103\\}\\) segundos (¡verifícalo!). A continuación se muestra un resumen de los estadísticos de tendencia central calculados para los tiempos de reacción acústico y visual, usando R: acoustic &lt;- c(86, 102, 103, 99, 108, 100, 118, 108, 109, 113, 114, 107, 107, 117, 120, 101, 126, 109, 106) visual &lt;- c(72, 95, 73, 99, 71, 90, 102, 97, 71, 75, 80, 70, 100, 104, 81, 103, 101, 103, 77, 78, 89) # Para el calculo de la moda f_a_acoustic &lt;- table(acoustic) mode_acoustic &lt;- names(f_a_acoustic)[which(f_a_acoustic == max(f_a_acoustic))] f_a_visual &lt;- table(visual) mode_visual &lt;- names(f_a_visual)[which(f_a_visual == max(f_a_visual))] tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Media = c(mean(acoustic), mean(visual)), Mediana = c(median(acoustic), median(visual)), Moda = c(paste(mode_acoustic, collapse=&quot;, &quot;), paste(mode_visual, collapse=&quot;, &quot;)) ) %&gt;% kbl() Estimulo Media Mediana Moda Acústico 108.05263 108 107, 108, 109 Visual 87.19048 89 71, 103 donde se muestra que: Para los tiempos de reacción a estímulos acústicos, la media y la mediana coinciden bastante, y aunque la moda consiste de tres elementos, la media y la mediana caen dentro de ese conjunto, lo cual indica que la distribución consiste de un solo pico simétrico. Para los tiempos de reacción a estímulos visuales, la mediana se desvía ligeramente de la media, indicando que la distribución es algo simétrica, pero como las modas están alejadas en promedio unas 16 unidades de la media/mediana, y unas 30 unidades entre sí, podemos decir que la distribución consiste de dos picos. Lo anterior implica que los saltamontes responden de forma única a estímulos acústicos, pero a los estímulos visuales una parte de la población de saltamontes responde rápidamente y la otra parte no tan rápido. 5.1.2 Medidas de posición. Para los datos no agrupados, los cuantiles se pueden calcular siguiendo los siguientes pasos: Se ordenan los datos de menor a mayor. Se asignan índices a las posiciones de cada observación. Se calcula la posición de los cuantiles usando: \\[C_i = \\begin{cases} \\frac{n\\times i}{d}, \\text{ si }n\\text{ es par.} \\\\ \\frac{(n+1)\\times i}{d}, \\text{ si }n\\text{ es impar.} \\end{cases}\\] Ubicamos las observaciones \\(X_{C_i}\\) correspondientes a cada cuantil. Si \\(C_i\\) no es un entero, se calcula el promedio \\(x_{C_i} = (x_{(C_i - 0{,}5)} + x_{(C_i + 0{,}5)}) / 2\\). dónde la \\(i\\) es un entero que corresponde al \\(i\\)-ésimo cuantil (ve más abajo el ejemplo), y la \\(d\\) representa en cuántas partes queremos dividir la distribución. Por ejemplo, si quisiéramos calcular los cuartiles, tendríamos que dividir la distribución en cuatro partes estableciendo \\(d=4\\), y las \\(i\\) tendrían valores de \\(1, 2\\) y \\(3\\) para los cuartiles \\(Q_1, Q_2\\) y \\(Q_3\\), respectivamente (cambiamos la notación de \\(C_i\\) a \\(Q_i\\) debido a que así se denotan usualmente en otros libros de texto y recursos). Para los tiempos de reacción ante estímulos acústicos ya se tienen los datos ordenados antes, junto con índices. Para calcular los cuartiles, como \\(n=19\\) es impar, calculamos \\(Q_i = (19 + 1)\\times i/4 = 5i\\), por lo que: \\[\\{Q_1, Q_2, Q_3\\} = \\{5, 10, 15\\}\\] Buscamos las observaciones en las posiciones dadas por el conjunto anterior: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, \\color{red}{102_{(5)}}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, \\color{red}{108_{(10)}}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;\\color{red}{114_{(15)}}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] Por lo tanto, los cuartiles son: \\[\\{x_{Q_1}, x_{Q_2}, x_{Q_3}\\} = \\{102, 108, 114\\}\\] Notamos dos cosas: la primera es que la mediana y el cuartil 2 coinciden como se esperaba. Segundo, si vemos los datos ordenados con los cuartiles en rojo, vemos que entre cada cuartil hay exactamente 4 datos, es decir, la distribución se dividió en cuatro partes, cada una con exactamente la misma cantidad de datos. Para los tiempos de reacción a estímulos visuales, se puede encontrar que \\(\\{Q_1, Q_2, Q_3\\} = \\{5{,}5; 11; 16{,}5\\}\\). Esta vez, los cuartiles son decimales, por lo que podemos usar el valor promedio de las observaciones entre las observaciones adyacentes. Por ejemplo, para \\(Q_1 = 5{,}5\\), se ubican las observaciones \\(x_5 = 73\\) y \\(x_6 = 75\\) (que son los enteros adyacentes a \\(5{,}5\\)) y calculamos el promedio \\((73 + 75) / 2 = 74\\). Realizamos el mismo procedimiento para el tercer cuartil y entonces \\(\\{x_{Q_1}, x_{Q_2}, x_{Q_3}\\} = \\{74; 89; 100{,}5\\}\\) (¡Verifica los resultado!). Ejercicio. Calcula para los datos de tiempos de reacción de saltamontes a estímulos acústicos y visuales, los deciles (\\(d=10\\)) y percentiles (\\(d = 100\\)). A continuación, se resumen los estadísticos de posición en conjunto con los de tendencia central para los tiempos de reacción (añadimos por conveniencia el mínimo y máximo valor registrado): tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Min = c(min(acoustic), min(visual)), Q_1 = c(102, 74), Media = c(mean(acoustic), mean(visual)), Mediana = c(median(acoustic), median(visual)), Q_3 = c(114, 100.5), Max = c(max(acoustic), max(visual)), Moda = c(paste(mode_acoustic, collapse=&quot;, &quot;), paste(mode_visual, collapse=&quot;, &quot;)) ) %&gt;% kbl() Estimulo Min Q_1 Media Mediana Q_3 Max Moda Acústico 86 102 108.05263 108 114.0 126 107, 108, 109 Visual 70 74 87.19048 89 100.5 104 71, 103 donde podemos ver que la distribución de datos en el grupo que recibió estímulos acústicos es más equitativa, ya que los cuartiles se distancian uno del otro en \\(6\\) unidades, mientras que los cuartiles de los datos de estímulos visuales se distancian \\(15\\) y \\(11{,}5\\) unidades, por lo que hay más datos hacia tiempos de reacción altos, y no tanto en tiempos de reacción bajos. Esto ayuda a enfatizar la asimetría pequeña de la distribución de estímulos visuales, y la simetría de la de estímulos acústicos. 5.1.3 Medidas de Dispersión. El recorrido o rango se define como la diferencia con respecto al valor máximo y el mínimo: \\[R = x_{max} - x_{min}\\] Para los tiempos de reacción se tiene que para estímulos acústicos \\(R_a = 126 - 86 = 40\\) segundos, y para estímulos visuales \\(R = 104 - 70 = 34\\) segundos. Esto quiere decir, que el tiempo de reacción a estímulos visuales ocupa una mayor cantidad de posibles observaciones, lo cual lo hace más variable la respuesta que la respuesta a estímulos acústicos. El rango intercuartílico (\\(IQR\\)) es un recorrido pero tomado desde el primer cuartil al tercer cuartil: \\[IQR = Q_3-Q_1\\] Para los tiempos de reacción a estímulos acústicos \\(IQR = 114 -102 = 12\\) segundos, mientras que para estímulos visuales \\(IQR = 100{,}5 - 74 = 26{,}5\\) segundos. Esto quiere decir que el 50% de las observaciones típicas para la respuesta a estímulos acústicos caen en un intervalo más pequeño comparado con la respuesta típica a estímulos visuales, haciendo el tiempo de reacción más consistente en el primer caso. La varianza se define como: \\[S^2 = \\frac{\\sum_{i=1}^k f_i(x_i - \\bar{X})^2}{n - 1}\\] es decir, a cada observación le quitamos el valor medio, y al resultado lo elevamos al cuadrado. Luego, sumamos los resultados y dividimos entre \\(n-1\\). Para los datos de tiempos de reacción a estímulos acústicos, se tiene que las diferencias con respecto a la media son: \\[\\begin{aligned} &amp;-22{,}05; -6{,}05; -5{,}05; -9{,}05; -0{,}05; -8{,}05; 9{,}95; -0{,}05; 0{,}95; \\\\ &amp;4{,}95; 5{,}95; -1{,}05; -1{,}05; 8{,}95; 11{,}95; -7{,}05; 17{,}95; 0{,}95; -2{,}05 \\end{aligned}\\] donde las desviaciones más grandes son aquellas que se alejan más de la media. Elevando al cuadrado permite eliminar los signos, de forma que al sumar no se cancelen los terminos, se obtiene: \\[\\begin{aligned} &amp;486{,}32; 36{,}63; 25{,}53; 81{,}95; 0; 64{,}84; 98{,}95; 0; 0{,}9; 24{,}48; \\\\ &amp;35{,}37; 1{,}11; 1{,}11; 80{,}06; 142{,}74; 49{,}74; 322{,}11; 0{,}9; 4{,}21 \\end{aligned}\\] Luego, sumando estos valores se obtiene: \\[S^2 = \\frac{1456{,}95}{19 - 1} = 80{,}94\\text{ s}^2\\] Para los tiempos de reacción a estímulos visuales se tiene \\(S^2= 167{,}16\\text{ s}^2\\) (verifícalo!). La desviación estándar es: \\[S = \\sqrt{S^2}\\] Y sirve como medida de la distancia promedio de las observaciones con respecto a la media. Usando el resultado anterior, se tiene para los tiempos de reacción a estímulos acústicos \\(S = 8{,}997\\) segundos, y para los tiempos de reacción a estímulos visuales \\(S = 12{,}929\\) segundos. Esto quiere decir que el \\(68{,}2\\)% de las observaciones típicas para la respuesta a estímulos acústicos caen a una distancia de una \\(\\sim9\\) segundos de la media, mientras que para los estímulos visuales caen a unos \\(\\sim13\\) segundos de la media, haciendo más variable la respuesta a estímulos visuales. La última medida de variación importante es el coeficiente de variación, \\(CV\\), que se define como: \\[CV = \\frac{S}{\\bar{X}}\\] Y nos dice que tan grande es la desviación estándar con respecto a la media. Para una distribución normal este valor es \\(\\sim0{,}3\\) (\\(30\\)% en valor porcentual). Para los tiempos de reacción se tiene que para estímulos acústicos \\(CV = 8{,}997/ 108{,}05 = 0{,}0833\\) (\\(\\sim8{,}3\\)%), y para estímulos visuales \\(CV = 12{,}929 / 87{,}19 = 0{,}1483\\) (\\(\\sim14{,}8\\)%). Esto quiere decir, que el tiempo de reacción a estímulos visuales tiene una variación mayor (de aproximadamente 6% mayor) que la respuesta a estímulos acústicos, dado que la desviación estándar representa una mayor proporción de la magnitud de la media. Resumimos los estadísticos de dispersión a continuación: tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Rango = c(max(acoustic) - min(acoustic), max(visual) - min(visual)), IQR = c(114 - 102, 100.5 - 74), Varianza = c(var(acoustic), var(visual)), Std.Dev = c(sd(acoustic), sd(visual)), CV = c(sd(acoustic) / mean(acoustic), sd(visual) / mean(visual)) * 100 ) %&gt;% kbl() Estimulo Rango IQR Varianza Std.Dev CV Acústico 40 12.0 80.94152 8.996751 8.326267 Visual 34 26.5 167.16190 12.929111 14.828581 5.1.4 Medidas de Forma. El coeficiente de asimetría es una medida de forma que busca cuantificar la simetría de una distribución. Se calcula como: \\[A = \\frac{\\sum_{i=1}^n(x_i - \\bar{X})^3}{n S^3}\\] Para una distribución simétrica, se esperaría obtener el mismo número de diferencias \\((x_i - \\bar{X})^3\\) negativas como positivas, y al sumar la magnitud de todas las diferencias negativas, esta sería de igual magnitud que la suma de todas las diferencias positivas, por lo que \\(A = 0\\) si la distribución es simétrica (aunque lo contrario no es cierto: el que \\(A = 0\\) no asegura que la distribución sea simétrica). La asimetría puede ser positiva o negativa dependiendo de la dirección del sesgo. Para distribuciones sesgadas hacia la derecha, \\(A &gt; 0\\). Para distribuciones sesgadas hacia la izquierda, \\(A &lt; 0\\). Se puede calcular la sumatoria de cubos para los tiempos de reacción a estímulos acústicos y visuales, y obtener el coeficiente de asimetría, que para las población de saltamontes sometida a estímulos acústicos \\(A = -0{,}23\\), y para la que fue sometida a estímulos visuales \\(A = -0{,}01\\). Esto muestra que los tiempos de reacción a estímulos visuales es solo muy ligeramente asimétrica, pero que la de tiempos de reacción a estímulos acústicos es in poco más sesgada hacia la izquierda (aunque no tan apreciablemente). La curtosis se define como: \\[K = \\frac{\\sum_{i=1}^n(x_i - \\bar{X})^4}{n S^4} - 3\\] lo cual nos dice algo sobre la forma como se concentran los datos alrededor de la media. Para la distribución normal (que es simétrica y se usa como estándar de comparación) se puede verificar que \\(K = 0\\). Se dice entonces que la distribución: * Es platicúrtica si \\(K &lt; 0\\), y esto implica que las distribuciones extremas son más probables con respecto a la normal. * Es mesocúrtica si \\(K=0\\), y las observaciones se distribuyen más como una normal. * Es leptocúrtica si \\(K &gt; 0\\), y entonces las observaciones tienden a aglomerarse alrededor de la media más de lo que ocurre en la distribución normal. Para las población de saltamontes sometida a estímulos acústicos \\(K = 0{,}166\\), y para la que fue sometida a estímulos visuales \\(K = -1{,}79\\). Esto muestra que los tiempos de reacción a estímulos acústicos se asemeja a una distribución mesocúrtica y los tiempos de reacción a estímulos visuales es platicúrtica, indicando una distribución que tiene colas pesadas, siendo la de estímulos visuales mucho más dispersa alrededor de la media (esto tiene sentido dado nuestro descubrimiento de que se trata de una distribución multimodal). tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Asimetria = c(1 / 19, 1 / 21) * c(sum((acoustic - mean(acoustic)) ** 3) / sd(acoustic) ** 3, sum((visual - mean(visual)) ** 3) / sd(visual) ** 3), Curtosis = c(1 / 19, 1 / 21) * c(sum((acoustic - mean(acoustic)) ** 4) / sd(acoustic) ** 4, sum((visual - mean(visual)) ** 4) / sd(visual) ** 4) - 3 ) %&gt;% kbl() Estimulo Asimetria Curtosis Acústico -0.2299108 0.1659499 Visual -0.0100155 -1.7877228 Ejercicio Para el siguiente conjunto de datos de pesos de individuos de una población, separados en Machos (M) y Hembras (H), realice un análisis descriptivo. # Datos Diferencias Peso de Machos y hembras ungrouped &lt;- tibble( Sex = unlist(strsplit(&quot;HMHMHMHMMHMMHHHMHMMHMHMMHMMHHMHMMHMMHMHMMHMHMMHHMM&quot;, &quot;&quot;)), Peso = c(98.5, 150.6, 108.3, 159.4, 162.6, 122.5, 118.5, 167.5, 170.5, 120.4, 177.5, 186.5, 115.4, 115.5, 52.5, 157.6, 134.4, 148.5, 131.5, 143.4, 145.6, 108.6, 155.5, 110.6, 154.5, 183.5, 191.5, 128.6, 135.4, 195.4, 137.5, 205.6, 190.7, 120.5, 188.7, 176.3, 118.5, 158.5, 116.5, 161.4, 165.5, 142.6, 164.6, 120.5, 170.4, 195.5, 132.5, 129.5, 215.6, 176.5) ) "],["datos-agrupados..html", "5.2 Datos Agrupados.", " 5.2 Datos Agrupados. Para comprender los conceptos de la estadística descriptiva en datos agrupados, usamos el conjunto de datos obtenidos del experimento que busca evaluar los niveles de glicemia (en mg dL\\({}^{-1}\\)) en 25 pacientes, cuyos resultados fueron: \\[75, 82, 90, 95, 101, 112, 121, 132, 140, 97, 84, 90, 96, 102, 114, 121, 138, 87, 91, 96, 104, 123, 89, 93, 99\\] Como vimos antes, estos datos se pueden agrupar en clases que denotan intervalos (que pueden ser continuos o aparentes) en los cuales caen las observaciones con cierta frecuencia absoluta, como se muestra a continuación: glicemia &lt;- tribble( ~Lim_rel_inf, ~Lim_rel_sup, ~`Marca de Clase`, ~`f_i`, ~`fr_i`, ~`F_i`, ~`Fr_i`, 74.5, 86.5, 80.5, 3, 12 / 100, 3, 12 / 100, 86.5, 98.5, 92.5, 10, 40 / 100, 13, 52 / 100, 98.5, 110.5, 104.5, 4, 16 / 100, 17, 68 / 100, 110.5, 122.5, 116.5, 4, 16 / 100, 21, 84 / 100, 122.5, 134.5, 128.5, 2, 8 / 100, 23, 92 / 100, 134.5, 146.5, 140.5, 2, 8 / 100, 25, 100 / 100) glicemia %&gt;% kbl() Lim_rel_inf Lim_rel_sup Marca de Clase f_i fr_i F_i Fr_i 74.5 86.5 80.5 3 0.12 3 0.12 86.5 98.5 92.5 10 0.40 13 0.52 98.5 110.5 104.5 4 0.16 17 0.68 110.5 122.5 116.5 4 0.16 21 0.84 122.5 134.5 128.5 2 0.08 23 0.92 134.5 146.5 140.5 2 0.08 25 1.00 5.2.1 Medidas de Tendencia Central. El promedio se define como: \\[\\bar{X} = \\frac{\\sum_{i=1}^k f_ic_i}{n}\\] dónde \\(c_i\\) es la \\(i\\)-ésima marca de clase, y \\(f_i\\) es la frecuencia absoluta asociada a la marca de clase (cuantas observaciones \\(c_i\\) contiene dentro de los límites del intervalo). Para las marcas de clases de los índices de glicemia medidos se calcularía entonces: \\[\\bar{X} = \\frac{3\\times80{,}5 + 10\\times92{,}5 + 4\\times104{,}5 + 4\\times116{,}5 + 2\\times128{,}5 + 2\\times140{,}5}{25}=103{,}54 \\text{ g dL}^{-1}\\] La mediana para datos agrupados se consigue siguiendo los siguientes pasos: Se calcula el índice de posición de la mediana como \\(n/2\\). Se localiza la clase que identifica el intervalo mediano (aquel que contiene la mediana) al buscar la primera clase cuya frecuencia absoluta acumulada sea igual o mayor a \\(n/2\\). La mediana es entonces: \\[M = L_i + \\left(\\frac{n/2 - F_{i-1}}{f_i}\\right)\\cdot a_i\\] donde \\(L_i\\) es el límite inferior del intervalo mediano, \\(F_{i-1}\\) es la frecuencia absoluta acumulada de la clase anterior, \\(f_i\\) es la frecuencia absoluta de la clase que contiene la mediana, y \\(a_i\\) es la amplitud de los intervalos. Para el caso de los índices de glicemia, se tiene que \\(n/2 = 25/2 =12{,}5\\). La primera clase con \\(F_i \\ge 12{,}5\\) es la segunda clase (\\(F_i = 13\\)). Inspeccionando la tabla de datos agrupados podemos calcular entonces: \\[M = 86{,}5 + \\left(\\frac{12{,}5 - 3}{10}\\right)\\cdot 12 = 97{,}9\\text{ g dL}^{-1}\\] La moda se calcula, para datos agrupados, siguiendo estos pasos: Se busca la clase modal (aquella que contiene la moda) determinando cuál de ellas tiene la mayor frecuencia absoluta. Se calcula la moda como: \\[Moda(\\{c_i\\}_{i=1}^k) = L_i + \\left(\\frac{f_i - f_{i-1}}{(f_i - f_{i-1}) + (f_i - f_{i+1})}\\right)\\cdot a_i\\] Para el caso de los índices de glicemia, la clase con la mayor frecuencia absoluta es la segunda clase, por lo que: \\[Moda(\\{c_i\\}_{i=1}^k) = 86{,}5 + \\left(\\frac{10 - 3}{(10 - 3) + (10 - 4)}\\right)\\cdot 12 = 92{,}96\\text{ g dL}^{-1}\\] Notamos que los resultados sobre los datos de frecuencia muestran que \\(Moda &lt; M &lt; \\bar{X}\\), lo cual nos indica que la distribución esta sesgada a la derecha. 5.2.2 Medidas de posición. Para los cuantiles de datos agrupados, se sigue un procedimiento similar al de datos sin agrupar, identificando primero las clases que contiene los cuantiles. Para ello, se sigue el procedimiento: Calcula \\(\\frac{n\\times i}{d}\\) donde \\(i\\) representa el \\(i\\)-ésimo cuantil, y \\(d\\) el número de partes en las que se desea dividir la distribución. Se busca en la tabla de datos agrupados la clase cuya frecuencia absoluta acumulada sea mayor o igual a \\(\\frac{n\\times i}{d}\\). Este será la clase cuantílica. Se calcula el cuantil como: \\[C_i = L_i + \\left(\\frac{f_i - f_{i-1}}{f_i}\\right)\\cdot a_i\\] Digamos que queremos calcular los cuartiles de los datos de índice de glicemia. En este caso, calculamos para \\(i = 1,2,3\\) los valores de \\(i\\times n / 4\\), los cuales son \\(6{,}25, 12{,}5,\\) y \\(18{,}75\\). Calculamos los cuartiles: para el primer cuartil, el resultado muestra que el cuartil se encuentra en la segunda clase, por lo que: \\[Q_1 = 86{,}5 + \\left(\\frac{10 - 3}{10}\\right)\\cdot 12 = 95{,}7\\text{ g dL}^{-1}\\] Para el segundo cuartil nos damos cuenta que el método arroja \\(Q_2 = Q_1\\), lo cual es un error (asegúrese de verificar este resultado, ¿por qué sucede esto?). La razón de esto es que la resolución de los datos no permite la estimación de los cuantiles dado el tamaño de la muestra (vea la discusión más adelante en la siguiente sección). En este caso, recordamos que \\(Q_2 = M\\), y usamos el valor de la mediana calculado anteriormente: \\[Q_2 = 97{,}9\\text{ g dL}^{-1}\\] Para el tercer cuartil, vemos que la clase que contiene el cuartil es la cuarta clase, por lo que: \\[Q_3 = 110{,}5 + \\left(\\frac{21 - 17}{21}\\right)\\cdot 12 = 112{,}79\\text{ g dL}^{-1}\\] De los resultados anteriores, podemos notar que la distancia entre el primer y segundo cuartil es un orden de magnitud menor que la distancia entre el segundo y tercer cuartil, indicando que las observaciones en la tercera parte de la distribución tienen una mayor dispersión, y que en la segunda parte de la distribución las observaciones se aglomeran. Esto refuerza la intuición obtenida antes con las medidas de tendencia central que la distribución está sesgada hacia la derecha. 5.2.3 Medidas de dispersión. El recorrido (o rango) y el rango intercuartílico (IQR) se calculan igual que antes para datos sin agrupar. Sin embargo, la definición de varianza la modificamos para usar las marcas de clase, y no las observaciones: \\[S^2 = \\frac{\\sum_{i=1}^k f_i(c_i - \\bar{X})^2}{n - 1}\\] A partir de esta, es posible calcular la desviación estándar y el coeficiente de variación tal como se definieron para datos sin agrupar. Se tiene que \\(R = 140 - 75 = 65\\) g dL\\({}^{-1}\\) y \\(IQR = 112{,}79 - 95{,}7 = 17{,}09\\) g dL\\({}^{-1}\\), los cuales nos indican que el 50% de las observaciones solo se encuentran ocupando aproximadamente un \\(4\\)% del dominio posible de las observaciones. La varianza es \\(S^2 = 311{,}04\\) (g dL\\({}^{-1}\\))\\({}^2\\), y \\(S = 17{,}64\\) g dL\\({}^{-1}\\) con \\(CV = 0{,}1703\\). Esto nos indica que la distribución de los datos parece no ser tan variable, pero esto puede ser engañoso ya que sabemos que la distribución esta sesgada. 5.2.4 Medidas de forma. Para el cálculo del coeficiente de asimetría y curtosis, se procede al igual que antes para datos sin agrupar, pero usamos las marcas de clases en lugar de las observaciones para realizar el cálculo. El coeficiente de asimetría se calcula como: \\[A = \\frac{\\sum_{i=1}^kf_i(c_i - \\bar{X})^3}{nS^3}\\] y la curtosis como: \\[A = \\frac{\\sum_{i=1}^kf_i(c_i - \\bar{X})^4}{nS^4} - 3\\] Para los índices de glicemia resumidos en la tabla de datos agrupados, obtenemos \\(A = 0{,}661\\) y \\(K = 2{,}32\\). Esto nos indica que la distribución es ligeramente sesgada hacia la derecha (como ya parecíamos intuir de las otras medidas) y leptocúrtica. Esto indica que el sesgo observado es resultado de observaciones atípicas. Media &lt;- sum(glicemia$f_i * glicemia$`Marca de Clase`) / 25 Std.Dev &lt;- sqrt(sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 2) / 24) tribble(~Media, ~`Desv. Est.`, ~Asimetria, ~Curtosis, Media, Std.Dev, sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 3) / (25 * Std.Dev ** 3), sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 4) / (25 * Std.Dev ** 4) ) %&gt;% kbl() Media Desv. Est. Asimetria Curtosis 103.54 17.63633 0.6609389 2.32162 Ejercicio. En un estudio del síndrome de Down, se examinaron 180 niños afectados y la siguiente tabla da la distribución de frecuencias para el cociente intelectual (IQ) de los niños. Complete la tabla de datos agrupados, añadiendo las columnas que hagan falta, y determine las medidas de tendencia central, los cuartiles, deciles y percentiles, las medidas de dispersión y de forma. Discuta los resultados. tibble( Clase = c(1, 2, 3, 4, 5, 6, 7, 8, 9), `Límites de clase` = c(&quot;10.5 a 20.5&quot;, &quot;20.5 a 30.5&quot;, &quot;30.5 a 40.5&quot;, &quot;40.5 a 50.5&quot;, &quot;50.5 a 60.5&quot;, &quot;60.5 a 70.5&quot;, &quot;70.5 a 80.5&quot;, &quot;80.5 a 90.5&quot;, &quot;90.5 a 100.5&quot;), `Marca de clase` = c(15.5, 25.5, 35.5, 45.5, 55.5, 65.5, 75.5, 85.5, 95.5), `Frecuencia f_i` = c(4, 34, 0, 70, 43, 19, 7, 2, 1) ) %&gt;% kbl() Clase Límites de clase Marca de clase Frecuencia f_i 1 10.5 a 20.5 15.5 4 2 20.5 a 30.5 25.5 34 3 30.5 a 40.5 35.5 0 4 40.5 a 50.5 45.5 70 5 50.5 a 60.5 55.5 43 6 60.5 a 70.5 65.5 19 7 70.5 a 80.5 75.5 7 8 80.5 a 90.5 85.5 2 9 90.5 a 100.5 95.5 1 "],["distribuciones-de-probabilidad..html", "6 Distribuciones de Probabilidad.", " 6 Distribuciones de Probabilidad. Hasta este punto, hemos definido ya lo que es una variable aleatoria y cómo podemos usar esta variable para definir probabilidades en intervalos de números reales. Estas funciones se pueden definir para obtener información de las características de las variables aleatorias. Las funciones usadas son la función de distribución (o función de densidad probabilística) y la función de distribución (o función de distribución acumulada). "],["función-de-probabilidad..html", "6.1 Función de Probabilidad.", " 6.1 Función de Probabilidad. Esta función es la que no da información sobre la probabilidad de un evento cualquiera dentro del espacio probabilístico. El establecer la definición de la función de probabilidad se debe hacer para los casos en los que se tienen variables aleatorias discretas, y para los casos en los que se tienen variables aleatorias continuas. Caso discreto. Sea una v.a. \\(X\\) que toma valores \\(x_0, x_1, \\ldots\\) con probabilidades \\(p_0 = P(X = x_0), p_1 = P(X = x_1), \\ldots\\) (esta es una lista infinita, pero numerable, de probabilidades asignadas a los posibles valores de \\(X\\)). Se define entonces la función de probabilidad discreta como: \\[f(x) = \\begin{cases} P(X = x), &amp; \\text{ si }x = x_0, x_1, \\ldots \\\\ 0, &amp; \\text{ de otra forma.} \\end{cases}\\] Notamos que una función definida de esta forma, cumple con todos los axiomas de Kolmogorov y es, por lo tanto, una medida de probabilidad. De lo que se tiene: \\[\\begin{align} f(x) &amp;\\ge 0 \\\\ \\sum f(x) &amp;= 1 \\end{align}\\] De esta forma, podemos definir la probabilidad de un evento cualquiera como: \\[P(X \\in A) = \\sum_{x \\in A} f(x)\\] Esto es así ya que, como vimos, \\(A\\) estaría formado por la unión de eventos disjuntos, cuya probabilidad es la sumatoria de las probabilidades individuales. Ejemplo. Sea \\(X\\) una v.a. que toma los valores \\(1,2,3\\) con probabilidades \\(0{,}3, 0{,}5, 0{,}2\\), respectivamente. Definimos la función de probabilidad como: \\[f(x) = \\begin{cases} 0{,}3, &amp; \\text{ si }x = 1 \\\\ 0{,}5, &amp; \\text{ si }x = 2 \\\\ 0{,}2, &amp; \\text{ si }x = 3 \\\\ 0, &amp; \\text{ de otra forma.} \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = 1:3, y=c(.3, .5, .2))) + geom_segment(aes(x = 1:3, y = rep(0, 3), xend = 1:3, yend = c(.3, .5, .2)), linewidth = 2, linetype = &quot;dashed&quot;, colour = &quot;gray75&quot;) + geom_point(size = 3) + geom_hline(yintercept = 0, linewidth = 3) + geom_point(aes(y = rep(0, 3)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;f(x)&quot;) + xlab(&quot;x&quot;) + theme_light(base_size = 14) + theme(panel.grid = element_blank()) a partir de la cual podemos calcular la probabilidad de cualquier evento, como por ejemplo \\(P(X\\ge2) = P(X=2) + P(X=3) + P(X=4) + \\ldots = 0{,}5 + 0{,}2 + 0 + \\ldots = 0{,}7\\) o \\(P(\\vert X\\vert = 1) = P(X = 1) + P(X = -1) = 0{,}3 + 0 = 0{,}3\\). En el ejemplo anterior vemos que no hubo necesidad de definir un experimento aleatorio para construir una función de probabilidad. Esta libertad nos permite definir arbitrariamente funciones de probabilidad en esquemas genéricos, lo único que se necesita es que obedezcan los axiomas de Kolmogorov. Ejercicio. Una muestra de \\(7\\) semillas contiene \\(2\\) infectadas con una enfermedad. Un agrónomo compra \\(3\\) de las semillas al azar. Si \\(x\\) es el número de unidades defectuosas que compra el agrónomo, calcule la distribución de probabilidad de \\(X\\). Exprese los resultados de forma gráfica como un histograma de probabilidad. Ahora definimos la función de probabilidad para el caso contínuo. Caso continuo. Sea \\(X\\) una v.a. continua. Decimos que \\(f(x)\\) es la función de densidad de la variable \\(X\\) en el intervalo \\([a, b]\\in\\mathbb{R}\\) si se cumple: \\[P(X \\in[a,b]) = \\int_a^b f(x)dx\\] donde \\(f(x)\\) es una función no negativa e integrable en el intervalo \\([a,b]\\) Bajo esta definición, es claro que se cumplen los criterios de Kolmogorov: \\[\\begin{align} f(x) &amp;\\ge 0 \\space \\forall \\space x \\in \\mathbb{R} \\\\ \\int_{-\\infty}^{\\infty} f(x) &amp;= 1 \\end{align}\\] Ejemplo. Sea \\(X\\) una v.a. continua con función de probabilidad definida como: \\[f(x) = \\begin{cases} 3x^2/2, &amp; \\text{ si } -1 &lt; x &lt; 1 \\\\ 0, &amp; \\text{ de otra forma} \\end{cases}\\] cuyo gráfico se muestra a continuación. x &lt;- seq(-1, 1, by=.1) ggplot(NULL, aes(x = x)) + geom_line(aes(y=1.5 * x ** 2), linewidth = 2) + geom_line(aes(x = c(-1.5, -1), y=c(0, 0)), linewidth = 2) + geom_line(aes(x = c(1, 1.5), y=c(0, 0)), linewidth = 2) + geom_point(aes(x=c(-1, 1), y = rep(0, 2)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;f(x)&quot;) + xlab(&quot;x&quot;) + theme_light() a partir de la cual podemos calcular la probabilidad de cualquier evento, como por ejemplo: \\[\\begin{aligned} P(X \\le 1/3) &amp;= \\int_{-\\infty}^{1/3}f(x)dx \\\\ &amp;= \\int_{-\\infty}^{-1}0dx + \\int_{-1}^{1/3}\\frac{3}{2}x^2dx \\\\ &amp;= 0 + \\frac{(1/3)^3}{2} - \\frac{(-1)^3}{2} \\\\ &amp;= \\frac{1}{54} + \\frac{1}{2} \\\\ &amp;= \\frac{14}{27} \\end{aligned}\\] Al igual que antes, no hubo necesidad de definir un experimento aleatorio para construir una función de probabilidad continua. Ejercicio. El tiempo que pasa, en segundos, para que un murciélago detecte entre árboles sucesivos a una presa que se mueve a una velocidad dada es una variable aleatoria continua con una función de distribución acumulativa: \\[F(x) = \\begin{cases} 0, &amp; x &lt; 0, \\\\ 1 - e^{-8x}, &amp; x ≥ 0 \\end{cases}\\] Calcule la probabilidad de que el tiempo que pase para que el murciélago detecte entre árboles sucesivos a las presas que exceden una velocidad de escape sea menor de 12 minutos. "],["función-de-distribución..html", "6.2 Función de distribución.", " 6.2 Función de distribución. Sea \\(X\\) una variable aleatoria cualquiera, la función de distribución, denotada como \\(F(x) : \\mathbb{R} \\rightarrow \\mathbb{R}\\) (lo cual se lee como: \\(F(x)\\) toma un número real y lo transformar en otro número real) se define como la probabilidad: \\[F(x) = P(X \\le x)\\] Vemos entonces porque la llamamos también función de probabilidad acumulada, ya que \\(F(x)\\) denota la probabilidad acumulada hasta el valor observado \\(x\\). También notamos que, como las probabilidades son todas mayores o iguales a cero, y que la probabilidad del espacio muestral en su totalidad es \\(1\\), la función de distribución se define entre \\(0\\) y \\(1\\). Al igual que antes, se hace necesario distinguir entre la función de distribución en el caso discreto y en el caso continuo. Caso discreto. Si \\(X\\) es una v.a. discreta con función de distribución \\(f(x)\\), entonces se define: \\[F(x) = \\sum_{t \\le x} f(t)\\] Ejemplo. Para el ejemplo anterior dado para la función de probabilidad de una v.a. discreta, podemos construir la función de distribución considerando todos los intervalos donde la probabilidad se mantenga contante. De esta forma obtenemos: \\[F(x) = \\begin{cases} 0, &amp; x &lt; 1 \\\\ 0{,}3, &amp; 1 \\le x &lt; 2 \\\\ 0{,}8, &amp; 2 \\le x &lt; 3 \\\\ 1, &amp; x \\ge 3 \\\\ \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = 1:3, y=c(.3, .8, 1))) + geom_point(size = 3) + geom_line(aes(x = c(0, 1), y = c(0, 0)), linewidth = 2) + geom_line(aes(x = c(1, 2), y = c(.3, .3)), linewidth = 2) + geom_line(aes(x = c(2, 3), y = c(.8, .8)), linewidth = 2) + geom_line(aes(x = c(3, 4), y = c(1, 1)), linewidth = 2) + geom_point(aes(x = c(1, 2, 3), y = c(0, .3, .8)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;F(x)&quot;) + xlab(&quot;x&quot;) + theme_light() Caso continuo. Si \\(X\\) es una v.a. continua con función de distribución \\(f(x)\\), entonces se define: \\[F(x) = \\int_{-\\infty}^x f(t)dt\\] Ejemplo. Se tiene una variable aleatoria \\(X\\) con función de probabilidad dada por: \\(f(x) = \\begin{cases}\\vert x\\vert, &amp; -1 &lt; x &lt; 1 \\\\ 0 &amp; \\text{ de otra forma}\\end{cases}\\). La obtención de la función de distribución se obtiene aplicando la definición en cada intervalo en los que la definición de \\(f(x)\\) cambia. Empezando con el intervalo de \\((-\\infty, -1)\\), se tiene: \\[F(X) = \\int_{-\\infty}^-1 0dx = 0\\] Luego, en el intervalo \\([-1,0)\\) se tiene: \\[F(X) = \\int_{-1}^x -xdx = (1-x^2)/2\\] y así seguimos hasta obtener \\(F(x)\\) en todos los reales: \\[F(x) = \\begin{cases} 0, &amp;x \\le -1 \\\\ (1-x^2)/2, &amp;-1 \\le x &lt; 0 \\\\ (1+x^2)/2, &amp;0 &lt; x \\le 1 \\\\ 1, &amp;x \\ge 1 \\\\ \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = c(-1.5, -1), y=c(0, 0))) + geom_line(linewidth = 2) + geom_line(aes(x = seq(-1, 0, by = .1), y = (1 - seq(-1, 0, by = .1) ** 2) / 2), linewidth = 2) + geom_line(aes(x = seq(0, 1, by = .1), y = (1 + seq(0, 1, by = .1) ** 2) / 2), linewidth = 2) + geom_line(aes(x = c(1, 1.5), y = c(1, 1)), linewidth = 2) + ylab(&quot;F(x)&quot;) + xlab(&quot;x&quot;) + theme_light() 6.2.1 Propiedades de la función de distribución. La función de distribución resulta ser muy importante desde el punto de vista matemático, pues siempre puede definirse dicha función para cualquier variable aleatoria y a través de ella quedan representadas todas las propiedades de la variable aleatoria. Cualquier función que cumpla las siguientes propiedades es una función de distribución, sea que tenga o no una variable aleatoria asociada. \\(F(x)\\) está acotada por arriba por \\(1\\), lo cual se puede escribir como \\[\\lim_{x\\rightarrow\\infty} F(x) = 1\\]. Esto es así dado que la probabilidad de todo el espacio muestral es \\(1\\). \\(F(x)\\) está acotada por abajo por \\(0\\), lo cual se puede escribir como \\[\\lim_{x\\rightarrow-\\infty} F(x) = 0\\]. Esto es así dado que las probabilidades son no negativas. \\(F(x)\\) es monótona no decreciente, esto es, si \\(x_1 \\le x_2\\), entonces \\(F(x_1) \\le F(x_2)\\). \\(F(x)\\) es continua por la derecha, lo cual se puede escribir como \\[F(x) = \\lim_{x\\rightarrow x_0^+} F(x)\\]. "],["cálculos-con-funciones-de-probabilidades..html", "6.3 Cálculos con funciones de probabilidades.", " 6.3 Cálculos con funciones de probabilidades. "],["distribuciones-de-probabilidad-de-variables-discretas..html", "7 Distribuciones de probabilidad de variables discretas.", " 7 Distribuciones de probabilidad de variables discretas. "],["distribuciones-de-probabilidad-de-variables-continuas..html", "8 Distribuciones de probabilidad de variables continuas. ", " 8 Distribuciones de probabilidad de variables continuas. "],["distribución-normal..html", "8.1 Distribución Normal.", " 8.1 Distribución Normal. La distribución normal (o gaussiana) es de las distribuciones más importantes que estudiaremos. Esta la usaremos con frecuencia más adelante cuando realicemos inferencias a partir de observaciones realizadas en un experimento. Aparecerá primero en el teorema del límite central (capítulo Teoría de Muestreo.), que es uno de los teoremas más importantes que tiene aplicaciones directas en la práctica. Decimos que una variable aleatoria \\(X\\) tiene una distribución normal si su función de densidad viene dada por: \\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty,\\] donde \\(\\mu, \\sigma^2 \\in \\mathbb{R}\\), con \\(\\sigma^2 &gt; 0\\), son los parámetros de la distribución. Si la variable \\(X\\) se distribuye como normal se escribe: \\[X \\sim N(\\mu, \\sigma^2)\\] La grafica de la función de densidad normal tiene forma de campana, siendo simétrica con respecto a la vertical que pasa por la media \\(\\mu\\), la cual es el centro de la campana. Siendo \\(\\sigma\\) (raíz cuadrada de la varianza \\(\\sigma^2\\)) es la distancia del centro a cualquiera de los puntos de inflexión de la curva (como se muestra en la figura 8.1). Figure 8.1: Función de densidad de una variable aleatoria normal N(\\(\\mu, \\sigma^2\\)). Esta información se resume como: \\[\\begin{aligned} &amp; E(X) = \\mu \\\\ &amp; Var(X) = \\sigma^2 \\end{aligned}\\] La función de distribución viene dada por la integral: \\[F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}dy\\] la cual no tiene primitiva analítica asociada y debe resolverse por métodos numéricos. Es posible obtener valores de probabilidad acumulada en R usando el comando pnorm, el cual da \\(F(x) = P(X \\le x)\\), como se muestra en la figura siguiente. Figure 8.2: Función de distribución acumulada de una variable aleatoria normal N(\\(\\mu, \\sigma^2\\)). Por ejemplo, supongamos que se tiene la variable aleatoria \\(X\\) que representa la longitud del ala de abejas de cierta granja de apicultores. Se sabe, de estudios anteriores, que la longitud media es de \\(1{,}5 \\pm 0{,}73\\) cm. Los mismos estudios preliminares han mostrado que \\(X\\) sigue una distribución normal, por lo que se escribe: \\[X \\sim N(\\mu = 1{,}5, \\sigma^2 = 0.533)\\] Podemos obtener la probabilidad de que una abeja tenga una longitud del ala menor a \\(1\\) cm, \\(P(X \\le 1\\text{ cm})\\) como pnorm(1, 1.5, 0.73) (figura 8.3, a la izquierda). Si queremos la probabilidad de aquellas con una longitud del ala mayor a \\(3\\) cm, \\(P(X \\ge 3\\text{ cm}) = 1 - P(X \\le 3\\text{ cm})\\), que se calcula en R como 1 - pnorm(3, 1.5, 0.73) (figura 8.3, a la derecha). Figure 8.3: Función de densidad de una variable aleatoria normal N(\\(\\mu = 1{,}5, \\sigma^2 = 0{,}532\\)), donde se muestra de forma grafica la probabilidad acumulada \\(P(X \\le 1)\\) (a la izquierda) y \\(P(X \\ge 3)\\) (a la derecha). Notamos, de las figuras del ejemplo, que la probabilidad acumulada se puede entender como el área debajo de la función de densidad para un valor de \\(X\\) observado o menor, y que para encontrar valores acumulados hacia arriba, solo necesitamos usar \\(1 - F(X)\\). Ejercicio. Un investigador informa que unos ratones a los que primero se les restringen drásticamente sus dietas y después se les enriquecen con vitaminas y proteínas vivirán un promedio de \\(40\\) meses. Si suponemos que la vida de tales ratones se distribuye normalmente, con una desviación estándar de \\(6{,}3\\) meses, calcule la probabilidad de que un ratón determinado viva a) más de \\(32\\) meses; b) menos de \\(28\\) meses; c) entre \\(37\\) y \\(49\\) meses. Un caso particular de esta distribución, que es muy útil, es cuando \\(\\mu = 0\\) y \\(\\sigma^2 = 1\\), la cual da lugar a la distribución normal estándar, cuya función de densidad queda: \\[f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}, \\quad -\\infty &lt; x &lt; \\infty,\\] la cual también se denota como \\(\\phi(x)\\). Esto es importante porque significa que siempre es posible transformar una v.a. normal en una estándar por una simple operación: \\[Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\] que se conoce como estandarización. La importancia de este procedimiento es que el cálculo de probabilidades de una variable aleatoria normal se puede reducir al cálculo de probabilidad de una variable aleatoria de distribución normal estándar. Esto es fácil de ver ya que: \\[\\begin{aligned} P(a &lt; X &lt; b) &amp;= P(a - \\mu &lt; X - \\mu &lt; b - \\mu) \\\\ &amp;= P\\left(\\frac{a - \\mu}{\\sigma} &lt; \\frac{C - \\mu}{\\sigma} &lt; \\frac{b - \\mu}{\\sigma}\\right) \\\\ &amp;= P\\left(\\frac{a - \\mu}{\\sigma} &lt; Z &lt; \\frac{b - \\mu}{\\sigma}\\right) \\end{aligned}\\] Se puede demostrar que si \\(X\\) se distribuye como una normal estándar, entonces la variable \\(-X\\) también tiene distribución normal estándar, y: \\[\\Phi(-x) = 1 - \\Phi(x)\\] Definimos los cuantiles de la distribución normal estándar \\(z_\\alpha\\) para cada valor de \\(\\alpha\\) en el intervalo \\((0,1)\\) como aquel para el cual: \\[\\Phi(z_{\\alpha}) = 1 - \\alpha\\] Algunos cuantiles importantes que vale la pena recordar, y que usaremos frecuentemente más adelante son: \\(z_{0{,}9} = 1{,}28\\), \\(z_{0{,}95} = 1{,}64\\), \\(z_{0{,}975} = 1{,}96\\) y \\(z_{0{,}99} = 2{,}33\\). Ahora, mencioanmos una proposición muy útil sobre la suma de dos variables aleatorias normales. Sean \\(X_1\\) y \\(X_2\\) dos variables aleatorias independientes con distribución \\(N(\\mu_1, \\sigma_1^2)\\) y \\(N(\\mu_2, \\sigma_2^2)\\), entonces: \\[X_1 + X_2 \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\] Veamos un ejemplo. Por ejemplo, digamos que un investigador tiene una población de ciertas bacterias en un cultivo puro que se esta estudiando por sus capacidades de producir cierta proteína transmembrana de interés, que sirve como transportador de un metabolito que se desea degradar. Se sabe que esta se produce en este cultivo con una densidad de \\(35 \\pm 4{,}3\\) unidades por centímetro cuadrado por célula. Sin embargo, en un accidente, el investigador mezclo dos cultivos con capacidades distintas de producir la proteína transmembrana. Este segundo cultivo tiene una capacidad menor de producir la proteína que funciona como transportador, haciendola menos efectiva en metabolizar el metabolito, con una densidad de solo \\(11 \\pm 7{,}1\\) unidades por centímetro cuadrado por célula. Si suponemos que la v. a. densidad de la proteína por centímetro cuadrado por célula se distribuye como una normal, entonces \\(X_{\\text{Cultivo 1}}\\sim N(35, 18{,}49)\\) y \\(X_{\\text{Cultivo 2}}\\sim N(11, 50{,}41)\\) en ambos cultivos aislados; y luego del accidente, \\(X_{\\text{Cultivo Mezclado}}\\sim N(46, 68{,}9)\\). "],["distribución-ji-cuadrada..html", "8.2 Distribución Ji-Cuadrada.", " 8.2 Distribución Ji-Cuadrada. Se dice que una variable aleatoria continúa \\(X\\) sigue una distribución Ji-Cuadrada con \\(n\\) grados de libertad (\\(n&gt;0\\)), si su función de densidad viene dada por: \\[f(x) = \\begin{cases} \\frac{1}{\\gamma(n/2)}\\left(\\frac{1}{2}\\right)^{n/2}x^{n/2 - 1}e^{-x/2} &amp;\\text{ si }x &gt; 0 \\\\ 0 &amp;\\text{ en otro caso}\\end{cases}\\] Esta función se distribuye en el intervalo \\((0, \\infty)\\) y su único parámetro son los grados de libertad \\(n\\) que puede ser cualquier valor positivo, aunque la mayoría de las veces tomará solo valores enteros positivos. En la figura 8.4 se muestra esta distribución para \\(n = 1, 2, 5\\), y \\(8\\): a partir de \\(n=3\\) aparece un pico en la función, el cual se desplaza a valores mayores a medida que \\(n\\) aumenta. Figure 8.4: Función de densidad de una variable aleatoria Ji-Cuadrada \\(\\chi^2(n)\\). Si \\(X\\) sigue una distribución Ji-Cuadrada, escribiremos \\[X \\sim \\chi^2(n)\\] Su función de distribución viene dada por: \\[F(x) = \\int_0^{x} \\frac{1}{\\gamma(n/2)} \\left(\\frac{1}{2}\\right)^{n/2}u^{n/2 - 1}e^{-u/2}du\\] cuyo gráfico se muestra a comtinuación para una v. a. \\(X \\sim \\chi^2(n = 8)\\): Figure 8.5: Función de distribución acumulada de una variable aleatoria \\(\\chi^2(n = 8)\\). Es posible obtener valores de probabilidad acumulada en R usando el comando pchisq, el cual da \\(F(x) = P(X \\le x)\\). Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = n \\\\ &amp; Var(X) = 2n \\end{aligned}\\] Antes de ver un ejemplo de calculo de porbabilidades a partir de la función de distribución, es bueno conocer los siguientes resultados. La distribución Ji-cuadrada se puede obtener como resultado de elevar al cuadrado una variable normal estándar. Si \\(X \\sim N (0, 1)\\), entonces: \\[X^2 \\sim \\chi^2(1)\\] Este resultado, junto con la siguiente proposición, nos permitirán entender la distribución de v. a. con distribución Ji-Cuadrada. Sea \\(X\\) y \\(Y\\) dos v. a. independientes con distribución \\(\\chi^2(n)\\) y \\(\\chi^2(m)\\), respectivamente. Entonces: \\[X + Y \\sim \\chi^2(n + m)\\] Este resultado se puede extender a la suma de \\(n\\) variables aleatorias independientes distribuidas como Ji-cuadrado. Esto nos dice que podemos entender cualquier variable aleatoria que sigue una distribución Ji-Cuadrado como una suma de v. a. con la misma distribución pero cada una con un solo grado de libertad, cada una de las cuales se entiende como una v. a. normal estándar al cuadrado. Se nos permite obtener el siguiente resultado que utilizaremos más adelante cuando hablemos de inferencia estadística, y que es tan importante para realizar inferencia sobre la varianza. Sean \\(X_1, \\ldots, X_n\\) variables aleatorias independientes, cada una de ellas con distribución \\(N(\\mu, \\sigma^2)\\). Entonces: \\[\\frac{(n - 1)S^2}{\\sigma^2} \\sim \\chi^2(n - 1)\\] donde \\(S^2 = \\frac{1}{n - 1}\\sum_{i=1}^n(X_i - \\bar{X})^2\\) y \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\). Veamos un ejemplo. Por ejemplo, se sabe que en madres que no consumieron cocaína al nacer, el peso de los bebés nacidos tiene una desviación entándar de \\(\\sigma = 696\\) g. En un estudio de los efectos que tiene el consumo de cocaína sobre los bebés durante el embarazo, se recolectaron datos de \\(n = 190\\) madres consumidoras de cocaína. Por la proposión anterior, sabemos que la variabilidad en el peso de los bebés con respecto al valor conocido se distribuye como una \\(\\chi^2(n - 1)\\). Podemos calcular la probabilidad de que la variabilidad sea la mitad del valor conocido de \\(696\\) g, calculando la variable aleatoria \\((190 - 1) \\frac{(1/2)\\sigma^2}{\\sigma^2} = 189/2 = 94{,}5\\), cuya probabilidad se puede calcular en R como pchisq(94.5, 189) que arroja un valor de 0 (se muestra en la gráfica de la izquierda de la figura 8.6). Si la variabilidad es aproximadamente 8% menor al valor conocido, entonces \\((190 - 1) \\frac{0{,}92\\sigma^2}{\\sigma^2} = 173{,}88\\), entonces se calcula pchisq(173.8, 189) que arroja 0.2222 (que se muestra en la gráfica de la derecha de la figura 8.6) Figure 8.6: Función de densidad de una variable aleatoria \\(\\chi^2(n = 189)\\), donde se muestra de forma grafica la probabilidad acumulada \\(P(X \\le 94{,}5)\\) (a la izquierda) y \\(P(X \\le 173{,}88)\\) (a la derecha). "],["distribución-t-student..html", "8.3 Distribución \\(t\\)-Student.", " 8.3 Distribución \\(t\\)-Student. Se dice que un variable aleatoria continua \\(X\\) sigue una distribución \\(t\\)-Student con \\(n\\) grados de libertad (\\(n&gt;0\\)), si su función de densidad viene dada por: \\[f(x) = \\frac{\\Gamma(\\frac{n + 1}{2})}{\\sqrt{n\\pi}\\Gamma(n/2)}\\left(1 + \\frac{x^2}{n}\\right)^{-(n+1)/2}, \\quad -\\infty &lt; x &lt; \\infty\\] y se escribe: \\[X \\sim t(n)\\] en donde \\(n\\) es un número real positivo, aunque tomaremos principalmente el caso cuando \\(n\\) es entero positivo. Figure 8.7: Función de densidad de una variable aleatoria \\(t\\)-Student \\(t(n)\\), para \\(n = 1\\) (línea sólida), \\(n = 4\\) (línea quebrada) y \\(n = 15\\) (línea punteada). En rojo se muestra la distribución normal estándar. La función de densidad es de campana como la normal, pero con colas más pesadas que esta última, esto es, la probabilidad de obtener una observación extrema es mayor que la probabilidad de esa misma observación proviniendo de una distribución normal. A medida que aumentan los grados de libertad, la amplitud de las colas disminuye y la distribución se aproxima a una normal, y en el límite cuando \\(n\\rightarrow\\infty\\), ambas densidades coinciden. La función de distribución tampoco tiene una expresión sencilla y se escribe como: \\[F(x) = \\int_{-\\infty}^\\infty \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi}\\Gamma(n/2)} \\left(1 + \\frac{u^2}{n}\\right)^{-(n+1)/2} du\\] cuyo gráfico se muestra a continuación para una \\(t\\)-Student de \\(4\\) grados de libertad. Figure 8.8: Función de distribución acumulada de una variable aleatoria \\(t(n = 4)\\). Es posible obtener valores de probabilidad acumulada en R usando el comando pt, el cual da \\(F(x) = P(X \\le x)\\) (vea el ejemplo al final de esta subsección). Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = 0, \\quad n &gt; 1 \\\\ &amp; Var(X) = \\frac{n}{n - 2}, \\quad n &gt; 2 \\end{aligned}\\] Esta distribución resulta cuando se estudian ciertas operaciones entre variables aleatorias. Un resultado que usaremos seguido en inferencia estadística es el siguiente: Si \\(X \\sim N(0,1)\\) y \\(Y \\sim \\chi^2(n)\\) son dos variables aleatorias independientes, entonces: \\[\\frac{X}{\\sqrt{Y/n}} \\sim t(n)\\] También se puede llegar al siguiente resultado: Sean \\(X_1, \\ldots, X_n\\) v. a. independientes, cada una de ellas con distribución normal \\(N(\\mu, \\sigma^2)\\). Entonces: \\[\\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t(n - 1)\\] donde \\(S^2 = \\frac{1}{n - 1}\\sum_{i=1}^n(X_i - \\bar{X})^2\\) y \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\). Veamos un ejemplo. Ejemplo. Un programa que busca probar la eficacia de productos microbiológicos por su capacidad de degradar contaminantes derivados del petroleo (de tal forma que se puedan limpiar los ecosistemas que sufren el efecto del derrames amplios en las costas) es puesto en marcha. Investigaciones iniciales muestran que el producto es capaz de disminuir la cantidad de derivados del petroleo en un promedio de \\(10\\) ppm. Las pruebas del producto, basadas en una muestra de \\(5\\) recolecciones al azar de agua contaminadas y se les aplica el producto y se encuentra una variabilidad en la apcidad de degradación de \\(5{,}2\\) ppm. La proposición anterior entoncs muestra que la variable aleatoria \\[\\frac{\\bar{X} - 10}{5{,}2/\\sqrt{5}} \\sim t(4)\\] Por lo que podemos calcular la probabilidad de que al aplicar el producto se reduzca la contamianción por petroleo a \\(5\\) ppm, introduciendo este valor en la ecuación anterior, obteniendose \\((5 - 10)/5{,}2/\\sqrt{5} = -0{,}4300\\). Luego usamos la función pt como en pt(-0.43, 4) que arroja un valor de 0.3447 (como se ve en la figura 8.9, a la izquierda). Si queremos la probabilidad de que la contaminación solo descienda hasta \\(20\\) ppm como mínimo, entonces se calcula \\((20 - 10)/5{,}2/\\sqrt{5} = 0{,}86\\), cuya probabilidad se puede calcular como 1 - pt(0.86, 4), cuyo valor es 0.7809 (como se ve en la figura 8.9, a la derecha). Figure 8.9: Función de distribución acumulada de una variable aleatoria \\(t(n = 4)\\). "],["distribución-f..html", "8.4 Distribución \\(F\\).", " 8.4 Distribución \\(F\\). Se dice que la variable aleatoria continua \\(X\\) tiene una distribución \\(F\\) de Fisher-Snedecor con \\(a &gt; 0\\) y \\(b &gt; 0\\) grados de libertad si su función de densidad viene dada por: \\[f(x) = \\begin{cases} \\frac{\\Gamma(\\frac{a + b}{2})}{\\Gamma(\\frac{a}{2})\\Gamma(\\frac{b}{2})}\\left(\\frac{a}{b}\\right)^{a/2}x^{a/2 - 1}\\left(1 + \\frac{a}{b}x\\right)^{-(a + b)/2} &amp; \\text{si }x &gt; 0 \\\\ 0 &amp; \\text{de otra forma.} \\end{cases}\\] y se escribe \\[X \\sim F(a, b)\\] La gráfica de \\(f(x)\\) se muestra a continuación. Figure 8.10: Función de densidad de una variable aleatoria \\(F(a, b)\\), para difernetes combinaciones de los parámetros. Es posible obtener valores de probabilidad acumulada en R usando el comando pf, el cual da \\(F(x) = P(X \\le x)\\), que no tiene una expresión sencilla reducida. Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = \\frac{b}{b - 2}, \\quad b &gt; 2 \\\\ &amp; Var(X) = \\frac{2b^2(a + b - 2)}{a(b - 2)^2(b - 4)}, \\quad n &gt; 4 \\end{aligned}\\] La distribución \\(F(a, b)\\) aparece como resultado de realizar operciones entre variables aleatorias con distribución Ju-Cuadrada, como se muestra en la siguiente proposición. Sean \\(X\\) y \\(Y\\) dos variables aleatorias independientes con distribución \\(\\chi^2(a)\\) y \\(\\chi^2(b)\\), respectivamente. Entonces: \\[\\frac{X/a}{Y/b} \\sim F(a, b)\\] "],["ejercicios..html", "8.5 Ejercicios.", " 8.5 Ejercicios. Se ha realizado un estudio sobre la velocidad en vuelo de diversas especies de pájaros. El propósito era comparar las velocidades de dos especies de aves relacioandas entre sí, pero que se ha visto tienen tiempos de vuelo similares, pero una de ellas es más variable que la otra dada la longitud mas variable de la envergadura de las alas. Estudios previos muestran que la velocidad de vuelo de la especie con tiempos de vuelo menos variable, volando con el viento de costado con una velocidad de viento de \\(5\\) a \\(8\\) millas h\\({}^{-1}\\), en promedio, es \\(26{,}05 \\pm 3{,}20\\) millas h\\({}^{-1}\\). Si se tomará una muestra de \\(12\\) individuos de esta población, determine la probabilidad de que la variabilidad a) sea mayor a \\(6\\) milla h\\({}^{-1}\\), b) sea menor a \\(1\\) milla h\\({}^{-1}\\), c) este entre \\(2{,}5\\) y \\(4{,}3\\) millas h\\({}^{-1}\\). Un proceso industrial nuevo quiere lograr conseguir producir biocombustible a base de microalgas. Para hacer rentable esto, se requiere conseguir una producción neta de biomasa de microalgas de \\(5{,}6\\) g L\\({}^{-1}\\). a) Determine la probabilidad de lograr alcanzar esta cantidad de biomasa o más, si el proceso actual genera en promedio solo \\(2{,}5 \\pm 0{,}89\\) g L\\({}^{-1}\\), asumiendo que la distribución de bioamsa es una normal. Un cambio en las condiciones de cultivo han logrado aumentar el rendimiento del proceso a \\(4{,}3 \\pm 0{,}78\\) g L\\({}^{-1}\\). Determine la probabilidad de b) alcanzar, al menos, el valor de \\(5{,}6\\) g L\\({}^{-1}\\) bajo este nuevo escenario, c) de lograr una producción de biomasa entre \\(4{,}0\\) y \\(5{,}6\\) g L\\({}^{-1}\\). Se han desarrollado aortas artificiales a base de celulosa que requieren tengan un diametro de interno de \\(20\\) mm para que sea efectiva. A partir de una muestra de \\(5\\) aortas producidas se encuentra que el diametro medio es de \\(18{,}2 \\pm 0{,}9\\) mm. ¿Cuál es la probabilidad de que a) el diámetro sea de, al menos, el valor requerido?, b) el diámetro este entre \\(19\\) y \\(23\\) mm?, c) el diámetro se como mínimo de \\(17\\) mm? La productividad de un proceso de cultivo de papás hidropónico en una planta de producción nueva en Valencia es de \\(27 \\pm 5\\) kg semanales. Calcule la probabilidad de a) que la productividad sea mayor a \\(30\\) kg semanales, b) que la productividad este entre los \\(25\\) y \\(30\\) kg semanales, c) que la producción sea menor a los \\(15\\) kg. Asuma que la productividad semanal sigue una distribución normal. En estudios de herencia, es posible saber si un carácter se hereda de forma autosómica o sexual, simplemente estudiando la progenie de un primer entrecruzamiento, que denominamos F1, verificando si la proporción de individuos que presentan los caracteres siguen las proporciones mendelianas \\(3:1\\). Para ello, se hace uso de la variable aleatoria: \\[X^2 = \\sum_{i=1}^k \\frac{d_i^2}{e_i} = \\sum_{i=1}^k \\frac{(o_i - e_i)^2}{e_i}\\] la cual se distribuye como una chi-cuadrado con \\(k-1\\) grados de libertad, donde \\(k\\) es el número de clases (o fenotipos) expresados por el gen considerado, y \\(e_i\\) es el valor esperado. En un experimento clásico, se contabilizaron el número de semillas rugosas y lisas obtenidas en F1 luego de un entrecruzamiento de semillas lisas y rugosas. El número de semillas lisas contabilizado fue de \\(384\\) semillas lisas, y \\(118\\) semillas rugosas. ¿Cuál es la probabilidad de que el caracter se transmita de forma autosómica? "],["inferencia-estadística..html", "9 Inferencia Estadística.", " 9 Inferencia Estadística. La inferencia es la parte de la estadística que se encarga de formalizar el proceso de estimación y contrastes de hipótesis. Su problema fundamental consiste en poder derivar declaraciones acerca de un fenómeno natural de interés a partir de observaciones realizadas del fenómeno. Las declaraciones de las que se hablan, son declaraciones en un sentido estadístico: esto es, las declaraciones se establecen con cierto grado de veracidad. No son verdades universales, sino que están sujetas a errores. El problema de la inferencia estadística es cuantificar que tan seguro estamos sobre esas declaraciones. La razón de que las declaraciones estén sujetas a variabilidad tiene que ver con las observaciones que realizamos del fenómeno. Las observaciones se usan para realizar inferencias acerca de las características o propiedades particulares del fenómeno en estudio. Estas observaciones no son perfectas y están limitadas a los recursos que posee el investigador para llevarlas a cabo: No son perfectas ya que cualquier medición está sujeta a que tan preciso es el instrumento con el que medimos (eso incluye nuestros sentidos). Además, las observaciones contienen una variabilidad inherente que es debida solo al azar. Esto hace que se tenga cierta incertidumbre al realizar mediciones, que son desviaciones aleatorias del valor real de lo que se está midiendo. Son limitadas dado que no se tiene siempre el dinero, el tiempo, o la energía para recolectar toda la información disponible. Esto hace que no se disponga siempre de toda la información que pueda ayudarnos a estudiar un fenómeno particular, sino que solo un subconjunto de esa información. En conclusión, las observaciones realizadas del fenómeno en cuestión contienen variación aleatoria que hacen imposible el observar directamente la característica o propiedad que se está estudiando, y dado que las declaraciones derivan de estas observaciones, se necesita cuantificar esta variabilidad/incertidumbre, necesitándose así modelos estocásticos para poder tratar con esta variación. Es por ello que se hace necesario de modelos estadísticos con los cuales manejar los datos. Estos modelos se originan de la matemática deductiva (aquellos que comienzan con teorías generales y que, por argumentos lógicos, se llega a conclusiones específicas), pero no necesariamente son los correctos, y esto hace que estén sujetos a incertidumbre. Obtener información (observaciones) no nos permite decir que modelo es el correcto. Simplemente no sabemos: al realizar inferencia y obtener declaraciones de estas, asumimos un modelo correcto y analizamos los datos bajo esta premisa, y toleramos/soportamos la posibilidad de caer en un error debido a una mala elección del modelo. Por ejemplo, al hablar de inferencia en los próximos capítulos, estaremos suponiendo que la distribución subyacente a los datos en una distribución normal. Esta suposición bajo la cual analizamos los datos y hacemos contrastes puede no ser la correcta, por lo que cualquier conclusión que derive de esas pruebas puede ser errada. Podríamos decidir usar otro modelo, otra distribución subyacente a partir de la cual hacer inferencias, pero aun así, este modelo podría no ser correcto de todos modos. Podemos cuantificar que tanto podemos aceptar la suposición de partida, pero estas decisiones también estarían sujetas a incertidumbre. Es un trade-off entre la necesidad de analizar los datos y la probabilidad de caer en un error debido a esa elección de un modelo. Esta incertidumbre de la que hablamos en el último apartado es lo que se conoce como incertidumbre inductiva y es esto lo que hace que los problemas estadísticos sean inductivos: se parte de las observaciones realizadas sobre una característica/propiedad que no podemos observar directamente al realizar un experimento. Es esta incertidumbre la que hace a las declaraciones derivadas de la inferencia, falibles. Para puntualizar, decimos que existen dos tipos de incertidumbre: Incertidumbre estocástica: es aquella que está relacionada a la aleatoriedad de las observaciones, y la capacidad de estas de dar información sobre parámetros fijos. Se puede manejar al aumentar el tamaño del experimento. Incertidumbre inductiva: se debe a que la información es incompleta al elegir un modelo. Aunque la anterior es fácil de manejar, esta no. Puede ser imposible cuantificarla o controlarla. La idea general de la inferencia es poder cuantificar la incertidumbre estocástica y explicar la variabilidad observada en los datos, pero el mecanismo subyacente no es tan importante de explicar. El problema es que la incertidumbre inductiva tiende a incrementar la incertidumbre estocástica, pero siempre podemos realizar análisis hasta tener un razonable control sobre esta última. Esta distinción entre tipos de incertidumbre y el manejo de ambas, es lo que hace que diferentes investigadores puedan llegar a distintas conclusiones. "],["cómo-se-enfrenta-a-la-inferencia-estadística.html", "9.1 ¿Cómo se enfrenta a la Inferencia Estadística?", " 9.1 ¿Cómo se enfrenta a la Inferencia Estadística? Básicamente, las corrientes de pensamiento tienen que ver sobre cómo se plantea la visión de probabilidad: Como la frecuencia esperada a la larga (luego de repetir el experimento muchas veces); o Como una noción subjetiva de incertidumbre. Por ejemplo: Si lanzamos una moneda, tenemos un sentido de incertidumbre acerca del resultado: decimos que la probabilidad de obtener una cara es de \\(0{,}5\\). Ahora, pensemos en el siguiente lanzamiento: ¿podemos decir que la incertidumbre sobre el resultado sigue siendo \\(0{,}5\\)?¿o la probabilidad de \\(0{,}5\\) solo tiene sentido a la larga? Si obtenemos cara durante el primer lanzamiento, la probabilidad de obtener otra cara, dado que el resultado anterior, viene dado por el teorema de Bayes. Sin embargo, el uso de este teorema requiere que especifiquemos la distribución de probabilidad a priori, y es aquí donde está el problema entre los frecuentistas y bayesianos: Los frecuentistas, estresarían el hecho de que lo que importa es la probabilidad a la larga, sin importar cuanta información tengamos a partir de los datos. Por lo que la distribución de probabilidad subyacente es \\(0{,}5\\) para ellos. Los bayesianos podrían concordar con los frecuentistas, pero también podrían inclinarse a darle importancia a la información que se tiene actualmente. Al haber solo un lanzamiento, la distribución binomial asigna una probabilidad a priori distinta de \\(0{,}5\\) para el siguiente lanzamiento. En este curso solo hablaremos de estadística en un sentido frecuentista, para mayor información sobre el enfoque bayesiano puede consultar @gelman1995 (otro enfoque posible es uno intermedio entre ambas posturas, frecuentista y bayesiana, que se basa en la idea de una probabilidad fiduciaria o función de verosimilitud. Para más información sobre este enfoque pueden consultar @pawitan2001all). Las dos problemáticas principales para un frecuentista son: La elección de una distribución a priori apropiada. El desacuerdo sobre el accionar bajo grados de creencias personales. Sin embargo, la incertidumbre inductiva es mayor por varios ordenes de magnitud a cualquier diferencia en la incertidumbre estocástica que resulta de analizar datos siguiendo el criterio de cualquier escuela de pensamiento. "],["frecuentistas-y-muestreo-repetido..html", "9.2 Frecuentistas y muestreo repetido.", " 9.2 Frecuentistas y muestreo repetido. Los frecuentistas, ven la probabilidad como una frecuencia a la larga, suponiendo que un experimento se repite, de forma hipotética, muchas veces. Es decir, se basa en el principio de muestreo repetido bajo las mismas condiciones. Para ellos, cualquier parámetro de importancia es fijo, y no puede tratarse como una variable aleatoria. Se trata de entender la relación entre el estimado de una propiedad que podemos calcular y el valor real de esa propiedad, imaginándonos como podría el resultado cambiar si en lugar de la muestra seleccionada, se hubiese recolectado otra igualmente probable. Imagine que desea conocer el peso promedio de una población de patos negros, que se distribuye como \\(N(1161\\text{ g}, 9604\\text{ g}^2)\\) (no necesariamente usted ssbe qje esta es la fistribución y, en general, no lo ssbe. Solo usamos este conocimiento para poder ralizqr simulaciones en este ejemplo), por lo que se toma una muestra aleatoria de \\(n = 50\\) patos. En esta muestra, encuentra que el peso promedio es \\(\\hat{\\mu} = 1158{,}2\\) g. Este peso promedio es una estimación del peso promedio verdadero de la población de \\(\\mu=1161\\) g. Ahora, supongamos que se realiza este experimento muchas veces, unas 10 mil veces digamos, y en cada repetición calculamos el peso promedio. En R, podemos lograr hacer esto de la siguiente forma: # Se asume media poblacional de 1161 g y desviacion estandar poblacional de 98 g bd_wieghts &lt;- tibble(duck_id = 1:2300, weight = rnorm(2300, 1161, 98)) # Se realizan 10000 muestreos aleatorios de 50 observaciones de la población. virtual_samples &lt;- bd_wieghts %&gt;% rep_sample_n(size = 50, reps = 10000) # A cada replica se le calcula el peso promedio virtual_bd_weights &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(`Peso Promedio` = mean(weight)) virtual_bd_weights %&gt;% head(10) %&gt;% kbl() replicate Peso Promedio 1 1173.352 2 1155.812 3 1171.502 4 1177.114 5 1156.068 6 1165.478 7 1176.597 8 1157.393 9 1157.340 10 1171.491 Se puede observar de la tabla anterior, que algunas muestras resultan en un peso mayor al valor real de \\(1161\\) g, y otras en valores menores; y si inspeccionáramos cuidadosamente los resultados, podríamos verificar si de hecho el peso promedio en alguna replica es igual al valor de peso verdadero. Cada uno de esos promedios corresponde a un estimador \\(\\hat{\\mu}\\) del verdadero valor medio \\(\\mu\\). Podríamos realizar un histograma de estos valores y veríamos que el verdadero valor esta en el centro de la distribución de valores medios obtenidos de las 10 mil replicas. # Se realiza un grafico de los valores promedios ggplot(virtual_bd_weights, aes(x = `Peso Promedio`)) + geom_histogram(binwidth = 0.1, boundary = 0.4, color = &quot;#213555&quot;) + geom_vline(xintercept = 1161, colour = &quot;white&quot;, linewidth = 1.2) + labs(x = &quot;Peso de los patos negros&quot;, y=&quot;Conteo&quot;, title = &quot;Distribution of 10,000 realizaciones de medidas de peso.&quot;) + theme_light() + theme(panel.grid = element_blank()) Figure 9.1: Distribución muestral de la media \\(\\bar{X}\\) generada por simulación. y calcular el valor promedio de los pesos promedios, que arroja un valor de 1162.19, el cual esta razonablemente en acuerdo con el valor verdadero (el valor real difiere, porque el número de replcias es finito). El gráfico del ejemplo anterior es la distribución muestral del estadísitco \\(\\hat{\\mu}\\). Esta distribución describe la variabilidad de los promedios calculados a partir de las replicas alrededor de la media verdadera \\(\\mu\\). Más adelante (en el capitulo Teoría de Muestreo.) estudiaremos esta distribución, pero en este punto, es importante notar que las propiedades de los estimadores que estudiaremos estan basados en el muestreo repetido hipotético (teoría frecuentista), que permite justificar el comportamiento de estos estimadores: esto es, nos permite derivar y justificar el uso de una distribución de porbabilidad para estos estimadores y, por lo tanto, justificar los resultados de inferencia estadística. "],["ámbito-de-la-inferencia-estadística..html", "9.3 Ámbito de la inferencia Estadística.", " 9.3 Ámbito de la inferencia Estadística. A la inferencia estadística le conciernen 4 tipos de problemas, de los cuales se buscan soluciones que permitan realizar estudios experimentales que lleven a conclusiones relevenates y significativas acerca de un fenómeno en estudio. Estos problemas son: Procesos de recolección de muestras. Hace referencia a la sistematización y correcta cosntrucción de los conjujntos muestras, de forma que sean representativos de la población que se está estudiando, disminuyendo cualquier sesgo en la elección de elementos que conformen la muestra. Estimación puntual y a intervalos. Hace referencia al proceso de hacer inferencia o predicciones acerca de los parámetros de una población, que estan ocultos a nosotros, pero de los cuales queremos precisar su valor a partir de una muestra, tolerando cierto grado de error, que debe cuantificarse. Contraste de hipótesis. Esta hace referencia a un proceso de toma de decisiones acerca de la veracidad estadística de una proposición que se formula como hipótesis de un experimento. Diseño experimental. Se refiere a la forma en la que se planea un experimento para poder realizar inferencias por medio de estimaciones y contrastes de hipótesis, y su correcto establecimiento determina la validez de las conclusiones que se obtienen del proceso de inferencia. En los proximos capitulos estudiaremos cada uno de estos puntos para diferentes posibles experimentos, haciendo mucho emfásis en el contraste de hipótesis. "],["teoría-de-muestreo..html", "10 Teoría de Muestreo.", " 10 Teoría de Muestreo. La teoría de muestreo lidia con el problema de construir un conjunto muestra a partir de un conjunto población. La validez de las conclusiones que se establecen sobre la población dependen de si la muestra se seleccionó de tal forma que representa a la población correctamente. En este punto, necesitamos definiciones precisas de población y muestra: Población: colección de individuos o unidades en la que estamos interesados, y que tiene una ley o distribución de probabilidad asociada. Se denota el tamaño poblacional (el número total de individuos) como \\(N\\). Muestra: es una colección de individuos o unidades tomados de una población. Se denota el tamaño de la muestra (el número total de individuos recolectados de la población) como \\(n\\). En general, \\(n &lt; N\\). Una muestra se puede recolectar de una poblacion por medio de un censo o por muestreo aleatorio. El primero se refiere a un muestreo exhaustivo de todos los individuos que conforman la población. El segundo hace referencia a un método que asegura la recolección aleatoria de elementos del conjunto población para construir un conjunto muestra más pequeño. Dado que la población tiene asociada una ley de probabilidad, se especifica para esta los parámetros de esa ley de probabilidad, y es sobre estos que haremos inferencias, por medio de estimadores. Hasta este punto, henos estado hablando sobre estimadores como valores que calculamos para tratar de saber el verdadero valor de una propiedad o caracteristica, y hemos hablado de parametros como esos valores verdaderos que estan ocultos a nosotros. Definimos ahora con precisión estos términos: Un parámetro es un valor numérico que logra resumir la información sobre una población. En casi todos los casos, este valor es completamente desconocido, pero se busca conocer su valor. Un estimador es un estadístico (es decir, tiene una distribución asociada), un valor numérico que ayuda a resumir la información sobre una muestra y se usa para estimar un parámetro poblacional desconocido. Notación. De forma general, se denota como \\(\\theta\\) al parámetro (o \\({\\boldsymbol\\theta}\\), en notación vectorial, si es más de un parámetro). Pero dependiendo de la ley de probabilidad, se puede especificar otra notación particular. Por ejemplo, es usual llamar a la media poblacional con la letra \\(\\mu\\), a la varianza poblacional con la letra \\(\\sigma^2\\), a las proporciones poblacionales con la letra \\(\\pi\\), entre otras. Si la media poblacional es el único parámetro desconocido, entonces \\(\\theta = \\mu\\). Pero si ambos, media y varianza son desconocidos, se resumen los parámetros con la notacion vectorial usual: \\[{\\boldsymbol\\theta} = \\begin{pmatrix} \\mu \\\\ \\sigma^2 \\end{pmatrix}\\] Ya nos hemos encontrado antes con estimadores y parámetros. En los capítulos de distribuciones de probabilidad (capítulos blah blah blah) definimos leyes de probabilidad discreta y continua de interés, y definimos allí los parámetros de los que dependen. Por ejemplo, para una distribución normal, el parámetro media poblacional se denota como \\(\\mu\\) y la desviación estándar poblacional se define como \\(\\sigma\\). Es usual hacer uso de letras griegas para definir parámetros poblacionales, pero por simplicidad, algunas veces se hace uso de notaciones alfanuméricas usuales. Por ejemplo, para la distribución binomial, la probabilidad de éxito la denotamos como \\(p\\). Sin embargo, también es usual hacer uso de la notación \\(\\pi\\) para describir este parámetro, de forma que no haya confusiones al trabajar con notación de probabilidades. Por otro lado, ya hemos encontrado antes estiamdores usuales: en el capítulo Estadística descriptiva. definimos el promedio, la varianza, la desviación estándar, y otras propiedades con que resumir la información contenida en los datos. Estos son ejemplos de estimadores. Se pueden denotar de dos formas: Usando la letra griega correspondiente al parámetro poblacional, pero colocandole un sombrero. Por ejemplo, podríamos denotar al estimador de la media poblacional \\(\\mu\\) como \\(\\hat{\\mu}\\), al estimador de la desviación estándar poblacional \\(\\sigma\\) como \\(\\hat{\\sigma}\\), y así sucesivamente. Usando letras alfanuméricas para los estimadores. Por ejemplo, al estimador de la media poblacional \\(\\mu\\) es usual denotarlo como \\(\\bar{X}\\), al estimador de la desviación estándar poblacional \\(\\sigma\\) como \\(S\\), y así sucesivamente. En los capítulos siguientes haremos lo posible por adherirnos a la notación de sombreritos donde sea conveniente, pero en algunos casos haremos uso de la notación usual por conveniencia. El contexto se hace claro dependiendo del caso, y se aclarará donde sea necesario en los lugares donde pueda ser causa de confusión. "],["estimación..html", "10.1 Estimación.", " 10.1 Estimación. La estimación consiste en hacer inferencias o predicciones sobre los parámetros de una población, los cuales están ocultos a nostoros, usando la información contenida en una muestra. La estimación puede ser de dos tipos: Estimación puntual: una estimación puntual es un estimador que se calcula a partir de una sola muestra particular. Este tiene tres propiedades esenciales: debe ser i) eficiente, ii) consistente, y iii) suficiente. Estimación por intervalos: son un par de estimadores que definen los límites de un intervalo, que se calculan a partir de una muestra, y se espera que contenga el parámetro que esta siendo estimado. Los estimadores, sea cuales sean, deben construirse. La construcción de estos debe ser tal que estos tengan varias propiedades deseables para hacer inferencia: no deben estar sesgados, lo cual se garantiza al emplear métodos de muestreo que permitan tomar muestras aleatorias. Además, deben elegirse de tal forma que sean de mínima varianza, y deben ser consistentes, esto es, a medida que aumente el esfuerzo de muestreo, el estimador debe ir acercandose cada vez más al valor del parámetro. Cualquier proceso de estimación comienza con la suposición de un modelo estocástico que se asume correcto (como se discute en el capítulo [Infererncia estadística.]) una vez recolectada la muestra. Este modelo es una función de densidad \\(f(x; \\theta)\\) que resume la forma en la que se generan las observaciones (la notación hace referencia a que \\(f\\) es una función de las observaciones \\(x\\), dados los parámetros \\(\\theta\\)). En capítulos anteriores hemos dicho que si una v. a. \\(X\\) sigue una distribución \\(f(x)\\), se escribe \\(X \\sim f(x; \\theta)\\). Cuando recolectamos una muestra, tenemos \\(n\\) observaciones \\(x_1, x_2, \\ldots, x_n\\) de una variable alestoria \\(X\\) en estudio. Sin embargo, resulta más conveniente entender a las observaciones como generadas por \\(n\\) variables aleatorias: \\(X_1\\) genera la observación \\(x_1\\), \\(X_2\\) genera la observación \\(x_2\\), \\(\\ldots\\), \\(X_n\\) genera la observación \\(x_n\\), donde cada v. a. se distribuye con la misma ley de probabilidad \\(f(x, \\theta)\\). En este caso, se dice que las \\(n\\) v. a. se distribuyen identicamente. Si además, estas variables son independientes entre sí, se dicen que las \\(n\\) v. a. son independiente e identicamente distribuidas, que se abrevia como iid, y se escribe: \\[X_i \\overset{iid}{\\sim} f(x_i; \\theta)\\text{ para }i = 1, 2, \\ldots, n\\] que se lee como: las variable aleatorias son independientes e identicamente distribuidas como \\(f(x; \\theta)\\). Ahora, si bien cada variable aleatoria tiene su ley de probabilidad, la muestra en su totalidad tiene una ley de probabilidad que se deriva de la ley de probabilidad de las v. a. individuales. La función de probabilidad conjunta de las \\(n\\) v. a. iid viene dada por: \\[f(X_1, \\ldots, X_n; \\theta) = f(x_1; \\theta) f(x_2; \\theta)\\ldots f(x_n; \\theta)\\] Ejemplo. Digamos que se recolecta una muestra de \\(n\\) observaciones generadas por las v. a. \\(X_1, \\ldots, X_n\\) que son iid como \\(N(\\mu, \\sigma^2)\\). Entonces la función de probabilidad conjunta es: \\[\\begin{aligned} f(x_1, x_2, \\ldots, x_n; \\mu, \\sigma^2) &amp;= f(x_1; \\mu, \\sigma^2) f(x_2; \\mu, \\sigma^2)\\ldots f(x_n; \\mu, \\sigma^2) \\\\ &amp;= \\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_1 - \\mu)^2}{2\\sigma^2}}\\right)\\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_2 - \\mu)^2}{2\\sigma^2}}\\right)\\ldots\\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_n - \\mu)^2}{2\\sigma^2}}\\right) \\\\ &amp;= \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\sum_{i=1}^n\\frac{(x_i - \\mu)^2}{2\\sigma^2}} \\end{aligned}\\] donde se usó las propiedades de producto de potencia con igual base. Al expandir la potencia y aplicar propiedades de sumatoria (tarea sencilla que puede verificar usted mismo) se obtiene que: \\[f(x_1, x_2, \\ldots, x_n; \\mu, \\sigma^2) = \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\frac{(\\bar{X} - \\mu)^2}{2\\sigma^2}}\\] y esta es la función de distribución conjunta para la muestra de tamaño \\(n\\) recolectada. El enfasis que se hace es que la muestra tiene una ley de probabilidad asociada que resulta de las suposiciones iniciales del modelo, y, por lo tanto, la inferencia es dependiente de estas suposiciones. También se trata de aclarar la notación que usaremos y que se encuentra frecuentemente en textos de estadística. "],["distribución-muestral-de-un-estimador..html", "10.2 Distribución muestral de un estimador.", " 10.2 Distribución muestral de un estimador. En el capítulo anterior construimos la distribución del estimador del peso promedio de una muestra de patos negros de tamaño \\(n\\) tomada de forma aleatoria. Dijimos que esta distribución de las medias obtenidas de cada una de esas muestras hipotéticas es conocida como distribución muestral y que esta muestra la variabilidad del estimador o los posibles valores que este puede tomar. Ahora profundizaremos en las propiedades de esta distribución que la hacen útil en inferencia estadística. La variabilidad de la distribución muestral depende del número de observaciones que componen a la población, del número de observaciones que componen a la muestra, y del procedimiento usado para tomar la muestra de la población. Esta variabilidad se puede cuantificar en una cantidad llamada error estándar, y a medida que aumenta el tamaño de la muestra tomada, menor es el error estándar. Esto se resume en el siguiente resultado: \\[SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\] donde \\(\\sigma\\) es la desviación estándar poblacional. Este resultado es cierto sin importar si la distribución subyacente de la población es normal o no. El valor de la desviación estándar poblacional esta oculto a nosotros, de la misma forma que lo esta la media poblacional, y por lo tanto, también se debe estimar a partir de la muestra. Y, al igual que con la media poblacional, diferentes muestran arrojaran diferentes posibles valores del estimador, y esta variabilidad se puede resumir en la distribución muestral de la desviación estándar. Así mismo, como con otro parámetros podemos obtener estimadores de muestras, entonces estos estimadores siempre tienen asociado una distribución muestral para describir su variabilidad, y esta última se cuantifica usando la medida de error estándar. Ejemplo. Volvamos al ejemplo de muestreo de patos negros para medir su peso total. Realizaremos simulaciones similares a la realizada antes, pero en lugar de repetir el experimento de tomar muestras miles de veces, el experimento de toma de muestra se hace una sola vez. Solo modificaremos el tamaño de la muestra obtenida de la población, de forma que no solo tomemos muestras de tamaño \\(50\\), sino que realizaremos la simulación usando muestras de tamaño \\(20\\), \\(100\\) y \\(1000\\) también. Los resultados se muestran en la figura 10.1. Figure 10.1: Dependencia de la distribución muestral con el tamaño de la muestra recolectada. Vemos que en el gráfico correspondiente a la muestra más pequeña, la variabilidad es bastante grande y la mayoría de los datos se concentran alrededor de la verdadera media \\(\\mu = 1161\\) kg. Pero se observan datos atípicos con frecuencias altas que sesgan mucho la forma de la distribución. Al aumentar el tamaño de la muestra recolectada, se puede observar que el sesgo va desapareciendo y las observaciones atípicas son cada vez menos frecuentes. De esta forma, podemos ver que el tamaño de la muestra afecta el calculo de cualquier estimador, haciendo que este este más o menos desviado del verdadero valor dependiendo del \\(n\\). Ésto es fácil de cuantificar si calculamos la desviación estándar en cada simulación (que se muestra en cada uno de los gráficos de la figura anterior) y luego calculamos el error estándar, como se muestra en el siguiente fragmento de código de R: size sample_size mean std_dev std_error small 20 1111.194 87.93181 19.662150 medium 50 1148.731 93.68288 13.248760 large 100 1154.611 89.12465 8.912465 very large 1000 1157.713 97.01411 3.067855 En la tabla vemos que nuestro estimador de la media y la desviación estándar se acercan cada vez más al valor real de \\(1161\\) kg y \\(98\\) kg, y que el error estándar es cada vez más pequeño a medida que aumenta el tamaño de la muestra. Esto quiere decir que, a medida que aumentamos el tamaño de la muestra, nuestros estimadores se acercan cada vez más al valor real, y se hacen cada vez más precisos. Este resultado se resume en uno de los teoremas más importantes de la teoría de probabilidades y estadística, la ley de los grandes números. La ley de los grandes números. Sea \\(X_1,X_2, \\ldots\\) una sucesión infinita de variables aleatorias independientes e idénticamente distribuidas con media finita \\(\\mu\\). Entonces, cuando \\(n \\rightarrow \\infty\\), \\[\\frac{1}{n}\\sum_{i=1}^{n}X_i \\rightarrow \\mu\\] en donde la convergencia se verifica en el sentido casi seguro (ley fuerte) y también en probabilidad (ley débil). Lo que dice el teorema es que, a medida que la muestra de la cual calculamos el estimador se hace más grande, el error en nuestra medida irá decreciendo más y más hasta converger al verdadero valor del parámetro (la convergencia casi segura y en probabilidad son formas de convergencia de una serie infinita de v. a. definidas en terminos de la probabilidad de que la convergencia se de es segura y de la probabilidad de las desviaciones tan pequelñas como se quiera del estimador y el parámetro es nula, respectivamente. Para más detalles, consulte @lehmann1999elements o @rincon2014introduccion). Aquí un comentario pertinente sobre la notación. Si bien usamos la letra griega \\(\\mu\\) en el teorema, misma letra que usamos para denotar la media poblacional de una distribución normal, no debemos pensar que el teorema solo es cierto oara este parámetro. En el teorema, se usa \\(\\mu\\) como notación más amplia de un parámetro verdadero cualquiera. Por ejemplo, si la variable aleatoría \\(X\\) son desviaciones estándar de la media, entonces el parámetro \\(\\mu\\) es el resltado de promediar todas las desviaciones estándar de todas las posibles muestras reclectadas de tamaño \\(n\\), este promedio es \\(\\sigma\\) y el teorema se escribiría: \\[\\frac{1}{n}\\sum_{i=1}^{n}X_i \\rightarrow \\sigma\\] "],["teorema-del-límite-central-tlc..html", "10.3 Teorema del Límite Central (TLC).", " 10.3 Teorema del Límite Central (TLC). Este teorema es el más importante de los que verémos en este libro. Este establce la distribución muestral de estadísticos construidos a partir de estimadores calculados de una muestra, y usa la ley de los grandes números para verificar que la ley de probabilidad del estadístico converge a una normal estándar. Sea \\(X_1, X_2, \\ldots\\) una sucesión infinita de variables aleatorias independientes e idénticamente distribuidas, con media \\(\\theta\\) y error estándar \\(SE(\\theta)\\). Entonces la función de distribución de la variable aleatoria: \\[Z_n = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)}\\] tiende a la función de distribución normal estándar cuando \\(n\\) tiende a infinito. El TLC establece que el estadístico \\(Z_n\\) tendrá una distribución normal estándar sin importar la distribución de las v. a. \\(X_1, X_2, \\ldots, X_n\\), para valores grnades de \\(n\\), aunque esto no implica que la muestra tendrá una forma de campana. En general, a mayor sea la muestra, más cercana esta estará de la verdadera distribución poblacional. Ahora, desde un punto de vista práctico, este teorema se cumple cuando \\(n \\ge 30\\), de forma que no se necesitan muestras demasiado grandes para poder justificar la normalidad al realizar inferencia. Cuando \\(n &lt; 30\\), la distribución muestral es más variables, haciendo que los estimadores de \\(\\theta\\) y \\(SE(\\theta)\\) sean menos precisos, y por lo tanto es mejor utilizar la distribución \\(t\\)-Student con \\(n - 1\\) grados de libertad. Ejemplo. Simulemos un conjunto de variables aleatoria \\(Z_n\\) para distintos valores de \\(n = 5, 10, 20, 30, 50, 100, 500, 1000\\). Para ellos, imaginemos un experimento donde se siembran \\(n\\) semillas y se registra despues de un tiempo, si la semilla germina o no. Denotamos un exito como la semilla no germnina, que se sabe tiene probabilidad de exito de \\(0{,}2\\). Con esta información, simulamos \\(1000\\) replicas del experimento para cada \\(n\\), y construimos un histograma sobre el cual dibujamos una curva de la función de densidad normal estándar para comparar. el gráfico muestra indudablemente que a medida que el \\(n\\) crece, el histograma de la distribución muestral se aproxima cada vez más a una normal estándar, y que el ajuste siempre es mejor cuando \\(n \\ge 30\\), y para valores menores a este, el ajuste no es tan bueno. Este ejemplo busca convencerlo de que el TLC es válido y aplicable a la hora de realizar inferenias. Pero también enfatiza la importancia de elegir un tamaño muestral aadecuado para que la suposición de normalidad tenga sentido de \\(Z_n\\) tenga sentido. En los próximos capítulos estaremos usando este teorema continuamente cuando derivemos la distribución muestral de los estadísitcos que cosntruiuremos para realizar estimaciones y contrastar hipóteis. Corrección por población finita. Para cualquier población estadística que consiste de \\(N\\) unidades, se define la corrección de población finita como: \\[1 - f = 1 - \\frac{n}{N}\\] Solo tiene importancia en poblaciones pequeñas, en las que \\(n &gt; 0{,}05\\times N\\). Modifica los estimadores de la desviación estandar. "],["necesidad-de-especificar-una-muestra..html", "10.4 Necesidad de especificar una muestra.", " 10.4 Necesidad de especificar una muestra. La especificación de una muestra requiere de varios pasos particulares, pero de forma invariable esta sujeta a la pregunta que se busca responder al realizar un experimento, y a la cantidad de esfuerzo y dinero que se puede invertir para realizar dicho experimento. El proceso de diseño y toma de muestra no es puramente matemático: Los objetivos del experimento deben especificarse, y esto involucra especificar variables que podrían ser importantes, el equipo de muestreo a usar, cuanto esfuerzo se puede invertir, … Toda esta información, es la que ayuda a elegir un método de estimación a usar. En este punto, se hace la pregunta: ¿Qué tan grande debe ser la muestra que voy a tomar? El tamaño de la muestra es muy importante para asegurar la representatividad de la población que se esta muestreando, pero también lo es para poder evaluar el efecto de un tratamiento al realizar un experimento (en, por ejemplo, diseños experimentales factoriales o por bloques, o análisis tan sencillos como pruebas \\(t\\)). La elección del tamaño de la muestra requiere de la especificación de: La prueba a utilizar. El nivel de significancia mínimo a usar (tradicionalmente, 1%, 5% o 10%). El tamaño del efecto a contrastar. La potencia deseada para la prueba (usualmente, 80%). La potencia tiene como valor el que su consideración obliga al investigador a pensar en términos de la fuerza de los efectos que su experimento es probable produzca. Aquí, la información a priori comienza a ser de vital importancia. Particularmente en el contraste de hipótesis El trabajo del investigador no es demostrar que un tratamiento no produce el mismo efecto que el control, es demostrar la efectividad del tratamiento. El tamaño de la muestra a usar tiene que ser tal que: * Permita determinar si un efecto dado (su magnitud) puede interpretarse como suficientemente confiable o válido como para que la comunidad científica acepte una hipótesis. * Permita determinar (o se determina tal que) que tan probable es que los datos de un estudio resulten en una significancia estadística antes de que el estudio se haya llevado a acabo. No solo es profesionalmente autodestructivo el diseñar experimentos que no tengan una alta probabilidad de éxito, sino que no es ético el hacerlo por la simple razón de que se consumen recursos escasos (monetarios, de esfuerzo o tiempo). "],["diseño-de-muestreo..html", "10.5 Diseño de muestreo.", " 10.5 Diseño de muestreo. El elegir un correcto método de muestreo es vital para obtener información representativa de la población. Se hace importante el diseño del muestreo: en cuanto al método (sistemático o aleatorio) y en cuanto al número de muestras. Muestreo aleatorio simple. Muestreo aleatorio estratificado. Muestreo adaptativo. Muestreo sistemático. Consideraciones. Debemos primero establecer de forma explícita la población estadística. Se debe especificar la unidad de muestreo. Luego se selecciona una muestra con algún plan particular. Cualquier estadístico asume que la muestra sigue los principios del muestreo probabilístico: Se define un conjunto de muestras distintas \\(S_1\\), \\(S_2\\), \\(\\ldots\\) en el que cierta unidad de muestreo especifica es asignado a \\(S_1\\), otro a \\(S_2\\), y así sucesivamente. A cada muestra \\(S_i\\) ( \\(i=1, 2, \\ldots\\) ) se le asigna una probabilidad de ser seleccionada. Se selecciona una de las muestras por la probabilidad adecuada, usando una tabla de números aleatorios. Claro, el muestreo puede no ser probabilístico: Muestreo de solo unidades accesibles. Muestreo influenciado por sesgos sensoriales. Influencia de prejuicios u otras subjetividades al muestrear unidades típicas El uso de solo voluntarios. 10.5.1 Muestreo Aleatorio Simple. 10.5.2 Muestreo Aleatorio Estratificado. La población de \\(N\\) individuos se subdivide en \\(h\\) subpoblaciones o estratos que no se solapen, de forma que \\(N = N_1 + N_2 + \\ldots + N_L\\). Cada estrato es entonces muestreado por separado, obteniéndose muestras \\(n_1\\), \\(n_2\\), \\(\\ldots\\), \\(n_L\\). La estratificación es recomendable cuando: * Se requieren estimadores de medias y varianzas separadamente para cada estrato. * La probabilidad de seleccionar una muestra varía de un área a otra. * Se necesita mayor precisión de un estimador. * La organización administrativa del equipo lo ve conveniente. ¿Cómo construir los estratos? * No deben exceder más de 6 estratos. * Basado en conocimiento a priori. * Basado en una variable a medir o controlar. * Las muestras pueden decidir estratificarse luego del proceso de recolección. 10.5.3 Muestreo Adaptativo. Hace uso de datos recolectados para realizar decisiones sobre el esfuerzo de muestreo. Puede ser de dos tipos: i) Muestreo adaptativo aglomerado (clusters): se realiza un muestreo aleatorio simple, como se describió antes. Si una o más unidades de muestreo tienen una muestra de interés, se seleccionan unidades de muestreo adicionales en la vecindad de estas. ii) Muestreo adaptativo aglomerado estratificado: se sigue el procedimiento descrito en i), pero sobre estratos definidos como se explica antes. 10.5.4 Muestreo Sistemático. Se usa principalmente por su simplicidad, y también para poder realizar muestreos uniformemente espaciados (en tiempo y espacio), que pueden tratarse como aleatorios, sin sesgo alguno. Se debe cuidar por la presencia de variaciones periódicas. "],["proceso-de-muestreo..html", "10.6 Proceso de Muestreo.", " 10.6 Proceso de Muestreo. Identificación del conjunto Población. Determinación del tamaño de nuestro conjunto muestral. Proporcionar un medio para la base de la selección de muestras del medio Población. Selección de muestras del medio utilizando una de las muchas técnicas de muestreo como el muestreo aleatorio simple, sistemático o estratificado. Verificar si el conjunto de muestra formado contiene elementos que realmente coinciden con los diferentes atributos del conjunto de población, sin grandes variaciones entre ellos. Comprobación de errores o estimaciones inexactas en el conjunto de muestras formadas, que pueden o no haber ocurrido. El conjunto que obtenemos después de realizar los pasos anteriores en realidad contribuye al conjunto de muestra. "],["teoría-de-estimación..html", "11 Teoría de Estimación.", " 11 Teoría de Estimación. La estimación consiste en realizar predicciones o inferencias sobre los parámetros de una distribución usando la información contenida en la muestra. Fromalmente Dada una variable aleatoria \\(X\\) cuya ley de probabilidad depende de un parámetro \\(\\theta\\), un estadístico \\(g(X)\\) se dice es estimador de \\(\\theta\\) si, para cualquier valor observado de \\(x \\in X\\), \\(g(x)\\) se considera un estimado de \\(\\theta\\). Esta definción se puede escribir de otra forma, como: Dadas las observaciones de variables aleatorias \\(X_1, X_2, \\ldots, X_n\\) identica e independientemente distribuidas (iid) con función de distirbución \\(F(x\\vert\\theta)\\), se estima \\(\\theta\\). En la primera definicón anterior, \\(g(x)\\) es una función de la muestra (esto es, de las observaciones realizadas de la v. a. \\(X\\)). La definción puede ser un poco dificil de comprender, pero podemos revisarla poco a poco usando un ejemplo. Ejemplo. Digamos que se realiza un muestreo aleatorio simple de una población cuya función de densidad es una v. a. normal de media \\(\\mu\\) y varianza finita \\(\\sigma^2\\). Digamos que se quiere construir un estimador de la media poblacional. Ya sabemos que el mejor estimador de la meda poblacional es \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\), y por lo tanto se tiene que \\(g(x) = \\bar{X}\\). De aquí en adelante, nos centraremos en construir estadísticos, además de usar los estimadores usuales ya conocidos. Pero antes, veamos las propiedades que tinene un buen estimador. "],["propiedades-de-un-estimador..html", "11.1 Propiedades de un estimador.", " 11.1 Propiedades de un estimador. Cualquier estimador que se precie de ser un buen estimador debe cumplir con 3 propiedades deseables para hacer inferencia. El estimador debe ser insesgado: un estimador se dice es insesgado cuando la esperanza de su estadístico es igual al valor del parametro siendo estimado. Se escribe el sesgo \\(B(\\hat{\\theta})\\) como: \\[B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta\\] Por ejemplo, podemos calcular la esperanza del estadístico \\(\\bar{X}\\) como \\(E[\\bar{X}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]\\). Usando las propiedades \\(E[cX] = cE[X]\\) y \\(E[\\sum_i X] = \\sum_iE[X]\\), donde \\(c\\) es una constante, se tiene que: \\[E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i]\\] Pero como \\(E[X] = \\mu\\) (el valor esperado de una v. a. es su media) entonces: \\[\\frac{1}{n}\\sum_{i=1}^n E[X_i] = \\frac{1}{n}\\sum_{i=1}^n\\mu_X = \\mu\\] y vemos que el estadísitico \\(\\bar{X}\\), que estima \\(\\mu\\) tiene sesgo nulo. Por lo tanto, \\(\\bar{X}\\) es un buen estimador de la media. El estimador debe ser eficiente: un estimador \\(g(x)\\) se dice que es eficiente si de todos los posibles estiamdores, \\(g(x)\\) tiene la mínima varianza posible. Más formalmente, si \\(\\hat{X_1}\\) y \\(\\hat{X_2}\\) son ambos estimadores insesgados de \\(X\\), entonces se dice que \\(\\hat{X_1}\\) es un estimador más eficiente de \\(X\\) que \\(\\hat{X_2}\\), si \\(\\sigma^2_{\\hat{X_1}} &lt; \\sigma^2_{\\hat{X_2}}\\). Figure 11.1: Ejemplo gráfico de estimadores: a) sesgado y no eficiente; b) insesgado pero no eficiente; c) sesgado y eficiente; d) insesgado y eficiente. El estimador debe ser consistente: se dice que un estimador \\(g(x)\\) es consistente si este se aproxima a al parámetro \\(\\theta\\) cuando el esfuerzo de meustreo se hace mayor. Formalmente: sea \\(X_1, X_2, \\ldots, X_n\\) variables aleatorias iid que se usan para obtener un estimador \\(\\hat{\\theta}\\) de \\(\\theta\\). Se dice que \\(\\hat{\\theta}\\) es un estimador consistente si converge en probabilidad a \\(\\theta\\), esto es: \\[\\lim\\limits_{n \\to \\infty} P\\left[\\vert\\hat{\\theta} - \\theta\\vert\\ge\\varepsilon\\right] = 0\\] El último límite se puede modificar para comprender mejor la propiedad de consistencia. Para ello, se puede usar la desigualdad de Chebyshev \\[P\\left[\\vert\\hat{\\theta} - \\theta\\vert\\ge\\varepsilon\\right] \\le \\frac{\\sigma^2_\\theta}{\\varepsilon^2}, \\varepsilon &gt; 0\\] y luego, tomando limites a ambos lados, podemos escribir la propiedad de consustencia como: \\[\\lim\\limits_{n \\to \\infty} \\sigma^2_\\hat{\\theta} = 0\\] Entonces, un estimador es consistente, cuando la varianza de este cae cero cuando aumentamos el esfuerzo de muestreo. Dicho de otra forma, el estimador es consistente cuando se acerca más al verdadero valor del parámetro cuando \\(n\\rightarrow\\infty\\). Ahora podemos proceder a estudiar los estimadores puntuales y por intervalos. "],["estimación-puntual..html", "11.2 Estimación puntual.", " 11.2 Estimación puntual. Cuando un investigador realiza un experimento, por lo general, solo toma una muestra represntativa de tamaño \\(n\\) de la población de interés y calcula estimadores que le permitan describir los datos obtenidos y relalizar inferencias. El investigador no se moelsta en realizar el experiemnto varias veces (no es como las simulaciones, donde podiamos realizar repeticiones tantas como quisieramos. En la realidad, no se tiene el esfuerzo, la aneergía o los emdios apra realizar multiples repeticiones de un experimento). En estos casos se usan estimadores puntuales para poder realizar inferencias basados en solo una repetición del experimento. Un estimador puntual de un parámetro \\(\\theta\\), es solo un valor \\(\\hat{\\theta}\\) de un estadístico \\(\\hat{\\Theta} = g(X)\\). Para aclarar la notación, \\(\\hat{\\Theta}\\) es el conjunto de todos los posibles valores del estadístico, y \\(\\hat{\\theta}\\) es un elemento de ese conjunto particular, calculado a partir de una muestra. Veamos algunos ejemplos. Ejemplo. Un experimento que busca evaluar la reacción de saltamontes a estímulos visuales o acústicos, en los que midieron el tiempo de reacción a estos antes del vuelo, encontraron que el tiempo de reacción promedio a estúmilos acústicos es \\(\\bar{X}_a = 108{,}05\\) segundos, y a estímulos visuales es \\(\\bar{X}_v=87{,}19\\) segundos. Estos dos valores son estimadores puntuales de las medias poblacionales \\(\\mu_a\\) y \\(\\mu_v\\). Ejemplo. Siniff y Skoog (1964) realizaron un muestreo aleatorio estratificado de una manada de caribúes de Nelchina en Alaska. Para ello, se establecieron 6 estratos (basados en estudios preliminares de la densidad relativa de los caribúes), y seleccionaron de manera aleatoria una muestra en cada uno de tamaño \\(n_i\\) ( \\(i=A, B, C, D, E, F\\) ), cada una de unidades muestrales de 4 millas cuadradas, obteniendose los datos mostrados en la tabla. Se desea saber el tamaño total de la población de caribúes. Estrato Tamaño del Estrato (\\(S\\)) Tamaño de muestra (\\(N_h\\)) Número promedio de Caribúes Varianza A 400 98 24.1 5575 B 30 10 25.6 4064 C 61 37 267.6 347556 D 18 6 179.0 22798 E 70 39 293.7 123578 F 120 21 33.2 9795 Total 699 211 NA NA Para poder conocer un estimado del tamaño poblacional total \\(\\hat{N}\\), se necesita primero de un estimado del número promedio de caribúes por unidad de muestreo. \\[ \\begin{aligned} \\bar{X}_{ST} &amp;= \\frac{\\sum_{h=1}^L N_h \\bar{x}_h}{N} \\\\ &amp;= \\frac{400\\times24{,}1 + 30\\times25{,}6 + 61\\times267{,}6 + \\ldots}{699} \\\\ &amp;= 77{,}96\\text{ caribúes milla}^{-2} \\end{aligned} \\] y se puede calcular la densidad de toda la población usando el total de millas cuadradas que conforman los estratos: \\[\\hat{N} = S \\times \\bar{X}_ST = 699\\text{ milla}^2 \\times 77{,}96\\text{ caribúes milla}^{-2} = 54.597\\text{ caribúes}\\] Sabemos entonces, que el estimado del número de caribúes es \\(\\hat{N} = 54.597\\text{ caribúes}\\). Sin embargo, aun necesitamos cuantificar la incertidumbre asociada a esta estimación. Podemos calcular la varianza de \\(\\bar{X}_ST\\) como: \\[Var(\\bar{X}_{ST}) = \\sum_{i=1}^L\\left[ \\frac{W_h^2 S_h^2}{n_h}(1 - f_h) \\right]\\] donde \\(W_h = N_h / N\\) es el ponderado del estrato y \\(f_h = n_h / N_h\\). Usando los datos de la tabla: \\[Var(\\bar{X}_{ST}) = \\left[ \\frac{0{,}572^2 5575}{98} \\right]\\left(1 - \\frac{98}{400}\\right) + \\left[ \\frac{0{,}043^2 4064}{10} \\right]\\left(1 - \\frac{10}{30}\\right) + \\ldots = 69{,}83\\] de forma que la varianza del tamaño de la población de caribúes es \\(69{,}83 \\times 699^2 = 34.105.734\\), y la desviación estándar es \\(\\sqrt{34.105.734} = 5.840\\) caribúes. Entonces el estimador buscado, con su medida de incertidumbre, es \\[54.597 \\pm 5.840 \\text{ caribúes}\\] Ejemplo. 11.2.1 Construcción de estadísitcos para inferencia. "],["estimación-por-intervalos..html", "11.3 Estimación por Intervalos.", " 11.3 Estimación por Intervalos. Una estimación por intervalo de un parámetro \\(\\theta\\) es un intervalo de la forma \\(\\hat{\\theta}_L &lt; \\theta &lt; \\hat{\\theta}_U\\), donde \\(\\hat{\\theta}_L\\) y \\(\\hat{\\theta}_U\\) dependen del valor del estadístico \\(\\hat{\\Theta}\\) para una muestra específica, y también de la distribución de muestreo de \\(\\hat{\\Theta}\\). Al intervalo \\(\\hat{\\theta}_L &lt; \\theta &lt; \\hat{\\theta}_U\\) se le llama intervalo de confinaza, y su longitud es un indicador de la precisión de una estimación puntual. Los valores \\(\\hat{\\theta}_L\\) y \\(\\hat{\\theta}_U\\) son estimadores de las variables aleatorias \\(\\hat{\\Theta}_L\\) \\(\\hat{\\Theta}_U\\). El razonamiento de la construcción de intervalos de confianza es utilizar la distriobución muesral de \\(\\hat{\\Theta}\\) para determinar los límites del intervalo de tal manera que: \\[P(\\hat{\\Theta}_L &lt; \\theta &lt; \\hat{\\Theta}_U) = 1 - \\alpha, \\quad 0 &lt; \\alpha &lt; 1\\] y decimos que hay una probabilidad de \\(1 - \\alpha\\) de que el intervalo contenga a \\(\\theta\\). El intervalo nos permite tener una confianza de \\(100(1 - \\alpha)\\)%. El grado de confianza es \\(1-\\alpha\\). El teorema del límite central se hace muy importante para la construcción de estadísticos cuya ley de probabilidad es conocida. Una muestra, varianza conocida. Cuando 14 estudiantes de segundo año de medicina del Bellevue Hospital midieron la presión sanguínea de la misma persona, obtuvieron los resultados que se listan abajo. Suponiendo que se sabe que la desviación estándar poblacional es de \\(10\\) mmHg, construya un estimado de un intervalo de confianza del 95% de la media poblacional. De manera ideal, ¿cuál debe ser el intervalo de confianza en esta situación? pressure &lt;- c(138, 130, 135, 140, 120, 125, 120, 130, 130, 144, 143, 140, 130, 150) Una manera de construir un estadístico es estableciendo una expresión que nos diga cuanto se desvía nuestro estimador del valor real: \\[\\hat{x} - \\mu = 133.93 - \\mu\\] La media se calcula usando mean(pressure). \\[\\hat{x} - \\mu = 133.93 - \\mu \\rightarrow \\frac{\\hat{x} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{133.93 - \\mu}{10 / \\sqrt{14}} \\sim N(0, 1)\\] Entonces podemos construir un intervalo de confianza del 95% (esto indica que \\(0{,}95 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}05\\)) como: \\[ \\begin{aligned} P(-z_{\\alpha/2} &lt; Z &lt; z_{\\alpha/2}) &amp;= P\\left(-z_{\\alpha/2} &lt; \\frac{133.93 - \\mu}{10 / \\sqrt{14}} &lt; z_{\\alpha/2}\\right) = 0{,}95 \\\\ &amp;= P\\left(-z_{\\alpha/2}\\frac{10}{\\sqrt{14}} &lt; 133.93 - \\mu &lt; z_{\\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ &amp;= P\\left(-133.93 - z_{\\alpha/2}\\frac{10}{\\sqrt{14}} &lt; - \\mu &lt; -133.93 + z_{\\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ &amp;= P\\left(133.93 - z_{\\alpha/2}\\frac{10}{\\sqrt{14}} &lt; \\mu &lt; 133.93 + z_{\\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ \\end{aligned} \\] Una muestra, varianza conocida. Tal que el intervalo es: \\[133.93 - z_{\\alpha/2}\\frac{10}{\\sqrt{14}} &lt; \\mu &lt; 133.93 + z_{\\alpha/2}\\frac{10}{\\sqrt{14}}\\] Como \\(\\alpha=0{,}05\\), entonces \\(\\alpha/2=0{,}025\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución normal, o usando qnorm(.025, 0, 1). En este caso, \\(z_{\\alpha/2} = -1.96\\): \\[139.17 \\text{ mmHg} &lt; \\mu &lt; 128.69 \\text{ mmHg}\\] ¿Cómo sé que mis datos son normales? Se calculan los residuales de la muestra: \\(\\varepsilon_i = X_i - \\bar{X}\\) para \\(i=1, 2, \\ldots, n\\) res &lt;- pressure - mean(pressure) # Grafico Cuantil-Cuantil ggplot(NULL, aes(sample=res)) + stat_qq() + stat_qq_line(colour=&quot;#213555&quot;, size=1.5) + theme_light() + theme( panel.background=element_rect(fill=&quot;#F5EFE7&quot;), plot.background=element_rect(fill=&quot;#F5EFE7&quot;) ) Una muestra, varianza desconocida. Control del plomo en el aire. A continuación se listan las cantidades de plomo medidas (en microgramos por metro cúbico o \\(\\mu g\\) \\({m}^{-3}\\)) en el aire. La Environmental Protection Agency estableció un estándar de calidad del aire para el plomo de \\(1{,}5\\) \\(\\mu g\\) \\({m}^{-3}\\). Las medidas que se presentan abajo se registraron en el edificio 5 del World Trade Center en diferentes días, inmediatamente después de la destrucción causada por los ataques terroristas del 11 de septiembre de 2001. Después del colapso de los dos edificios hubo una gran preocupación por la calidad del aire. Utilice los valores dados para construir un estimado del intervalo de confianza del 95% para la cantidad media de plomo en el aire. ¿Hay algo en este conjunto de datos que sugiera que el intervalo de confianza tal vez no sea muy bueno? Explique. air_quality &lt;- c(5.40, 1.10, 0.42, 0.73, 0.48, 1.10) Al igual que antes: \\[\\hat{x} - \\mu = 1.54 - \\mu\\] Esta diferencia la debemos entandarizar usando la desviacion típica: \\[\\hat{x} - \\mu = 1.54 - \\mu \\rightarrow \\frac{\\hat{x} - \\mu}{S/\\sqrt{n}} = \\frac{1.54 - \\mu}{1.914 / \\sqrt{6}} \\sim t(n - 1)\\] La desviación estándar se calcula usando sd(air_quality). Entonces podemos construir un intervalo de confianza del 95% (esto indica que \\(0{,}95 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}05\\)) como: \\[ \\begin{aligned} P(-t_{\\alpha/2, n - 1} &lt; T &lt; t_{\\alpha/2, n - 1}) &amp;= P\\left(-t_{\\alpha/2, n - 1} &lt; \\frac{1.54 - \\mu}{1.914 / \\sqrt{6}} &lt; t_{\\alpha/2, n - 1}\\right) = 0{,}95 \\\\ &amp;= P\\left(-t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}} &lt; 1.54 - \\mu &lt; t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}}\\right) = 0{,}95 \\\\ &amp;= P\\left(-133.93 - t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}} &lt; - \\mu &lt; -1.54 + t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}}\\right) = 0{,}95 \\\\ &amp;= P\\left(1.54 - t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}} &lt; \\mu &lt; 1.54 + t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}}\\right) = 0{,}95 \\\\ \\end{aligned} \\] Y el intervalo es: \\[1.54 - t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}} &lt; \\mu &lt; 1.54 + t_{\\alpha/2, n - 1}\\frac{1.914}{\\sqrt{6}}\\] Como \\(\\alpha=0{,}05\\), entonces \\(\\alpha/2=0{,}025\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución \\(t\\)-Student con \\(n - 1\\) grados de libertad, o usando qt(.025, 5). En este caso, \\(t_{\\alpha/2, 5} = -2.5706\\): \\[3.55 \\mu\\text{g m}^{-3} &lt; \\mu &lt; -0.47 \\mu\\text{g m}^{-3}\\] Proporciones \\[\\frac{\\hat{p} - \\pi}{\\sqrt{\\hat{p}(1 - \\hat{p})}} \\sim N(0, 1)\\] Diferencia de proporciones \\[\\frac{(\\hat{p}_1 - \\hat{p}_2) - \\Delta}{\\sqrt{\\hat{p}_1(1 - \\hat{p}_1)/n_1 + \\hat{p}_2(1 - \\hat{p}_2)/n_2}} \\sim N(0, 1)\\] Diferencia de medias, varianzas conocidas \\[\\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\sigma_1^2/n_1 + \\sigma_2^2/n_2}} \\sim N(0, 1)\\] \\[P(-z_{\\alpha/2} &lt; Z &lt; z_{\\alpha/2})= 1 - \\alpha\\] Diferencia de medias, varianzas desconocidas iguales Se usa el estimado puntual de la varianza agrupada (pooled): \\(S_{pool}^2 = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\\), de forma que: \\[\\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{S_{pool}\\sqrt{1/n_1 + 1/n_2}} \\sim t(n_1 + n_2 - 2)\\] \\[P(-t_{\\alpha/2, n - 1} &lt; T &lt; t_{\\alpha/2, n - 1}) = 1 - \\alpha\\] Diferencia de medias, varianzas desconocidas distintas En este caso, se usa una expresión un tanto más complicada para los grados de libertad: \\[\\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{S_1^2/n_1 + S_2^2/n_2}} \\sim t(\\nu), \\quad \\nu=\\frac{(S_1^2/n_1 + S_2^2/n_2)}{[(S_1^2/n_1)^2/(n_1 - 1) + (S_2^2/n_2)^2/(n_2 - 1)]}\\] Estimación de la varianza: varianza poblacional conocida En un estudio de los efectos sobre los bebés que tiene el consumo de cocaína durante el embarazo, se obtuvieron los siguientes datos muestrales de pesos al nacer: \\(n=190\\), \\(\\bar{x} = 2700\\) g, \\(S = 645\\) g (según datos de Cognitive Outcomes of Preschool Children with Prenatal Cocaine Exposure, de Singer et al., Journal of American Medical Association, vol. 291, núm. 20). Utilice los datos muestrales para construir un estimado del intervalo de confianza del 95% para la desviación estándar de todos los pesos al nacer de hijos de madres que consumieron cocaína durante el embarazo. Con base en el resultado, ¿parece que la desviación estándar difiere de la desviación estándar de \\(696\\) g de los pesos al nacer de hijos de madres que no consumieron cocaína durante el embarazo? Estimación de la varianza: varianza poblacional conocida Se usan proporciones para realizar las estimaciones: \\[\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\rightarrow P(\\chi^2_{1 - \\alpha/2, n-1} &lt; X^2 &lt; \\chi^2_{\\alpha/2, n-1}) = 1 - \\alpha\\] Estimación de la varianza: varianza poblacional conocida Se usan proporciones para realizar las estimaciones: \\[\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\rightarrow P(\\chi^2_{\\alpha/2, n-1} &lt; X^2 &lt; \\chi^2_{1 - \\alpha/2, n-1}) = 1 - \\alpha\\] Entonces podemos construir un intervalo de confianza del 95% (esto indica que \\(0{,}95 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}05\\)) como: \\[ \\begin{aligned} P(\\chi^2_{\\alpha/2, n-1} &lt; X^2 &lt; \\chi^2_{1 - \\alpha/2, n-1}) &amp;= P\\left(\\chi^2_{\\alpha/2, n-1} &lt; \\frac{(189)(645\\text{ g})^2}{\\sigma^2} &lt; \\chi^2_{1 - \\alpha/2, n-1}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{1}{\\chi^2_{1 - \\alpha/2, n-1}} &lt; \\frac{\\sigma^2}{(189)(645\\text{ g})^2} &lt; \\frac{1}{\\chi^2_{\\alpha/2, n-1}}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{(189)(645\\text{ g})^2}{\\chi^2_{1 - \\alpha/2, n-1}} &lt; \\sigma^2 &lt; \\frac{(189)(645\\text{ g})^2}{\\chi^2_{\\alpha/2, n-1}}\\right) = 0{,}95 \\end{aligned} \\] Y el intervalo es: \\[\\frac{(189)(645\\text{ g})^2}{\\chi^2_{1 - \\alpha/2, n-1}} &lt; \\sigma^2 &lt; \\frac{(189)(645\\text{ g})^2}{\\chi^2_{\\alpha/2, n-1}}\\] Como \\(\\alpha=0{,}05\\), entonces \\(\\alpha/2=0{,}025\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución chi-cuadrado, o usando qchisq(.025, 189). En este caso, \\(\\chi^2_{\\alpha/2, n-1} = 152.82\\): \\[3.4341145\\times 10^{5} &lt; \\sigma^2 &lt; 5.1451145\\times 10^{5}\\] De forma que el intervalo para la desviación estándar es: \\[586.01 \\text{ g} &lt; \\sigma &lt; 717.29 \\text{ g}\\] Estimación de la proporción de dos varianza Se ha realizado un estudio sobre la velocidad en vuelo de diversas especies de pájaros. El propósito era comparar las velocidades del pelícano pardo y el ostrero americano. Se cronometró una muestra de \\(9\\) pajaros pardos y \\(12\\) pajaros ostreros, volando con el viento de costado con una velocidad de viento de \\(5\\) a \\(8\\) millas h\\({}^{-1}\\), y se obtuvo que el pájaro pardo vuela, en promedio, a \\(26{,}05 \\pm 6{,}34\\) millas h\\({}^{-1}\\), y el ostrero a \\(30{,}19 \\pm 3{,}20\\) millas h\\({}^{-1}\\). Construya un intervalo de confianza del 95% para la proporción de varianzas. Al igual que antes, usamos la proporción entre las varianzas para construir un estadístico a partir del cual derivar nuestra ley de probabilidad: \\[\\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2} = \\frac{(6{,}34)^2\\sigma_2^2}{(3{,}20)^2\\sigma_1^2} \\sim F(n_1 - 1, n_2 - 1) \\rightarrow P(f_{1 - \\alpha/2, n_1-1, n_2-1} &lt; F &lt; f_{\\alpha/2, n_1-1, n_2-1})\\] Entonces podemos construir un intervalo de confianza del 90% (esto indica que \\(0{,}90 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}1\\)) como: \\[ \\begin{aligned} P(f_{\\alpha/2, \\nu_1, \\nu_2} &lt; F &lt; f_{1 - \\alpha/2, \\nu_1, \\nu_2}) &amp;= P\\left(f_{\\alpha/2, \\nu_1, \\nu_2} &lt; \\frac{(6{,}34)^2\\sigma_2^2}{(3{,}20)^2\\sigma_1^2} &lt; f_{1 - \\alpha/2, \\nu_1, \\nu_2}\\right) = 0{,}90 \\\\ &amp;= P\\left(f_{\\alpha/2, \\nu_1, \\nu_2}\\frac{(3{,}20)^2}{(6{,}34)^2} &lt; \\frac{\\sigma_2^2}{\\sigma_1^2} &lt; f_{1 - \\alpha/2, \\nu_1, \\nu_2}\\frac{(3{,}20)^2}{(6{,}34)^2}\\right) = 0{,}90 \\\\ &amp;= P\\left(\\frac{1}{f_{1 - \\alpha/2, \\nu_1, \\nu_2}}\\frac{(6{,}34)^2}{(3{,}20)^2} &lt; \\frac{\\sigma_1^2}{\\sigma_2^2} &lt; \\frac{1}{f_{\\alpha/2, \\nu_1, \\nu_2}}\\frac{(6{,}34)^2}{(3{,}20)^2}\\right) = 0{,}90 \\end{aligned} \\] Y el intervalo es: \\[\\frac{1}{f_{\\alpha/2, n_1-1, n_2-1}}\\frac{(6{,}34)^2}{(3{,}20)^2} &lt; \\frac{\\sigma_1^2}{\\sigma_2^2} &lt; \\frac{1}{f_{1 - \\alpha/2, n_1-1, n_2-1}}\\frac{(6{,}34)^2}{(3{,}20)^2}\\] Como \\(\\alpha=0{,}1\\), entonces \\(\\alpha/2=0{,}05\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución \\(F\\), o usando qf(.05, 8, 11): \\[1.33 &lt; \\frac{\\sigma_1^2}{\\sigma_2^2} &lt; 13\\] 11.3.1 Intervalos Unilaterales. Simplemente se prescinde de uno de las desigualdades. Pueden ser intervalos a la derecha: \\[P(Z &gt; z_{1 - \\alpha/2}) = 1 - \\alpha\\] … o intervalos a la izquierda: \\[P(Z &lt; z_{\\alpha/2}) = 1 - \\alpha\\] "],["introducción-al-contraste-de-hipótesis..html", "12 Introducción al Contraste de Hipótesis.", " 12 Introducción al Contraste de Hipótesis. Sirve como método que facilite el proceso de toma de decisión en base a datos recolectados de una población. Su finalidad es producir conclusiones sobre la población partiendo de una hipótesis particular. Se parte de una conjetura inicial sobre el sistema en estudio, esta determina la forma de . Se necesitan de datos experimentales. La hipótesis estadística es una aseveración o conjetura respecto a una o más poblaciones que se estudian. Puede ser verdadera o no. Una decisión tomada en base al contraste, esta plagada de incertidumbre. Debe ser formalizada en un planteamiento matemático concreto. "],["nos-podemos-equivocar.html", "13 Nos podemos equivocar…", " 13 Nos podemos equivocar… La decisión tomada esta enlazada a una medida de incertidumbre, por lo que es posible cometer errores en las conclusiones. Cualquier afirmación estadística, por tanto, debe ser establecida en conjunto con una medida de que tan seguros estamos de que la decisión tomada es correcta. La razón principal es la dependencia con la muestra usada para refutar o ratificar la hipótesis planteada. Ejemplo. Se examinó la influencia del fármaco succinilcolina sobre los niveles de circulación de andrógenos en la sangre. Se obtuvieron muestras de sangre de venados salvajes inmediatamente después de recibir una inyección intramuscular de succinilcolina con dardos de un rifle de caza. Treinta minutos después se obtuvo una segunda muestra de sangre y después los venados fueron liberados. Los niveles de andrógenos de 15 venados al momento de la captura y 30 minutos más tarde, medidos en nanogramos por mililitro (ng \\({\\text{mL}}^{-1}\\)), se presentan en la tabla. Conc. Adrogenos (ng/mL) Venado Al inyectar 30 min después 1 2.76 7.02 2 5.18 3.10 3 2.68 5.44 4 3.05 3.99 5 4.10 5.21 6 7.05 10.26 7 6.60 13.91 8 4.79 18.53 9 7.39 7.91 10 7.30 4.85 11 11.78 11.10 12 3.90 3.74 13 26.00 94.03 14 67.48 94.03 15 17.04 41.70 De los métodos de estimación sabemos que la cantidad de andrógenos al momento de la inyección es de \\(11.81 \\pm 16.64\\). Mientras que la concentración de andrógenos 30 minutos después de la inyección fue de \\(21.65 \\pm 30.92\\). Podemos construir un intervalo de confianza para las diferencias entre la concentración de andrógenos antes y después, para cada venado. \\[-0.38 &lt; D &lt; 20.08\\] El planteamiento de un sistema de experimentación comienza con una conjetura sobre lo que se desea estudiar (en el ejemplo anterior, se desea saber si la succinilcolina es capaz de disminuir los niveles de andrógenos en la sangre). Nos preocuparemos por obtener una muestra de tamaño \\(n\\) que esté descrita por los valores \\(x_1, x_2, \\ldots, x_n\\) de una variable aleatoria \\(X\\). Suponemos que cada valor es independiente de los demás. Por tanto, podemos conceptualizar estos valores como una secuencia \\(X_1, X_2, \\ldots, X_n\\) de variables aleatorias independientes e idénticamente distribuidas, cada una de las cuales tiene la misma distribución que \\(X\\). \\[ \\begin{aligned} \\text{Conjetura}: &amp; \\text{ La succinilcolina modifica la concentración de andrógeno en venados.} \\\\ \\text{No hay cambio}: &amp; \\text{ La succinilcolina no cambia la concentración de andrógeno en venados.} \\end{aligned} \\] La formalización comienza con el establecimiento de un conjunto de hipótesis posibles al que llamamos \\(\\Theta\\); y luego, se seleccionan dos hipótesis \\(\\Theta_0 \\subseteq \\Theta\\) y \\(\\Theta_1 \\subseteq \\Theta\\) tales que: \\[\\Theta_0 \\cup \\Theta_1 = \\Theta, \\qquad \\Theta_0 \\cap \\Theta_1 = \\emptyset\\] Al conjunto \\(\\Theta_0\\) se le conoce como hipótesis nula, \\(H_0\\); y al conjunto \\(\\Theta_1\\) se le conoce como hipótesis alternativa, \\(H_1\\); y se escribe: \\[ \\begin{aligned} H_0: &amp; \\Theta_0 \\subseteq \\Theta \\\\ H_1: &amp; \\Theta_1 \\subseteq \\Theta \\end{aligned} \\] Además, si se hace un contraste para obtener una conclusión con respecto a un parámetro \\(\\theta\\), el conjunto \\(\\Theta_0\\) debe contener la conjetura de que no hay un cambio en el valor esperado del parámetro (más específicamente, en la desviación esperada). En el ejemplo sobre el efecto de la succinilcolina sobre la concentración de andrógeno en venados, vimos que la diferencia en la concentración de andrógenos antes y después de la inyección de succinilcolina sirve como variable aleatoria para realizar inferencias. De forma que podemos definir el conjunto de todas las posibles hipótesis como \\(\\Theta = \\{D \\in\\mathbb{R}\\}\\). Entonces: \\[ \\begin{aligned} H_0: &amp; \\{D \\in \\mathbb{R}\\vert D = 0\\} \\\\ H_1: &amp; \\{D \\in \\mathbb{R}\\vert D \\ne 0\\} \\end{aligned} \\] \\[ \\begin{aligned} H_0: &amp; D = 0 \\\\ H_1: &amp; D \\ne 0 \\end{aligned} \\] La forma de las hipótesis planteadas para un experimento es algo subjetiva, dado que su proposición depende de lo que sabe el investigador y su experiencia. Las condiciones de los conjuntos asociados a cada hipótesis pueden ser más precisos, dando lugar a pruebas unilaterales. Por ejemplo: \\[ \\begin{aligned} H_0: &amp; D \\le 0 \\\\ H_1: &amp; D &gt; 0 \\end{aligned} \\] La hipótesis a probar es la hipótesis nula. Esta es la que se toma como cierta, como la proposición de que no hay cambio alguno, y debemos utilizar la información disponible como evidencia para respaldar nuestra conjetura alternativa. La hipótesis nula nunca se acepta, solo no se rechaza. No es que la condición de igualdad no se mantenga, sino que la información que se tiene no es capaz de refutarla. "],["estadístico-de-prueba..html", "13.1 Estadístico de Prueba.", " 13.1 Estadístico de Prueba. El estadístico de prueba se construye a partir de la información que se tiene del parámetro poblacional \\(\\theta\\), es decir, usando el estimador \\(\\hat{\\theta}\\) calculado a partir de la muestra de tamaño \\(n\\). Para desviaciones con respecto a un valor promedio: \\[\\hat{Z} = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)} \\sim N(0,1), \\qquad \\hat{T} = \\frac{\\hat{\\theta} - \\theta}{\\hat{SE(\\hat{\\theta})}} \\sim t(n-1)\\] Para contrastes de hipótesis sobre la varianza: \\[\\hat{\\chi}^2 = \\frac{(n-1) Var(\\hat{\\theta})}{Var(\\theta)} \\sim \\chi^2(n-1), \\qquad \\hat{F} = \\frac{Var(\\hat{\\theta_1})}{Var(\\hat{\\theta_2})} \\sim F(n_1 - 1, n_2 - 1)\\] Ejemplo. Siguiendo con nuestro ejemplo de la concentración de andrógenos luego de una inyección de succinilcolina, podemos trabajar con las diferencias \\(d_i\\) (pata \\(i=1, \\ldots, n\\)) entre la concentración de andrógenos antes y 30 min después de la inyección del metabolito. Cada \\(d_i\\) es la realización de una variable aleatoria \\(D_i\\), cuya ley de probabilidad es la misma para todo \\(i=1,\\ldots, n\\). Podemos construir un estadístico basado en estas diferencias para verificar si de verdad hay un cambio en la concentración de andrógenos: \\[\\bar{d} - D, \\text{ que para el caso particular, } D=0 \\text{ por lo que queda } \\bar{d} - 0\\] \\[\\frac{\\bar{d} - 0}{\\hat{S}_d/\\sqrt{n}} = 0.14 \\sim t(n - 1)\\] "],["referencias..html", "Referencias.", " Referencias. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
