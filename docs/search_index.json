[["index.html", "Prefacio", " Prefacio Esta es el sitio web para la primera edición de Bioestadística, publicado en Julio de 2023. Este libro es el resultado de la preparación del material de los cursos de Bioestadística I y Bioestadística II dados en la Universidad de Carabobo, y los cuales tuve la suerte de dictar durante el primer periodo académico de la universidad en 2023. En este libro tú aprenderas "],["introducción..html", "1 Introducción.", " 1 Introducción. "],["teoría-de-probabilidades..html", "2 Teoría de Probabilidades.", " 2 Teoría de Probabilidades. "],["combinatoria..html", "3 Combinatoria.", " 3 Combinatoria. "],["otros-teoremas-de-probabilidades..html", "4 Otros Teoremas de Probabilidades.", " 4 Otros Teoremas de Probabilidades. "],["estadística-descriptiva..html", "5 Estadística Descriptiva.", " 5 Estadística Descriptiva. Un conjunto de datos se puede describir usando: Medidas de tendencia central: estas son valores que describen el centro alrededor del cual el conjunto de observaciones se distribuye. De esta forma, nos permite describir donde se localizan la mayoría de las observaciones. Son tres: Media: describe la localización media de las observaciones. Mediana: es el valor que distribuye las observaciones de tal forma que 50% de estas quedan por encima de ella, y el otro 50% por debajo. Moda: describe la posición de la (o las) observación (observaciones) más frecuente(s). Medidas de posición: vienen definidos por los cuantiles de una distribución. Un cuantil \\(C_p\\) se define como el valor que deja por debajo de si \\(p\\times100\\)% de las observaciones. Por ejemplo, el cuantil \\(C_{0{,}3}\\) es aquel valor que deja por debajo de si \\(30\\)% de las observaciones. Nos ayuda a describir la posición que una observación ocupa dentro del dominio sobre el cual se distribuyen los datos. Algunos ejemplos son: Los cuartiles: (\\(C_{0{,}25i}\\) con \\(i = 1,2,3\\)) que distribuyen las observaciones en \\(4\\) partes, Los deciles (\\(C_{0{,}1i}\\) con \\(i = 1,2,\\ldots,10\\)), que dividen la distribución en \\(10\\) partes, y Los percentiles (\\(C_{0{,}01i}\\) con \\(i = 1,2,\\ldots, 99, 100\\)), que dividen la distribución en \\(100\\) partes. Medidas de dispersión: estas son medidas de que tan variables son las observaciones. Sirven para describir la dispersión de las observaciones en su dominio y alrededor de su centro. Pueden ser: Rango o Recorrido: es la diferencia entre el valor máximo y el valor mínimo observado. Nos dice que tan amplio es el dominio ocupado por las observaciones, o que tan amplio es el intervalo sobre el cual se distribuyen todas las observaciones. Rango intercuartílico: es la diferencia entre el tercer cuartil y primer cuartil, y por lo tanto, describe la amplitud del intervalo que contiene a un \\(50\\)% de las observaciones. Varianza: es una medida de las diferencias cuadráticas promedio de las observaciones con respecto a la media (sumatoria de cuadrados promedio). Sirve como una medida de cuán variable es un conjunto de datos, dado que a mayor son las desviaciones de la media, más grande es la varianza. Desviación estándar: es una medida de la distancia promedio de las observaciones con respeto a la media. Al igual que antes para la varianza, la desviación estándar sirve como medida de variabilidad con respecto al centro, dado que a mayor la distancia de las observaciones a la media, mayor será la desviación estándar. Coeficiente de variación: es el valor proporcional de la desviación estándar con respecto a la media (Desviación estándar / Media). Esta sirve como medida de dispersión relativa, dado que permite comparar distribuciones basado en cuán distantes, en promedio, de la media están las observaciones, basados en el tamaño relativo de esta con respecto a la media. Medidas de forma: estas nos ayudan a describir la simetría y ensanchamiento de la distribución de las observaciones. Estas son: Asimetría: es un coeficiente cuyo valor nos permite decir si las observaciones se encuentran acumuladas a la derecha, o la izquierda de la distribución (a esto se le llama sesgo). Curtosis: esta describe que tan amplio es el pico de la distribución de observaciones, permitiéndonos decir si se trata de una colina amplía o de un pico estrecho. "],["datos-no-agrupados..html", "5.1 Datos no agrupados.", " 5.1 Datos no agrupados. Para comprender los conceptos de la estadística descriptiva en datos sin agrupar, usamos el conjunto de datos obtenidos del experimento que busca evaluar la reacción de saltamontes a estímulos visuales o acústicos, que midieron el tiempo de reacción a estos antes del vuelo: A estímulos acústicos: \\(86, 102, 103, 99, 108, 100, 118, 108, 109, 113, 114, 107, 107, 117, 120, 101, 126, 109, 106\\). A estímulos visuales: \\(72, 95, 73, 99, 71, 90, 102, 97, 71, 75, 80, 70, 100, 104, 81, 103, 101, 103, 77, 78, 89\\). 5.1.1 Medidas de Tendencia central. El promedio se define como: \\[\\bar{X} = \\frac{\\sum_{i=1}^k f_ix_i}{n}\\] dónde \\(x_i\\) es la \\(i\\)-ésima observación, y \\(f_i\\) es la frecuencia absoluta de la observación (cuantas veces se repite \\(x_i\\) entre el número de observaciones), y la sumatoria se hace sobre los \\(k\\) observaciones únicas (las observaciones repetidas no se toman en cuenta, ya se están tomando en cuenta al multiplicar por \\(f_i\\) el valor observado repetido). Para el tiempo de reacción a estímulos acústicos se calcularía entonces: \\[\\begin{aligned} \\bar{X}_a &amp;= \\frac{86+102+103+99+2\\times108+100+118+109+113+114+2\\times107+117+120+101+126+109+106}{19} \\\\ &amp;= 108{,}05 \\text{ segundos} \\end{aligned}\\] Procediendo de igual forma para el tiempo de reacción a estímulos visuales se obtiene \\(\\bar{X}_v=87{,}19\\) segundos (¡verifícalo!). La mediana para datos agrupados se consigue siguiendo los siguientes pasos: Se ordenan de menor a mayor las observaciones. Se asignan índices a los datos ordenados desde \\(1\\) a \\(n\\): al menor dato se le asigna el índice \\(1\\), al siguiente el \\(2\\), y así sucesivamente. Se calcula el índice de posición de la mediana como \\((n + 1)/2\\) si \\(n\\) es impar. Si \\(n\\) es par se calculan los índices de posición \\((n-1)/2\\) y \\((n+1)/2\\). La mediana es entonces: \\[M = \\begin{cases} x_{(n + 1)/2}, &amp; \\text{ si }n\\text{ es impar}. \\\\ \\frac{x_{(n-1)/2} + x_{(n+1)/2}}{2}, &amp; \\text{ si }n\\text{ es par.} \\end{cases}\\] Para el caso del tiempo de reacción a estímulos acústicos, tenemos que los datos ordenados son: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, 102_{(5)}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, 108_{(10)}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;114_{(15)}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] dónde el subscrito entre paréntesis corresponde al índice dado al dato. Cómo \\(n=19\\) es impar, se calcula el índice de posición \\((n+1)/2 = 10\\). Ubicamos en los datos ordenados la observación con el índice \\(10\\): \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, 102_{(5)}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, \\color{red}{108_{(10)}}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;114_{(15)}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] y este valor corresponde a la mediana, de forma que: \\[M =108 \\text{ segundos}\\] Se puede proceder de igual forma con el tiempo de reacción a estímulos visuales y obtener \\(M = 89\\text{ segundos}\\) (¡verifícalo!). La moda para datos no agrupados es la observación con la máxima frecuencia registrada (la frecuencia con la mayor magnitud): \\[Moda(\\{x_i\\}_{i=1}^n) = \\{x_i \\vert f_{x_i} = max(\\{f_{x_1}, f_{x_2}, \\ldots, f_{x_n}\\})\\}\\] (la notación \\(\\{x_i\\}_{i=1}^k\\) y \\(\\{x_1, x_2, \\ldots, x_n\\}\\) son equivalentes, es decir, \\(\\{x_i\\}_{i=1}^n = \\{x_1, x_2, \\ldots, x_n\\}\\)). Para calcular la moda se siguen los siguientes pasos: Se ordenan los datos de menor a mayor. Se calculan las frecuencias absolutas \\(f_{x_i}\\) contando el número de veces que la magnitud de una observación se repite. La moda es el valor que más se repita. Si hay más de un valor con la misma frecuencia absoluta, entonces la moda es un conjunto de todos los valores con la misma frecuencia absoluta. En el caso del tiempo de reacción a estímulos acústicos, los datos ordenados son: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(1)}, 100_{(1)}, 101_{(1)}, 102_{(1)}, 103_{(1)}, 106_{(1)}, 107_{(2)}, 108_{(2)}, \\\\ &amp;109_{(2)}, 113_{(1)}, 114_{(1)}, 117_{(1)}, 118_{(1)}, 120_{(1)}, 126_{(1)} \\end{aligned}\\] donde se muestra como subscrito en paréntesis la frecuencia absoluta \\(f_{x_i}\\). Como la \\(max(\\{f_{x_i}\\}) = 2\\), que es la frecuencia absoluta de \\(107, 108\\) y \\(109\\) todos en segundos, por lo que \\(Moda(\\{x_i\\}_{i=1}^n) = \\{107, 108, 109\\}\\) y los datos son multimodales. En el caso de estímulos visuales, \\(max(\\{f_{x_i}\\}) = 2\\) para \\(71\\) y \\(103\\) todos en segundos, por lo que \\(Moda(\\{x_i\\}_{i=1}^n) = \\{71, 103\\}\\) segundos (¡verifícalo!). A continuación se muestra un resumen de los estadísticos de tendencia central calculados para los tiempos de reacción acústico y visual, usando R: acoustic &lt;- c(86, 102, 103, 99, 108, 100, 118, 108, 109, 113, 114, 107, 107, 117, 120, 101, 126, 109, 106) visual &lt;- c(72, 95, 73, 99, 71, 90, 102, 97, 71, 75, 80, 70, 100, 104, 81, 103, 101, 103, 77, 78, 89) # Para el calculo de la moda f_a_acoustic &lt;- table(acoustic) mode_acoustic &lt;- names(f_a_acoustic)[which(f_a_acoustic == max(f_a_acoustic))] f_a_visual &lt;- table(visual) mode_visual &lt;- names(f_a_visual)[which(f_a_visual == max(f_a_visual))] tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Media = c(mean(acoustic), mean(visual)), Mediana = c(median(acoustic), median(visual)), Moda = c(paste(mode_acoustic, collapse=&quot;, &quot;), paste(mode_visual, collapse=&quot;, &quot;)) ) %&gt;% kbl() Estimulo Media Mediana Moda Acústico 108.05263 108 107, 108, 109 Visual 87.19048 89 71, 103 donde se muestra que: Para los tiempos de reacción a estímulos acústicos, la media y la mediana coinciden bastante, y aunque la moda consiste de tres elementos, la media y la mediana caen dentro de ese conjunto, lo cual indica que la distribución consiste de un solo pico simétrico. Para los tiempos de reacción a estímulos visuales, la mediana se desvía ligeramente de la media, indicando que la distribución es algo simétrica, pero como las modas están alejadas en promedio unas 16 unidades de la media/mediana, y unas 30 unidades entre sí, podemos decir que la distribución consiste de dos picos. Lo anterior implica que los saltamontes responden de forma única a estímulos acústicos, pero a los estímulos visuales una parte de la población de saltamontes responde rápidamente y la otra parte no tan rápido. 5.1.2 Medidas de posición. Para los datos no agrupados, los cuantiles se pueden calcular siguiendo los siguientes pasos: Se ordenan los datos de menor a mayor. Se asignan índices a las posiciones de cada observación. Se calcula la posición de los cuantiles usando: \\[C_i = \\begin{cases} \\frac{n\\times i}{d}, \\text{ si }n\\text{ es par.} \\\\ \\frac{(n+1)\\times i}{d}, \\text{ si }n\\text{ es impar.} \\end{cases}\\] Ubicamos las observaciones \\(X_{C_i}\\) correspondientes a cada cuantil. Si \\(C_i\\) no es un entero, se calcula el promedio \\(x_{C_i} = (x_{(C_i - 0{,}5)} + x_{(C_i + 0{,}5)}) / 2\\). dónde la \\(i\\) es un entero que corresponde al \\(i\\)-ésimo cuantil (ve más abajo el ejemplo), y la \\(d\\) representa en cuántas partes queremos dividir la distribución. Por ejemplo, si quisiéramos calcular los cuartiles, tendríamos que dividir la distribución en cuatro partes estableciendo \\(d=4\\), y las \\(i\\) tendrían valores de \\(1, 2\\) y \\(3\\) para los cuartiles \\(Q_1, Q_2\\) y \\(Q_3\\), respectivamente (cambiamos la notación de \\(C_i\\) a \\(Q_i\\) debido a que así se denotan usualmente en otros libros de texto y recursos). Para los tiempos de reacción ante estímulos acústicos ya se tienen los datos ordenados antes, junto con índices. Para calcular los cuartiles, como \\(n=19\\) es impar, calculamos \\(Q_i = (19 + 1)\\times i/4 = 5i\\), por lo que: \\[\\{Q_1, Q_2, Q_3\\} = \\{5, 10, 15\\}\\] Buscamos las observaciones en las posiciones dadas por el conjunto anterior: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, \\color{red}{102_{(5)}}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, \\color{red}{108_{(10)}}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;\\color{red}{114_{(15)}}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] Por lo tanto, los cuartiles son: \\[\\{x_{Q_1}, x_{Q_2}, x_{Q_3}\\} = \\{102, 108, 114\\}\\] Notamos dos cosas: la primera es que la mediana y el cuartil 2 coinciden como se esperaba. Segundo, si vemos los datos ordenados con los cuartiles en rojo, vemos que entre cada cuartil hay exactamente 4 datos, es decir, la distribución se dividió en cuatro partes, cada una con exactamente la misma cantidad de datos. Para los tiempos de reacción a estímulos visuales, se puede encontrar que \\(\\{Q_1, Q_2, Q_3\\} = \\{5{,}5; 11; 16{,}5\\}\\). Esta vez, los cuartiles son decimales, por lo que podemos usar el valor promedio de las observaciones entre las observaciones adyacentes. Por ejemplo, para \\(Q_1 = 5{,}5\\), se ubican las observaciones \\(x_5 = 73\\) y \\(x_6 = 75\\) (que son los enteros adyacentes a \\(5{,}5\\)) y calculamos el promedio \\((73 + 75) / 2 = 74\\). Realizamos el mismo procedimiento para el tercer cuartil y entonces \\(\\{x_{Q_1}, x_{Q_2}, x_{Q_3}\\} = \\{74; 89; 100{,}5\\}\\) (¡Verifica los resultado!). Ejercicio. Calcula para los datos de tiempos de reacción de saltamontes a estímulos acústicos y visuales, los deciles (\\(d=10\\)) y percentiles (\\(d = 100\\)). A continuación, se resumen los estadísticos de posición en conjunto con los de tendencia central para los tiempos de reacción (añadimos por conveniencia el mínimo y máximo valor registrado): tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Min = c(min(acoustic), min(visual)), Q_1 = c(102, 74), Media = c(mean(acoustic), mean(visual)), Mediana = c(median(acoustic), median(visual)), Q_3 = c(114, 100.5), Max = c(max(acoustic), max(visual)), Moda = c(paste(mode_acoustic, collapse=&quot;, &quot;), paste(mode_visual, collapse=&quot;, &quot;)) ) %&gt;% kbl() Estimulo Min Q_1 Media Mediana Q_3 Max Moda Acústico 86 102 108.05263 108 114.0 126 107, 108, 109 Visual 70 74 87.19048 89 100.5 104 71, 103 donde podemos ver que la distribución de datos en el grupo que recibió estímulos acústicos es más equitativa, ya que los cuartiles se distancian uno del otro en \\(6\\) unidades, mientras que los cuartiles de los datos de estímulos visuales se distancian \\(15\\) y \\(11{,}5\\) unidades, por lo que hay más datos hacia tiempos de reacción altos, y no tanto en tiempos de reacción bajos. Esto ayuda a enfatizar la asimetría pequeña de la distribución de estímulos visuales, y la simetría de la de estímulos acústicos. 5.1.3 Medidas de Dispersión. El recorrido o rango se define como la diferencia con respecto al valor máximo y el mínimo: \\[R = x_{max} - x_{min}\\] Para los tiempos de reacción se tiene que para estímulos acústicos \\(R_a = 126 - 86 = 40\\) segundos, y para estímulos visuales \\(R = 104 - 70 = 34\\) segundos. Esto quiere decir, que el tiempo de reacción a estímulos visuales ocupa una mayor cantidad de posibles observaciones, lo cual lo hace más variable la respuesta que la respuesta a estímulos acústicos. El rango intercuartílico (\\(IQR\\)) es un recorrido pero tomado desde el primer cuartil al tercer cuartil: \\[IQR = Q_3-Q_1\\] Para los tiempos de reacción a estímulos acústicos \\(IQR = 114 -102 = 12\\) segundos, mientras que para estímulos visuales \\(IQR = 100{,}5 - 74 = 26{,}5\\) segundos. Esto quiere decir que el 50% de las observaciones típicas para la respuesta a estímulos acústicos caen en un intervalo más pequeño comparado con la respuesta típica a estímulos visuales, haciendo el tiempo de reacción más consistente en el primer caso. La varianza se define como: \\[S^2 = \\frac{\\sum_{i=1}^k f_i(x_i - \\bar{X})^2}{n - 1}\\] es decir, a cada observación le quitamos el valor medio, y al resultado lo elevamos al cuadrado. Luego, sumamos los resultados y dividimos entre \\(n-1\\). Para los datos de tiempos de reacción a estímulos acústicos, se tiene que las diferencias con respecto a la media son: \\[\\begin{aligned} &amp;-22{,}05; -6{,}05; -5{,}05; -9{,}05; -0{,}05; -8{,}05; 9{,}95; -0{,}05; 0{,}95; \\\\ &amp;4{,}95; 5{,}95; -1{,}05; -1{,}05; 8{,}95; 11{,}95; -7{,}05; 17{,}95; 0{,}95; -2{,}05 \\end{aligned}\\] donde las desviaciones más grandes son aquellas que se alejan más de la media. Elevando al cuadrado permite eliminar los signos, de forma que al sumar no se cancelen los terminos, se obtiene: \\[\\begin{aligned} &amp;486{,}32; 36{,}63; 25{,}53; 81{,}95; 0; 64{,}84; 98{,}95; 0; 0{,}9; 24{,}48; \\\\ &amp;35{,}37; 1{,}11; 1{,}11; 80{,}06; 142{,}74; 49{,}74; 322{,}11; 0{,}9; 4{,}21 \\end{aligned}\\] Luego, sumando estos valores se obtiene: \\[S^2 = \\frac{1456{,}95}{19 - 1} = 80{,}94\\text{ s}^2\\] Para los tiempos de reacción a estímulos visuales se tiene \\(S^2= 167{,}16\\text{ s}^2\\) (verifícalo!). La desviación estándar es: \\[S = \\sqrt{S^2}\\] Y sirve como medida de la distancia promedio de las observaciones con respecto a la media. Usando el resultado anterior, se tiene para los tiempos de reacción a estímulos acústicos \\(S = 8{,}997\\) segundos, y para los tiempos de reacción a estímulos visuales \\(S = 12{,}929\\) segundos. Esto quiere decir que el \\(68{,}2\\)% de las observaciones típicas para la respuesta a estímulos acústicos caen a una distancia de una \\(\\sim9\\) segundos de la media, mientras que para los estímulos visuales caen a unos \\(\\sim13\\) segundos de la media, haciendo más variable la respuesta a estímulos visuales. La última medida de variación importante es el coeficiente de variación, \\(CV\\), que se define como: \\[CV = \\frac{S}{\\bar{X}}\\] Y nos dice que tan grande es la desviación estándar con respecto a la media. Para una distribución normal este valor es \\(\\sim0{,}3\\) (\\(30\\)% en valor porcentual). Para los tiempos de reacción se tiene que para estímulos acústicos \\(CV = 8{,}997/ 108{,}05 = 0{,}0833\\) (\\(\\sim8{,}3\\)%), y para estímulos visuales \\(CV = 12{,}929 / 87{,}19 = 0{,}1483\\) (\\(\\sim14{,}8\\)%). Esto quiere decir, que el tiempo de reacción a estímulos visuales tiene una variación mayor (de aproximadamente 6% mayor) que la respuesta a estímulos acústicos, dado que la desviación estándar representa una mayor proporción de la magnitud de la media. Resumimos los estadísticos de dispersión a continuación: tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Rango = c(max(acoustic) - min(acoustic), max(visual) - min(visual)), IQR = c(114 - 102, 100.5 - 74), Varianza = c(var(acoustic), var(visual)), Std.Dev = c(sd(acoustic), sd(visual)), CV = c(sd(acoustic) / mean(acoustic), sd(visual) / mean(visual)) * 100 ) %&gt;% kbl() Estimulo Rango IQR Varianza Std.Dev CV Acústico 40 12.0 80.94152 8.996751 8.326267 Visual 34 26.5 167.16190 12.929111 14.828581 5.1.4 Medidas de Forma. El coeficiente de asimetría es una medida de forma que busca cuantificar la simetría de una distribución. Se calcula como: \\[A = \\frac{\\sum_{i=1}^n(x_i - \\bar{X})^3}{n S^3}\\] Para una distribución simétrica, se esperaría obtener el mismo número de diferencias \\((x_i - \\bar{X})^3\\) negativas como positivas, y al sumar la magnitud de todas las diferencias negativas, esta sería de igual magnitud que la suma de todas las diferencias positivas, por lo que \\(A = 0\\) si la distribución es simétrica (aunque lo contrario no es cierto: el que \\(A = 0\\) no asegura que la distribución sea simétrica). La asimetría puede ser positiva o negativa dependiendo de la dirección del sesgo. Para distribuciones sesgadas hacia la derecha, \\(A &gt; 0\\). Para distribuciones sesgadas hacia la izquierda, \\(A &lt; 0\\). Se puede calcular la sumatoria de cubos para los tiempos de reacción a estímulos acústicos y visuales, y obtener el coeficiente de asimetría, que para las población de saltamontes sometida a estímulos acústicos \\(A = -0{,}23\\), y para la que fue sometida a estímulos visuales \\(A = -0{,}01\\). Esto muestra que los tiempos de reacción a estímulos visuales es solo muy ligeramente asimétrica, pero que la de tiempos de reacción a estímulos acústicos es in poco más sesgada hacia la izquierda (aunque no tan apreciablemente). La curtosis se define como: \\[K = \\frac{\\sum_{i=1}^n(x_i - \\bar{X})^4}{n S^4} - 3\\] lo cual nos dice algo sobre la forma como se concentran los datos alrededor de la media. Para la distribución normal (que es simétrica y se usa como estándar de comparación) se puede verificar que \\(K = 0\\). Se dice entonces que la distribución: * Es platicúrtica si \\(K &lt; 0\\), y esto implica que las distribuciones extremas son más probables con respecto a la normal. * Es mesocúrtica si \\(K=0\\), y las observaciones se distribuyen más como una normal. * Es leptocúrtica si \\(K &gt; 0\\), y entonces las observaciones tienden a aglomerarse alrededor de la media más de lo que ocurre en la distribución normal. Para las población de saltamontes sometida a estímulos acústicos \\(K = 0{,}166\\), y para la que fue sometida a estímulos visuales \\(K = -1{,}79\\). Esto muestra que los tiempos de reacción a estímulos acústicos se asemeja a una distribución mesocúrtica y los tiempos de reacción a estímulos visuales es platicúrtica, indicando una distribución que tiene colas pesadas, siendo la de estímulos visuales mucho más dispersa alrededor de la media (esto tiene sentido dado nuestro descubrimiento de que se trata de una distribución multimodal). tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Asimetria = c(1 / 19, 1 / 21) * c(sum((acoustic - mean(acoustic)) ** 3) / sd(acoustic) ** 3, sum((visual - mean(visual)) ** 3) / sd(visual) ** 3), Curtosis = c(1 / 19, 1 / 21) * c(sum((acoustic - mean(acoustic)) ** 4) / sd(acoustic) ** 4, sum((visual - mean(visual)) ** 4) / sd(visual) ** 4) - 3 ) %&gt;% kbl() Estimulo Asimetria Curtosis Acústico -0.2299108 0.1659499 Visual -0.0100155 -1.7877228 Ejercicio Para el siguiente conjunto de datos de pesos de individuos de una población, separados en Machos (M) y Hembras (H), realice un análisis descriptivo. # Datos Diferencias Peso de Machos y hembras ungrouped &lt;- tibble( Sex = unlist(strsplit(&quot;HMHMHMHMMHMMHHHMHMMHMHMMHMMHHMHMMHMMHMHMMHMHMMHHMM&quot;, &quot;&quot;)), Peso = c(98.5, 150.6, 108.3, 159.4, 162.6, 122.5, 118.5, 167.5, 170.5, 120.4, 177.5, 186.5, 115.4, 115.5, 52.5, 157.6, 134.4, 148.5, 131.5, 143.4, 145.6, 108.6, 155.5, 110.6, 154.5, 183.5, 191.5, 128.6, 135.4, 195.4, 137.5, 205.6, 190.7, 120.5, 188.7, 176.3, 118.5, 158.5, 116.5, 161.4, 165.5, 142.6, 164.6, 120.5, 170.4, 195.5, 132.5, 129.5, 215.6, 176.5) ) "],["datos-agrupados..html", "5.2 Datos Agrupados.", " 5.2 Datos Agrupados. Para comprender los conceptos de la estadística descriptiva en datos agrupados, usamos el conjunto de datos obtenidos del experimento que busca evaluar los niveles de glicemia (en mg dL\\({}^{-1}\\)) en 25 pacientes, cuyos resultados fueron: \\[75, 82, 90, 95, 101, 112, 121, 132, 140, 97, 84, 90, 96, 102, 114, 121, 138, 87, 91, 96, 104, 123, 89, 93, 99\\] Como vimos antes, estos datos se pueden agrupar en clases que denotan intervalos (que pueden ser continuos o aparentes) en los cuales caen las observaciones con cierta frecuencia absoluta, como se muestra a continuación: glicemia &lt;- tribble( ~Lim_rel_inf, ~Lim_rel_sup, ~`Marca de Clase`, ~`f_i`, ~`fr_i`, ~`F_i`, ~`Fr_i`, 74.5, 86.5, 80.5, 3, 12 / 100, 3, 12 / 100, 86.5, 98.5, 92.5, 10, 40 / 100, 13, 52 / 100, 98.5, 110.5, 104.5, 4, 16 / 100, 17, 68 / 100, 110.5, 122.5, 116.5, 4, 16 / 100, 21, 84 / 100, 122.5, 134.5, 128.5, 2, 8 / 100, 23, 92 / 100, 134.5, 146.5, 140.5, 2, 8 / 100, 25, 100 / 100) glicemia %&gt;% kbl() Lim_rel_inf Lim_rel_sup Marca de Clase f_i fr_i F_i Fr_i 74.5 86.5 80.5 3 0.12 3 0.12 86.5 98.5 92.5 10 0.40 13 0.52 98.5 110.5 104.5 4 0.16 17 0.68 110.5 122.5 116.5 4 0.16 21 0.84 122.5 134.5 128.5 2 0.08 23 0.92 134.5 146.5 140.5 2 0.08 25 1.00 5.2.1 Medidas de Tendencia Central. El promedio se define como: \\[\\bar{X} = \\frac{\\sum_{i=1}^k f_ic_i}{n}\\] dónde \\(c_i\\) es la \\(i\\)-ésima marca de clase, y \\(f_i\\) es la frecuencia absoluta asociada a la marca de clase (cuantas observaciones \\(c_i\\) contiene dentro de los límites del intervalo). Para las marcas de clases de los índices de glicemia medidos se calcularía entonces: \\[\\bar{X} = \\frac{3\\times80{,}5 + 10\\times92{,}5 + 4\\times104{,}5 + 4\\times116{,}5 + 2\\times128{,}5 + 2\\times140{,}5}{25}=103{,}54 \\text{ g dL}^{-1}\\] La mediana para datos agrupados se consigue siguiendo los siguientes pasos: Se calcula el índice de posición de la mediana como \\(n/2\\). Se localiza la clase que identifica el intervalo mediano (aquel que contiene la mediana) al buscar la primera clase cuya frecuencia absoluta acumulada sea igual o mayor a \\(n/2\\). La mediana es entonces: \\[M = L_i + \\left(\\frac{n/2 - F_{i-1}}{f_i}\\right)\\cdot a_i\\] donde \\(L_i\\) es el límite inferior del intervalo mediano, \\(F_{i-1}\\) es la frecuencia absoluta acumulada de la clase anterior, \\(f_i\\) es la frecuencia absoluta de la clase que contiene la mediana, y \\(a_i\\) es la amplitud de los intervalos. Para el caso de los índices de glicemia, se tiene que \\(n/2 = 25/2 =12{,}5\\). La primera clase con \\(F_i \\ge 12{,}5\\) es la segunda clase (\\(F_i = 13\\)). Inspeccionando la tabla de datos agrupados podemos calcular entonces: \\[M = 86{,}5 + \\left(\\frac{12{,}5 - 3}{10}\\right)\\cdot 12 = 97{,}9\\text{ g dL}^{-1}\\] La moda se calcula, para datos agrupados, siguiendo estos pasos: Se busca la clase modal (aquella que contiene la moda) determinando cuál de ellas tiene la mayor frecuencia absoluta. Se calcula la moda como: \\[Moda(\\{c_i\\}_{i=1}^k) = L_i + \\left(\\frac{f_i - f_{i-1}}{(f_i - f_{i-1}) + (f_i - f_{i+1})}\\right)\\cdot a_i\\] Para el caso de los índices de glicemia, la clase con la mayor frecuencia absoluta es la segunda clase, por lo que: \\[Moda(\\{c_i\\}_{i=1}^k) = 86{,}5 + \\left(\\frac{10 - 3}{(10 - 3) + (10 - 4)}\\right)\\cdot 12 = 92{,}96\\text{ g dL}^{-1}\\] Notamos que los resultados sobre los datos de frecuencia muestran que \\(Moda &lt; M &lt; \\bar{X}\\), lo cual nos indica que la distribución esta sesgada a la derecha. 5.2.2 Medidas de posición. Para los cuantiles de datos agrupados, se sigue un procedimiento similar al de datos sin agrupar, identificando primero las clases que contiene los cuantiles. Para ello, se sigue el procedimiento: Calcula \\(\\frac{n\\times i}{d}\\) donde \\(i\\) representa el \\(i\\)-ésimo cuantil, y \\(d\\) el número de partes en las que se desea dividir la distribución. Se busca en la tabla de datos agrupados la clase cuya frecuencia absoluta acumulada sea mayor o igual a \\(\\frac{n\\times i}{d}\\). Este será la clase cuantílica. Se calcula el cuantil como: \\[C_i = L_i + \\left(\\frac{f_i - f_{i-1}}{f_i}\\right)\\cdot a_i\\] Digamos que queremos calcular los cuartiles de los datos de índice de glicemia. En este caso, calculamos para \\(i = 1,2,3\\) los valores de \\(i\\times n / 4\\), los cuales son \\(6{,}25, 12{,}5,\\) y \\(18{,}75\\). Calculamos los cuartiles: para el primer cuartil, el resultado muestra que el cuartil se encuentra en la segunda clase, por lo que: \\[Q_1 = 86{,}5 + \\left(\\frac{10 - 3}{10}\\right)\\cdot 12 = 95{,}7\\text{ g dL}^{-1}\\] Para el segundo cuartil nos damos cuenta que el método arroja \\(Q_2 = Q_1\\), lo cual es un error (asegúrese de verificar este resultado, ¿por qué sucede esto?). La razón de esto es que la resolución de los datos no permite la estimación de los cuantiles dado el tamaño de la muestra (vea la discusión más adelante en la siguiente sección). En este caso, recordamos que \\(Q_2 = M\\), y usamos el valor de la mediana calculado anteriormente: \\[Q_2 = 97{,}9\\text{ g dL}^{-1}\\] Para el tercer cuartil, vemos que la clase que contiene el cuartil es la cuarta clase, por lo que: \\[Q_3 = 110{,}5 + \\left(\\frac{21 - 17}{21}\\right)\\cdot 12 = 112{,}79\\text{ g dL}^{-1}\\] De los resultados anteriores, podemos notar que la distancia entre el primer y segundo cuartil es un orden de magnitud menor que la distancia entre el segundo y tercer cuartil, indicando que las observaciones en la tercera parte de la distribución tienen una mayor dispersión, y que en la segunda parte de la distribución las observaciones se aglomeran. Esto refuerza la intuición obtenida antes con las medidas de tendencia central que la distribución está sesgada hacia la derecha. 5.2.3 Medidas de dispersión. El recorrido (o rango) y el rango intercuartílico (IQR) se calculan igual que antes para datos sin agrupar. Sin embargo, la definición de varianza la modificamos para usar las marcas de clase, y no las observaciones: \\[S^2 = \\frac{\\sum_{i=1}^k f_i(c_i - \\bar{X})^2}{n - 1}\\] A partir de esta, es posible calcular la desviación estándar y el coeficiente de variación tal como se definieron para datos sin agrupar. Se tiene que \\(R = 140 - 75 = 65\\) g dL\\({}^{-1}\\) y \\(IQR = 112{,}79 - 95{,}7 = 17{,}09\\) g dL\\({}^{-1}\\), los cuales nos indican que el 50% de las observaciones solo se encuentran ocupando aproximadamente un \\(4\\)% del dominio posible de las observaciones. La varianza es \\(S^2 = 311{,}04\\) (g dL\\({}^{-1}\\))\\({}^2\\), y \\(S = 17{,}64\\) g dL\\({}^{-1}\\) con \\(CV = 0{,}1703\\). Esto nos indica que la distribución de los datos parece no ser tan variable, pero esto puede ser engañoso ya que sabemos que la distribución esta sesgada. 5.2.4 Medidas de forma. Para el cálculo del coeficiente de asimetría y curtosis, se procede al igual que antes para datos sin agrupar, pero usamos las marcas de clases en lugar de las observaciones para realizar el cálculo. El coeficiente de asimetría se calcula como: \\[A = \\frac{\\sum_{i=1}^kf_i(c_i - \\bar{X})^3}{nS^3}\\] y la curtosis como: \\[A = \\frac{\\sum_{i=1}^kf_i(c_i - \\bar{X})^4}{nS^4} - 3\\] Para los índices de glicemia resumidos en la tabla de datos agrupados, obtenemos \\(A = 0{,}661\\) y \\(K = 2{,}32\\). Esto nos indica que la distribución es ligeramente sesgada hacia la derecha (como ya parecíamos intuir de las otras medidas) y leptocúrtica. Esto indica que el sesgo observado es resultado de observaciones atípicas. Media &lt;- sum(glicemia$f_i * glicemia$`Marca de Clase`) / 25 Std.Dev &lt;- sqrt(sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 2) / 24) tribble(~Media, ~`Desv. Est.`, ~Asimetria, ~Curtosis, Media, Std.Dev, sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 3) / (25 * Std.Dev ** 3), sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 4) / (25 * Std.Dev ** 4) ) %&gt;% kbl() Media Desv. Est. Asimetria Curtosis 103.54 17.63633 0.6609389 2.32162 Ejercicio. En un estudio del síndrome de Down, se examinaron 180 niños afectados y la siguiente tabla da la distribución de frecuencias para el cociente intelectual (IQ) de los niños. Complete la tabla de datos agrupados, añadiendo las columnas que hagan falta, y determine las medidas de tendencia central, los cuartiles, deciles y percentiles, las medidas de dispersión y de forma. Discuta los resultados. tibble( Clase = c(1, 2, 3, 4, 5, 6, 7, 8, 9), `Límites de clase` = c(&quot;10.5 a 20.5&quot;, &quot;20.5 a 30.5&quot;, &quot;30.5 a 40.5&quot;, &quot;40.5 a 50.5&quot;, &quot;50.5 a 60.5&quot;, &quot;60.5 a 70.5&quot;, &quot;70.5 a 80.5&quot;, &quot;80.5 a 90.5&quot;, &quot;90.5 a 100.5&quot;), `Marca de clase` = c(15.5, 25.5, 35.5, 45.5, 55.5, 65.5, 75.5, 85.5, 95.5), `Frecuencia f_i` = c(4, 34, 0, 70, 43, 19, 7, 2, 1) ) %&gt;% kbl() Clase Límites de clase Marca de clase Frecuencia f_i 1 10.5 a 20.5 15.5 4 2 20.5 a 30.5 25.5 34 3 30.5 a 40.5 35.5 0 4 40.5 a 50.5 45.5 70 5 50.5 a 60.5 55.5 43 6 60.5 a 70.5 65.5 19 7 70.5 a 80.5 75.5 7 8 80.5 a 90.5 85.5 2 9 90.5 a 100.5 95.5 1 "],["distribuciones-de-probabilidad..html", "6 Distribuciones de Probabilidad.", " 6 Distribuciones de Probabilidad. Hasta este punto, hemos definido ya lo que es una variable aleatoria y cómo podemos usar esta variable para definir probabilidades en intervalos de números reales. Estas funciones se pueden definir para obtener información de las características de las variables aleatorias. Las funciones usadas son la función de distribución (o función de densidad probabilística) y la función de distribución (o función de distribución acumulada). "],["función-de-probabilidad..html", "6.1 Función de Probabilidad.", " 6.1 Función de Probabilidad. Esta función es la que no da información sobre la probabilidad de un evento cualquiera dentro del espacio probabilístico. El establecer la definición de la función de probabilidad se debe hacer para los casos en los que se tienen variables aleatorias discretas, y para los casos en los que se tienen variables aleatorias continuas. Caso discreto. Sea una v.a. \\(X\\) que toma valores \\(x_0, x_1, \\ldots\\) con probabilidades \\(p_0 = P(X = x_0), p_1 = P(X = x_1), \\ldots\\) (esta es una lista infinita, pero numerable, de probabilidades asignadas a los posibles valores de \\(X\\)). Se define entonces la función de probabilidad discreta como: \\[f(x) = \\begin{cases} P(X = x), &amp; \\text{ si }x = x_0, x_1, \\ldots \\\\ 0, &amp; \\text{ de otra forma.} \\end{cases}\\] Notamos que una función definida de esta forma, cumple con todos los axiomas de Kolmogorov y es, por lo tanto, una medida de probabilidad. De lo que se tiene: \\[\\begin{align} f(x) &amp;\\ge 0 \\\\ \\sum f(x) &amp;= 1 \\end{align}\\] De esta forma, podemos definir la probabilidad de un evento cualquiera como: \\[P(X \\in A) = \\sum_{x \\in A} f(x)\\] Esto es así ya que, como vimos, \\(A\\) estaría formado por la unión de eventos disjuntos, cuya probabilidad es la sumatoria de las probabilidades individuales. Ejemplo. Sea \\(X\\) una v.a. que toma los valores \\(1,2,3\\) con probabilidades \\(0{,}3, 0{,}5, 0{,}2\\), respectivamente. Definimos la función de probabilidad como: \\[f(x) = \\begin{cases} 0{,}3, &amp; \\text{ si }x = 1 \\\\ 0{,}5, &amp; \\text{ si }x = 2 \\\\ 0{,}2, &amp; \\text{ si }x = 3 \\\\ 0, &amp; \\text{ de otra forma.} \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = 1:3, y=c(.3, .5, .2))) + geom_segment(aes(x = 1:3, y = rep(0, 3), xend = 1:3, yend = c(.3, .5, .2)), linewidth = 2, linetype = &quot;dashed&quot;, colour = &quot;gray75&quot;) + geom_point(size = 3) + geom_hline(yintercept = 0, linewidth = 3) + geom_point(aes(y = rep(0, 3)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;f(x)&quot;) + xlab(&quot;x&quot;) + theme_light(base_size = 14) + theme(panel.grid = element_blank()) a partir de la cual podemos calcular la probabilidad de cualquier evento, como por ejemplo \\(P(X\\ge2) = P(X=2) + P(X=3) + P(X=4) + \\ldots = 0{,}5 + 0{,}2 + 0 + \\ldots = 0{,}7\\) o \\(P(\\vert X\\vert = 1) = P(X = 1) + P(X = -1) = 0{,}3 + 0 = 0{,}3\\). En el ejemplo anterior vemos que no hubo necesidad de definir un experimento aleatorio para construir una función de probabilidad. Esta libertad nos permite definir arbitrariamente funciones de probabilidad en esquemas genéricos, lo único que se necesita es que obedezcan los axiomas de Kolmogorov. Ejercicio. Una muestra de \\(7\\) semillas contiene \\(2\\) infectadas con una enfermedad. Un agrónomo compra \\(3\\) de las semillas al azar. Si \\(x\\) es el número de unidades defectuosas que compra el agrónomo, calcule la distribución de probabilidad de \\(X\\). Exprese los resultados de forma gráfica como un histograma de probabilidad. Ahora definimos la función de probabilidad para el caso contínuo. Caso continuo. Sea \\(X\\) una v.a. continua. Decimos que \\(f(x)\\) es la función de densidad de la variable \\(X\\) en el intervalo \\([a, b]\\in\\mathbb{R}\\) si se cumple: \\[P(X \\in[a,b]) = \\int_a^b f(x)dx\\] donde \\(f(x)\\) es una función no negativa e integrable en el intervalo \\([a,b]\\) Bajo esta definición, es claro que se cumplen los criterios de Kolmogorov: \\[\\begin{align} f(x) &amp;\\ge 0 \\space \\forall \\space x \\in \\mathbb{R} \\\\ \\int_{-\\infty}^{\\infty} f(x) &amp;= 1 \\end{align}\\] Ejemplo. Sea \\(X\\) una v.a. continua con función de probabilidad definida como: \\[f(x) = \\begin{cases} 3x^2/2, &amp; \\text{ si } -1 &lt; x &lt; 1 \\\\ 0, &amp; \\text{ de otra forma} \\end{cases}\\] cuyo gráfico se muestra a continuación. x &lt;- seq(-1, 1, by=.1) ggplot(NULL, aes(x = x)) + geom_line(aes(y=1.5 * x ** 2), linewidth = 2) + geom_line(aes(x = c(-1.5, -1), y=c(0, 0)), linewidth = 2) + geom_line(aes(x = c(1, 1.5), y=c(0, 0)), linewidth = 2) + geom_point(aes(x=c(-1, 1), y = rep(0, 2)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;f(x)&quot;) + xlab(&quot;x&quot;) + theme_light() a partir de la cual podemos calcular la probabilidad de cualquier evento, como por ejemplo: \\[\\begin{aligned} P(X \\le 1/3) &amp;= \\int_{-\\infty}^{1/3}f(x)dx \\\\ &amp;= \\int_{-\\infty}^{-1}0dx + \\int_{-1}^{1/3}\\frac{3}{2}x^2dx \\\\ &amp;= 0 + \\frac{(1/3)^3}{2} - \\frac{(-1)^3}{2} \\\\ &amp;= \\frac{1}{54} + \\frac{1}{2} \\\\ &amp;= \\frac{14}{27} \\end{aligned}\\] Al igual que antes, no hubo necesidad de definir un experimento aleatorio para construir una función de probabilidad continua. Ejercicio. El tiempo que pasa, en segundos, para que un murciélago detecte entre árboles sucesivos a una presa que se mueve a una velocidad dada es una variable aleatoria continua con una función de distribución acumulativa: \\[F(x) = \\begin{cases} 0, &amp; x &lt; 0, \\\\ 1 - e^{-8x}, &amp; x ≥ 0 \\end{cases}\\] Calcule la probabilidad de que el tiempo que pase para que el murciélago detecte entre árboles sucesivos a las presas que exceden una velocidad de escape sea menor de 12 minutos. "],["función-de-distribución..html", "6.2 Función de distribución.", " 6.2 Función de distribución. Sea \\(X\\) una variable aleatoria cualquiera, la función de distribución, denotada como \\(F(x) : \\mathbb{R} \\rightarrow \\mathbb{R}\\) (lo cual se lee como: \\(F(x)\\) toma un número real y lo transformar en otro número real) se define como la probabilidad: \\[F(x) = P(X \\le x)\\] Vemos entonces porque la llamamos también función de probabilidad acumulada, ya que \\(F(x)\\) denota la probabilidad acumulada hasta el valor observado \\(x\\). También notamos que, como las probabilidades son todas mayores o iguales a cero, y que la probabilidad del espacio muestral en su totalidad es \\(1\\), la función de distribución se define entre \\(0\\) y \\(1\\). Al igual que antes, se hace necesario distinguir entre la función de distribución en el caso discreto y en el caso continuo. Caso discreto. Si \\(X\\) es una v.a. discreta con función de distribución \\(f(x)\\), entonces se define: \\[F(x) = \\sum_{t \\le x} f(t)\\] Ejemplo. Para el ejemplo anterior dado para la función de probabilidad de una v.a. discreta, podemos construir la función de distribución considerando todos los intervalos donde la probabilidad se mantenga contante. De esta forma obtenemos: \\[F(x) = \\begin{cases} 0, &amp; x &lt; 1 \\\\ 0{,}3, &amp; 1 \\le x &lt; 2 \\\\ 0{,}8, &amp; 2 \\le x &lt; 3 \\\\ 1, &amp; x \\ge 3 \\\\ \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = 1:3, y=c(.3, .8, 1))) + geom_point(size = 3) + geom_line(aes(x = c(0, 1), y = c(0, 0)), linewidth = 2) + geom_line(aes(x = c(1, 2), y = c(.3, .3)), linewidth = 2) + geom_line(aes(x = c(2, 3), y = c(.8, .8)), linewidth = 2) + geom_line(aes(x = c(3, 4), y = c(1, 1)), linewidth = 2) + geom_point(aes(x = c(1, 2, 3), y = c(0, .3, .8)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;F(x)&quot;) + xlab(&quot;x&quot;) + theme_light() Caso continuo. Si \\(X\\) es una v.a. continua con función de distribución \\(f(x)\\), entonces se define: \\[F(x) = \\int_{-\\infty}^x f(t)dt\\] Ejemplo. Se tiene una variable aleatoria \\(X\\) con función de probabilidad dada por: \\(f(x) = \\begin{cases}\\vert x\\vert, &amp; -1 &lt; x &lt; 1 \\\\ 0 &amp; \\text{ de otra forma}\\end{cases}\\). La obtención de la función de distribución se obtiene aplicando la definición en cada intervalo en los que la definición de \\(f(x)\\) cambia. Empezando con el intervalo de \\((-\\infty, -1)\\), se tiene: \\[F(X) = \\int_{-\\infty}^-1 0dx = 0\\] Luego, en el intervalo \\([-1,0)\\) se tiene: \\[F(X) = \\int_{-1}^x -xdx = (1-x^2)/2\\] y así seguimos hasta obtener \\(F(x)\\) en todos los reales: \\[F(x) = \\begin{cases} 0, &amp;x \\le -1 \\\\ (1-x^2)/2, &amp;-1 \\le x &lt; 0 \\\\ (1+x^2)/2, &amp;0 &lt; x \\le 1 \\\\ 1, &amp;x \\ge 1 \\\\ \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = c(-1.5, -1), y=c(0, 0))) + geom_line(linewidth = 2) + geom_line(aes(x = seq(-1, 0, by = .1), y = (1 - seq(-1, 0, by = .1) ** 2) / 2), linewidth = 2) + geom_line(aes(x = seq(0, 1, by = .1), y = (1 + seq(0, 1, by = .1) ** 2) / 2), linewidth = 2) + geom_line(aes(x = c(1, 1.5), y = c(1, 1)), linewidth = 2) + ylab(&quot;F(x)&quot;) + xlab(&quot;x&quot;) + theme_light() 6.2.1 Propiedades de la función de distribución. La función de distribución resulta ser muy importante desde el punto de vista matemático, pues siempre puede definirse dicha función para cualquier variable aleatoria y a través de ella quedan representadas todas las propiedades de la variable aleatoria. Cualquier función que cumpla las siguientes propiedades es una función de distribución, sea que tenga o no una variable aleatoria asociada. \\(F(x)\\) está acotada por arriba por \\(1\\), lo cual se puede escribir como \\[\\lim_{x\\rightarrow\\infty} F(x) = 1\\]. Esto es así dado que la probabilidad de todo el espacio muestral es \\(1\\). \\(F(x)\\) está acotada por abajo por \\(0\\), lo cual se puede escribir como \\[\\lim_{x\\rightarrow-\\infty} F(x) = 0\\]. Esto es así dado que las probabilidades son no negativas. \\(F(x)\\) es monótona no decreciente, esto es, si \\(x_1 \\le x_2\\), entonces \\(F(x_1) \\le F(x_2)\\). \\(F(x)\\) es continua por la derecha, lo cual se puede escribir como \\[F(x) = \\lim_{x\\rightarrow x_0^+} F(x)\\]. "],["cálculos-con-funciones-de-probabilidades..html", "6.3 Cálculos con funciones de probabilidades.", " 6.3 Cálculos con funciones de probabilidades. "],["distribuciones-de-probabilidad-de-variables-discretas..html", "7 Distribuciones de probabilidad de variables discretas.", " 7 Distribuciones de probabilidad de variables discretas. "],["distribuciones-de-probabilidad-de-variables-continuas..html", "8 Distribuciones de probabilidad de variables continuas. ", " 8 Distribuciones de probabilidad de variables continuas. "],["distribución-normal..html", "8.1 Distribución Normal.", " 8.1 Distribución Normal. La distribución normal (o gaussiana) es de las distribuciones más importantes que estudiaremos. Esta la usaremos con frecuencia más adelante cuando realicemos inferencias a partir de observaciones realizadas en un experimento. Aparecerá primero en el teorema del límite central (capítulo Teoría de Muestreo.), que es uno de los teoremas más importantes que tiene aplicaciones directas en la práctica. Decimos que una variable aleatoria \\(X\\) tiene una distribución normal si su función de densidad viene dada por: \\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty,\\] donde \\(\\mu, \\sigma^2 \\in \\mathbb{R}\\), con \\(\\sigma^2 &gt; 0\\), son los parámetros de la distribución. Si la variable \\(X\\) se distribuye como normal se escribe: \\[X \\sim N(\\mu, \\sigma^2)\\] La grafica de la función de densidad normal tiene forma de campana, siendo simétrica con respecto a la vertical que pasa por la media \\(\\mu\\), la cual es el centro de la campana. Siendo \\(\\sigma\\) (raíz cuadrada de la varianza \\(\\sigma^2\\)) es la distancia del centro a cualquiera de los puntos de inflexión de la curva (como se muestra en la figura 8.1). Figure 8.1: Función de densidad de una variable aleatoria normal N(\\(\\mu, \\sigma^2\\)). Esta información se resume como: \\[\\begin{aligned} &amp; E(X) = \\mu \\\\ &amp; Var(X) = \\sigma^2 \\end{aligned}\\] La función de distribución viene dada por la integral: \\[F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}dy\\] la cual no tiene primitiva analítica asociada y debe resolverse por métodos numéricos. Es posible obtener valores de probabilidad acumulada en R usando el comando pnorm, el cual da \\(F(x) = P(X \\le x)\\), como se muestra en la figura siguiente. Figure 8.2: Función de distribución acumulada de una variable aleatoria normal N(\\(\\mu, \\sigma^2\\)). Por ejemplo, supongamos que se tiene la variable aleatoria \\(X\\) que representa la longitud del ala de abejas de cierta granja de apicultores. Se sabe, de estudios anteriores, que la longitud media es de \\(1{,}5 \\pm 0{,}73\\) cm. Los mismos estudios preliminares han mostrado que \\(X\\) sigue una distribución normal, por lo que se escribe: \\[X \\sim N(\\mu = 1{,}5, \\sigma^2 = 0.533)\\] Podemos obtener la probabilidad de que una abeja tenga una longitud del ala menor a \\(1\\) cm, \\(P(X \\le 1\\text{ cm})\\) como pnorm(1, 1.5, 0.73) (figura 8.3, a la izquierda). Si queremos la probabilidad de aquellas con una longitud del ala mayor a \\(3\\) cm, \\(P(X \\ge 3\\text{ cm}) = 1 - P(X \\le 3\\text{ cm})\\), que se calcula en R como 1 - pnorm(3, 1.5, 0.73) (figura 8.3, a la derecha). Figure 8.3: Función de densidad de una variable aleatoria normal N(\\(\\mu = 1{,}5, \\sigma^2 = 0{,}532\\)), donde se muestra de forma grafica la probabilidad acumulada \\(P(X \\le 1)\\) (a la izquierda) y \\(P(X \\ge 3)\\) (a la derecha). Notamos, de las figuras del ejemplo, que la probabilidad acumulada se puede entender como el área debajo de la función de densidad para un valor de \\(X\\) observado o menor, y que para encontrar valores acumulados hacia arriba, solo necesitamos usar \\(1 - F(X)\\). Ejercicio. Un investigador informa que unos ratones a los que primero se les restringen drásticamente sus dietas y después se les enriquecen con vitaminas y proteínas vivirán un promedio de \\(40\\) meses. Si suponemos que la vida de tales ratones se distribuye normalmente, con una desviación estándar de \\(6{,}3\\) meses, calcule la probabilidad de que un ratón determinado viva a) más de \\(32\\) meses; b) menos de \\(28\\) meses; c) entre \\(37\\) y \\(49\\) meses. Un caso particular de esta distribución, que es muy útil, es cuando \\(\\mu = 0\\) y \\(\\sigma^2 = 1\\), la cual da lugar a la distribución normal estándar, cuya función de densidad queda: \\[f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}, \\quad -\\infty &lt; x &lt; \\infty,\\] la cual también se denota como \\(\\phi(x)\\). Esto es importante porque significa que siempre es posible transformar una v.a. normal en una estándar por una simple operación: \\[Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\] que se conoce como estandarización. La importancia de este procedimiento es que el cálculo de probabilidades de una variable aleatoria normal se puede reducir al cálculo de probabilidad de una variable aleatoria de distribución normal estándar. Esto es fácil de ver ya que: \\[\\begin{aligned} P(a &lt; X &lt; b) &amp;= P(a - \\mu &lt; X - \\mu &lt; b - \\mu) \\\\ &amp;= P\\left(\\frac{a - \\mu}{\\sigma} &lt; \\frac{C - \\mu}{\\sigma} &lt; \\frac{b - \\mu}{\\sigma}\\right) \\\\ &amp;= P\\left(\\frac{a - \\mu}{\\sigma} &lt; Z &lt; \\frac{b - \\mu}{\\sigma}\\right) \\end{aligned}\\] Se puede demostrar que si \\(X\\) se distribuye como una normal estándar, entonces la variable \\(-X\\) también tiene distribución normal estándar, y: \\[\\Phi(-x) = 1 - \\Phi(x)\\] Definimos los cuantiles de la distribución normal estándar \\(z_\\alpha\\) para cada valor de \\(\\alpha\\) en el intervalo \\((0,1)\\) como aquel para el cual: \\[\\Phi(z_{\\alpha}) = 1 - \\alpha\\] Algunos cuantiles importantes que vale la pena recordar, y que usaremos frecuentemente más adelante son: \\(z_{0{,}9} = 1{,}28\\), \\(z_{0{,}95} = 1{,}64\\), \\(z_{0{,}975} = 1{,}96\\) y \\(z_{0{,}99} = 2{,}33\\). Ahora, mencioanmos una proposición muy útil sobre la suma de dos variables aleatorias normales. Sean \\(X_1\\) y \\(X_2\\) dos variables aleatorias independientes con distribución \\(N(\\mu_1, \\sigma_1^2)\\) y \\(N(\\mu_2, \\sigma_2^2)\\), entonces: \\[X_1 + X_2 \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\] Veamos un ejemplo. Por ejemplo, digamos que un investigador tiene una población de ciertas bacterias en un cultivo puro que se esta estudiando por sus capacidades de producir cierta proteína transmembrana de interés, que sirve como transportador de un metabolito que se desea degradar. Se sabe que esta se produce en este cultivo con una densidad de \\(35 \\pm 4{,}3\\) unidades por centímetro cuadrado por célula. Sin embargo, en un accidente, el investigador mezclo dos cultivos con capacidades distintas de producir la proteína transmembrana. Este segundo cultivo tiene una capacidad menor de producir la proteína que funciona como transportador, haciendola menos efectiva en metabolizar el metabolito, con una densidad de solo \\(11 \\pm 7{,}1\\) unidades por centímetro cuadrado por célula. Si suponemos que la v. a. densidad de la proteína por centímetro cuadrado por célula se distribuye como una normal, entonces \\(X_{\\text{Cultivo 1}}\\sim N(35, 18{,}49)\\) y \\(X_{\\text{Cultivo 2}}\\sim N(11, 50{,}41)\\) en ambos cultivos aislados; y luego del accidente, \\(X_{\\text{Cultivo Mezclado}}\\sim N(46, 68{,}9)\\). "],["distribución-ji-cuadrada..html", "8.2 Distribución Ji-Cuadrada.", " 8.2 Distribución Ji-Cuadrada. Se dice que una variable aleatoria continúa \\(X\\) sigue una distribución Ji-Cuadrada con \\(n\\) grados de libertad (\\(n&gt;0\\)), si su función de densidad viene dada por: \\[f(x) = \\begin{cases} \\frac{1}{\\gamma(n/2)}\\left(\\frac{1}{2}\\right)^{n/2}x^{n/2 - 1}e^{-x/2} &amp;\\text{ si }x &gt; 0 \\\\ 0 &amp;\\text{ en otro caso}\\end{cases}\\] Esta función se distribuye en el intervalo \\((0, \\infty)\\) y su único parámetro son los grados de libertad \\(n\\) que puede ser cualquier valor positivo, aunque la mayoría de las veces tomará solo valores enteros positivos. En la figura 8.4 se muestra esta distribución para \\(n = 1, 2, 5\\), y \\(8\\): a partir de \\(n=3\\) aparece un pico en la función, el cual se desplaza a valores mayores a medida que \\(n\\) aumenta. Figure 8.4: Función de densidad de una variable aleatoria Ji-Cuadrada \\(\\chi^2(n)\\). Si \\(X\\) sigue una distribución Ji-Cuadrada, escribiremos \\[X \\sim \\chi^2(n)\\] Su función de distribución viene dada por: \\[F(x) = \\int_0^{x} \\frac{1}{\\gamma(n/2)} \\left(\\frac{1}{2}\\right)^{n/2}u^{n/2 - 1}e^{-u/2}du\\] cuyo gráfico se muestra a comtinuación para una v. a. \\(X \\sim \\chi^2(n = 8)\\): Figure 8.5: Función de distribución acumulada de una variable aleatoria \\(\\chi^2(n = 8)\\). Es posible obtener valores de probabilidad acumulada en R usando el comando pchisq, el cual da \\(F(x) = P(X \\le x)\\). Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = n \\\\ &amp; Var(X) = 2n \\end{aligned}\\] Antes de ver un ejemplo de calculo de porbabilidades a partir de la función de distribución, es bueno conocer los siguientes resultados. La distribución Ji-cuadrada se puede obtener como resultado de elevar al cuadrado una variable normal estándar. Si \\(X \\sim N (0, 1)\\), entonces: \\[X^2 \\sim \\chi^2(1)\\] Este resultado, junto con la siguiente proposición, nos permitirán entender la distribución de v. a. con distribución Ji-Cuadrada. Sea \\(X\\) y \\(Y\\) dos v. a. independientes con distribución \\(\\chi^2(n)\\) y \\(\\chi^2(m)\\), respectivamente. Entonces: \\[X + Y \\sim \\chi^2(n + m)\\] Este resultado se puede extender a la suma de \\(n\\) variables aleatorias independientes distribuidas como Ji-cuadrado. Esto nos dice que podemos entender cualquier variable aleatoria que sigue una distribución Ji-Cuadrado como una suma de v. a. con la misma distribución pero cada una con un solo grado de libertad, cada una de las cuales se entiende como una v. a. normal estándar al cuadrado. Se nos permite obtener el siguiente resultado que utilizaremos más adelante cuando hablemos de inferencia estadística, y que es tan importante para realizar inferencia sobre la varianza. Sean \\(X_1, \\ldots, X_n\\) variables aleatorias independientes, cada una de ellas con distribución \\(N(\\mu, \\sigma^2)\\). Entonces: \\[\\frac{(n - 1)S^2}{\\sigma^2} \\sim \\chi^2(n - 1)\\] donde \\(S^2 = \\frac{1}{n - 1}\\sum_{i=1}^n(X_i - \\bar{X})^2\\) y \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\). Veamos un ejemplo. Por ejemplo, se sabe que en madres que no consumieron cocaína al nacer, el peso de los bebés nacidos tiene una desviación entándar de \\(\\sigma = 696\\) g. En un estudio de los efectos que tiene el consumo de cocaína sobre los bebés durante el embarazo, se recolectaron datos de \\(n = 190\\) madres consumidoras de cocaína. Por la proposión anterior, sabemos que la variabilidad en el peso de los bebés con respecto al valor conocido se distribuye como una \\(\\chi^2(n - 1)\\). Podemos calcular la probabilidad de que la variabilidad sea la mitad del valor conocido de \\(696\\) g, calculando la variable aleatoria \\((190 - 1) \\frac{(1/2)\\sigma^2}{\\sigma^2} = 189/2 = 94{,}5\\), cuya probabilidad se puede calcular en R como pchisq(94.5, 189) que arroja un valor de 0 (se muestra en la gráfica de la izquierda de la figura 8.6). Si la variabilidad es aproximadamente 8% menor al valor conocido, entonces \\((190 - 1) \\frac{0{,}92\\sigma^2}{\\sigma^2} = 173{,}88\\), entonces se calcula pchisq(173.8, 189) que arroja 0.2222 (que se muestra en la gráfica de la derecha de la figura 8.6) Figure 8.6: Función de densidad de una variable aleatoria \\(\\chi^2(n = 189)\\), donde se muestra de forma grafica la probabilidad acumulada \\(P(X \\le 94{,}5)\\) (a la izquierda) y \\(P(X \\le 173{,}88)\\) (a la derecha). "],["distribución-t-student..html", "8.3 Distribución \\(t\\)-Student.", " 8.3 Distribución \\(t\\)-Student. Se dice que un variable aleatoria continua \\(X\\) sigue una distribución \\(t\\)-Student con \\(n\\) grados de libertad (\\(n&gt;0\\)), si su función de densidad viene dada por: \\[f(x) = \\frac{\\Gamma(\\frac{n + 1}{2})}{\\sqrt{n\\pi}\\Gamma(n/2)}\\left(1 + \\frac{x^2}{n}\\right)^{-(n+1)/2}, \\quad -\\infty &lt; x &lt; \\infty\\] y se escribe: \\[X \\sim t(n)\\] en donde \\(n\\) es un número real positivo, aunque tomaremos principalmente el caso cuando \\(n\\) es entero positivo. Figure 8.7: Función de densidad de una variable aleatoria \\(t\\)-Student \\(t(n)\\), para \\(n = 1\\) (línea sólida), \\(n = 4\\) (línea quebrada) y \\(n = 15\\) (línea punteada). En rojo se muestra la distribución normal estándar. La función de densidad es de campana como la normal, pero con colas más pesadas que esta última, esto es, la probabilidad de obtener una observación extrema es mayor que la probabilidad de esa misma observación proviniendo de una distribución normal. A medida que aumentan los grados de libertad, la amplitud de las colas disminuye y la distribución se aproxima a una normal, y en el límite cuando \\(n\\rightarrow\\infty\\), ambas densidades coinciden. La función de distribución tampoco tiene una expresión sencilla y se escribe como: \\[F(x) = \\int_{-\\infty}^\\infty \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi}\\Gamma(n/2)} \\left(1 + \\frac{u^2}{n}\\right)^{-(n+1)/2} du\\] cuyo gráfico se muestra a continuación para una \\(t\\)-Student de \\(4\\) grados de libertad. Figure 8.8: Función de distribución acumulada de una variable aleatoria \\(t(n = 4)\\). Es posible obtener valores de probabilidad acumulada en R usando el comando pt, el cual da \\(F(x) = P(X \\le x)\\) (vea el ejemplo al final de esta subsección). Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = 0, \\quad n &gt; 1 \\\\ &amp; Var(X) = \\frac{n}{n - 2}, \\quad n &gt; 2 \\end{aligned}\\] Esta distribución resulta cuando se estudian ciertas operaciones entre variables aleatorias. Un resultado que usaremos seguido en inferencia estadística es el siguiente: Si \\(X \\sim N(0,1)\\) y \\(Y \\sim \\chi^2(n)\\) son dos variables aleatorias independientes, entonces: \\[\\frac{X}{\\sqrt{Y/n}} \\sim t(n)\\] También se puede llegar al siguiente resultado: Sean \\(X_1, \\ldots, X_n\\) v. a. independientes, cada una de ellas con distribución normal \\(N(\\mu, \\sigma^2)\\). Entonces: \\[\\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t(n - 1)\\] donde \\(S^2 = \\frac{1}{n - 1}\\sum_{i=1}^n(X_i - \\bar{X})^2\\) y \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\). Veamos un ejemplo. Ejemplo. Un programa que busca probar la eficacia de productos microbiológicos por su capacidad de degradar contaminantes derivados del petroleo (de tal forma que se puedan limpiar los ecosistemas que sufren el efecto del derrames amplios en las costas) es puesto en marcha. Investigaciones iniciales muestran que el producto es capaz de disminuir la cantidad de derivados del petroleo en un promedio de \\(10\\) ppm. Las pruebas del producto, basadas en una muestra de \\(5\\) recolecciones al azar de agua contaminadas y se les aplica el producto y se encuentra una variabilidad en la apcidad de degradación de \\(5{,}2\\) ppm. La proposición anterior entoncs muestra que la variable aleatoria \\[\\frac{\\bar{X} - 10}{5{,}2/\\sqrt{5}} \\sim t(4)\\] Por lo que podemos calcular la probabilidad de que al aplicar el producto se reduzca la contamianción por petroleo a \\(5\\) ppm, introduciendo este valor en la ecuación anterior, obteniendose \\((5 - 10)/5{,}2/\\sqrt{5} = -0{,}4300\\). Luego usamos la función pt como en pt(-0.43, 4) que arroja un valor de 0.3447 (como se ve en la figura 8.9, a la izquierda). Si queremos la probabilidad de que la contaminación solo descienda hasta \\(20\\) ppm como mínimo, entonces se calcula \\((20 - 10)/5{,}2/\\sqrt{5} = 0{,}86\\), cuya probabilidad se puede calcular como 1 - pt(0.86, 4), cuyo valor es 0.7809 (como se ve en la figura 8.9, a la derecha). Figure 8.9: Función de distribución acumulada de una variable aleatoria \\(t(n = 4)\\). "],["distribución-f..html", "8.4 Distribución \\(F\\).", " 8.4 Distribución \\(F\\). Se dice que la variable aleatoria continua \\(X\\) tiene una distribución \\(F\\) de Fisher-Snedecor con \\(a &gt; 0\\) y \\(b &gt; 0\\) grados de libertad si su función de densidad viene dada por: \\[f(x) = \\begin{cases} \\frac{\\Gamma(\\frac{a + b}{2})}{\\Gamma(\\frac{a}{2})\\Gamma(\\frac{b}{2})}\\left(\\frac{a}{b}\\right)^{a/2}x^{a/2 - 1}\\left(1 + \\frac{a}{b}x\\right)^{-(a + b)/2} &amp; \\text{si }x &gt; 0 \\\\ 0 &amp; \\text{de otra forma.} \\end{cases}\\] y se escribe \\[X \\sim F(a, b)\\] La gráfica de \\(f(x)\\) se muestra a continuación. Figure 8.10: Función de densidad de una variable aleatoria \\(F(a, b)\\), para difernetes combinaciones de los parámetros. Es posible obtener valores de probabilidad acumulada en R usando el comando pf, el cual da \\(F(x) = P(X \\le x)\\), que no tiene una expresión sencilla reducida. Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = \\frac{b}{b - 2}, \\quad b &gt; 2 \\\\ &amp; Var(X) = \\frac{2b^2(a + b - 2)}{a(b - 2)^2(b - 4)}, \\quad n &gt; 4 \\end{aligned}\\] La distribución \\(F(a, b)\\) aparece como resultado de realizar operciones entre variables aleatorias con distribución Ju-Cuadrada, como se muestra en la siguiente proposición. Sean \\(X\\) y \\(Y\\) dos variables aleatorias independientes con distribución \\(\\chi^2(a)\\) y \\(\\chi^2(b)\\), respectivamente. Entonces: \\[\\frac{X/a}{Y/b} \\sim F(a, b)\\] "],["ejercicios..html", "8.5 Ejercicios.", " 8.5 Ejercicios. Se ha realizado un estudio sobre la velocidad en vuelo de diversas especies de pájaros. El propósito era comparar las velocidades de dos especies de aves relacioandas entre sí, pero que se ha visto tienen tiempos de vuelo similares, pero una de ellas es más variable que la otra dada la longitud mas variable de la envergadura de las alas. Estudios previos muestran que la velocidad de vuelo de la especie con tiempos de vuelo menos variable, volando con el viento de costado con una velocidad de viento de \\(5\\) a \\(8\\) millas h\\({}^{-1}\\), en promedio, es \\(26{,}05 \\pm 3{,}20\\) millas h\\({}^{-1}\\). Si se tomará una muestra de \\(12\\) individuos de esta población, determine la probabilidad de que la variabilidad a) sea mayor a \\(6\\) milla h\\({}^{-1}\\), b) sea menor a \\(1\\) milla h\\({}^{-1}\\), c) este entre \\(2{,}5\\) y \\(4{,}3\\) millas h\\({}^{-1}\\). Un proceso industrial nuevo quiere lograr conseguir producir biocombustible a base de microalgas. Para hacer rentable esto, se requiere conseguir una producción neta de biomasa de microalgas de \\(5{,}6\\) g L\\({}^{-1}\\). a) Determine la probabilidad de lograr alcanzar esta cantidad de biomasa o más, si el proceso actual genera en promedio solo \\(2{,}5 \\pm 0{,}89\\) g L\\({}^{-1}\\), asumiendo que la distribución de bioamsa es una normal. Un cambio en las condiciones de cultivo han logrado aumentar el rendimiento del proceso a \\(4{,}3 \\pm 0{,}78\\) g L\\({}^{-1}\\). Determine la probabilidad de b) alcanzar, al menos, el valor de \\(5{,}6\\) g L\\({}^{-1}\\) bajo este nuevo escenario, c) de lograr una producción de biomasa entre \\(4{,}0\\) y \\(5{,}6\\) g L\\({}^{-1}\\). Se han desarrollado aortas artificiales a base de celulosa que requieren tengan un diametro de interno de \\(20\\) mm para que sea efectiva. A partir de una muestra de \\(5\\) aortas producidas se encuentra que el diametro medio es de \\(18{,}2 \\pm 0{,}9\\) mm. ¿Cuál es la probabilidad de que a) el diámetro sea de, al menos, el valor requerido?, b) el diámetro este entre \\(19\\) y \\(23\\) mm?, c) el diámetro se como mínimo de \\(17\\) mm? La productividad de un proceso de cultivo de papás hidropónico en una planta de producción nueva en Valencia es de \\(27 \\pm 5\\) kg semanales. Calcule la probabilidad de a) que la productividad sea mayor a \\(30\\) kg semanales, b) que la productividad este entre los \\(25\\) y \\(30\\) kg semanales, c) que la producción sea menor a los \\(15\\) kg. Asuma que la productividad semanal sigue una distribución normal. En estudios de herencia, es posible saber si un carácter se hereda de forma autosómica o sexual, simplemente estudiando la progenie de un primer entrecruzamiento, que denominamos F1, verificando si la proporción de individuos que presentan los caracteres siguen las proporciones mendelianas \\(3:1\\). Para ello, se hace uso de la variable aleatoria: \\[X^2 = \\sum_{i=1}^k \\frac{d_i^2}{e_i} = \\sum_{i=1}^k \\frac{(o_i - e_i)^2}{e_i}\\] la cual se distribuye como una chi-cuadrado con \\(k-1\\) grados de libertad, donde \\(k\\) es el número de clases (o fenotipos) expresados por el gen considerado, y \\(e_i\\) es el valor esperado. En un experimento clásico, se contabilizaron el número de semillas rugosas y lisas obtenidas en F1 luego de un entrecruzamiento de semillas lisas y rugosas. El número de semillas lisas contabilizado fue de \\(384\\) semillas lisas, y \\(118\\) semillas rugosas. ¿Cuál es la probabilidad de que el caracter se transmita de forma autosómica? "],["inferencia-estadística..html", "9 Inferencia Estadística.", " 9 Inferencia Estadística. La inferencia es la parte de la estadística que se encarga de formalizar el proceso de estimación y contrastes de hipótesis. Su problema fundamental consiste en poder derivar declaraciones acerca de un fenómeno natural de interés a partir de observaciones realizadas del fenómeno. Las declaraciones de las que se hablan, son declaraciones en un sentido estadístico: esto es, las declaraciones se establecen con cierto grado de veracidad. No son verdades universales, sino que están sujetas a errores. El problema de la inferencia estadística es cuantificar que tan seguro estamos sobre esas declaraciones. La razón de que las declaraciones estén sujetas a variabilidad tiene que ver con las observaciones que realizamos del fenómeno. Las observaciones se usan para realizar inferencias acerca de las características o propiedades particulares del fenómeno en estudio. Estas observaciones no son perfectas y están limitadas a los recursos que posee el investigador para llevarlas a cabo: No son perfectas ya que cualquier medición está sujeta a que tan preciso es el instrumento con el que medimos (eso incluye nuestros sentidos). Además, las observaciones contienen una variabilidad inherente que es debida solo al azar. Esto hace que se tenga cierta incertidumbre al realizar mediciones, que son desviaciones aleatorias del valor real de lo que se está midiendo. Son limitadas dado que no se tiene siempre el dinero, el tiempo, o la energía para recolectar toda la información disponible. Esto hace que no se disponga siempre de toda la información que pueda ayudarnos a estudiar un fenómeno particular, sino que solo un subconjunto de esa información. En conclusión, las observaciones realizadas del fenómeno en cuestión contienen variación aleatoria que hacen imposible el observar directamente la característica o propiedad que se está estudiando, y dado que las declaraciones derivan de estas observaciones, se necesita cuantificar esta variabilidad/incertidumbre, necesitándose así modelos estocásticos para poder tratar con esta variación. Es por ello que se hace necesario de modelos estadísticos con los cuales manejar los datos. Estos modelos se originan de la matemática deductiva (aquellos que comienzan con teorías generales y que, por argumentos lógicos, se llega a conclusiones específicas), pero no necesariamente son los correctos, y esto hace que estén sujetos a incertidumbre. Obtener información (observaciones) no nos permite decir que modelo es el correcto. Simplemente no sabemos: al realizar inferencia y obtener declaraciones de estas, asumimos un modelo correcto y analizamos los datos bajo esta premisa, y toleramos/soportamos la posibilidad de caer en un error debido a una mala elección del modelo. Por ejemplo, al hablar de inferencia en los próximos capítulos, estaremos suponiendo que la distribución subyacente a los datos en una distribución normal. Esta suposición bajo la cual analizamos los datos y hacemos contrastes puede no ser la correcta, por lo que cualquier conclusión que derive de esas pruebas puede ser errada. Podríamos decidir usar otro modelo, otra distribución subyacente a partir de la cual hacer inferencias, pero aun así, este modelo podría no ser correcto de todos modos. Podemos cuantificar que tanto podemos aceptar la suposición de partida, pero estas decisiones también estarían sujetas a incertidumbre. Es un trade-off entre la necesidad de analizar los datos y la probabilidad de caer en un error debido a esa elección de un modelo. Esta incertidumbre de la que hablamos en el último apartado es lo que se conoce como incertidumbre inductiva y es esto lo que hace que los problemas estadísticos sean inductivos: se parte de las observaciones realizadas sobre una característica/propiedad que no podemos observar directamente al realizar un experimento. Es esta incertidumbre la que hace a las declaraciones derivadas de la inferencia, falibles. Para puntualizar, decimos que existen dos tipos de incertidumbre: Incertidumbre estocástica: es aquella que está relacionada a la aleatoriedad de las observaciones, y la capacidad de estas de dar información sobre parámetros fijos. Se puede manejar al aumentar el tamaño del experimento. Incertidumbre inductiva: se debe a que la información es incompleta al elegir un modelo. Aunque la anterior es fácil de manejar, esta no. Puede ser imposible cuantificarla o controlarla. La idea general de la inferencia es poder cuantificar la incertidumbre estocástica y explicar la variabilidad observada en los datos, pero el mecanismo subyacente no es tan importante de explicar. El problema es que la incertidumbre inductiva tiende a incrementar la incertidumbre estocástica, pero siempre podemos realizar análisis hasta tener un razonable control sobre esta última. Esta distinción entre tipos de incertidumbre y el manejo de ambas, es lo que hace que diferentes investigadores puedan llegar a distintas conclusiones. "],["cómo-se-enfrenta-a-la-inferencia-estadística.html", "9.1 ¿Cómo se enfrenta a la Inferencia Estadística?", " 9.1 ¿Cómo se enfrenta a la Inferencia Estadística? Básicamente, las corrientes de pensamiento tienen que ver sobre cómo se plantea la visión de probabilidad: Como la frecuencia esperada a la larga (luego de repetir el experimento muchas veces); o Como una noción subjetiva de incertidumbre. Por ejemplo: Si lanzamos una moneda, tenemos un sentido de incertidumbre acerca del resultado: decimos que la probabilidad de obtener una cara es de \\(0{,}5\\). Ahora, pensemos en el siguiente lanzamiento: ¿podemos decir que la incertidumbre sobre el resultado sigue siendo \\(0{,}5\\)?¿o la probabilidad de \\(0{,}5\\) solo tiene sentido a la larga? Si obtenemos cara durante el primer lanzamiento, la probabilidad de obtener otra cara, dado que el resultado anterior, viene dado por el teorema de Bayes. Sin embargo, el uso de este teorema requiere que especifiquemos la distribución de probabilidad a priori, y es aquí donde está el problema entre los frecuentistas y bayesianos: Los frecuentistas, estresarían el hecho de que lo que importa es la probabilidad a la larga, sin importar cuanta información tengamos a partir de los datos. Por lo que la distribución de probabilidad subyacente es \\(0{,}5\\) para ellos. Los bayesianos podrían concordar con los frecuentistas, pero también podrían inclinarse a darle importancia a la información que se tiene actualmente. Al haber solo un lanzamiento, la distribución binomial asigna una probabilidad a priori distinta de \\(0{,}5\\) para el siguiente lanzamiento. En este curso solo hablaremos de estadística en un sentido frecuentista, para mayor información sobre el enfoque bayesiano puede consultar @gelman1995 (otro enfoque posible es uno intermedio entre ambas posturas, frecuentista y bayesiana, que se basa en la idea de una probabilidad fiduciaria o función de verosimilitud. Para más información sobre este enfoque pueden consultar @pawitan2001all). Las dos problemáticas principales para un frecuentista son: La elección de una distribución a priori apropiada. El desacuerdo sobre el accionar bajo grados de creencias personales. Sin embargo, la incertidumbre inductiva es mayor por varios ordenes de magnitud a cualquier diferencia en la incertidumbre estocástica que resulta de analizar datos siguiendo el criterio de cualquier escuela de pensamiento. "],["frecuentistas-y-muestreo-repetido..html", "9.2 Frecuentistas y muestreo repetido.", " 9.2 Frecuentistas y muestreo repetido. Los frecuentistas, ven la probabilidad como una frecuencia a la larga, suponiendo que un experimento se repite, de forma hipotética, muchas veces. Es decir, se basa en el principio de muestreo repetido bajo las mismas condiciones. Para ellos, cualquier parámetro de importancia es fijo, y no puede tratarse como una variable aleatoria. Se trata de entender la relación entre el estimado de una propiedad que podemos calcular y el valor real de esa propiedad, imaginándonos como podría el resultado cambiar si en lugar de la muestra seleccionada, se hubiese recolectado otra igualmente probable. Imagine que desea conocer el peso promedio de una población de patos negros, que se distribuye como \\(N(1161\\text{ g}, 9604\\text{ g}^2)\\) (no necesariamente usted ssbe qje esta es la fistribución y, en general, no lo ssbe. Solo usamos este conocimiento para poder ralizqr simulaciones en este ejemplo), por lo que se toma una muestra aleatoria de \\(n = 50\\) patos. En esta muestra, encuentra que el peso promedio es \\(\\hat{\\mu} = 1158{,}2\\) g. Este peso promedio es una estimación del peso promedio verdadero de la población de \\(\\mu=1161\\) g. Ahora, supongamos que se realiza este experimento muchas veces, unas 10 mil veces digamos, y en cada repetición calculamos el peso promedio. En R, podemos lograr hacer esto de la siguiente forma: # Se asume media poblacional de 1161 g y desviacion estandar poblacional de 98 g bd_wieghts &lt;- tibble(duck_id = 1:2300, weight = rnorm(2300, 1161, 98)) # Se realizan 10000 muestreos aleatorios de 50 observaciones de la población. virtual_samples &lt;- bd_wieghts %&gt;% rep_sample_n(size = 50, reps = 10000) # A cada replica se le calcula el peso promedio virtual_bd_weights &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(`Peso Promedio` = mean(weight)) virtual_bd_weights %&gt;% head(10) %&gt;% kbl() replicate Peso Promedio 1 1149.753 2 1168.715 3 1166.180 4 1160.754 5 1146.726 6 1167.976 7 1144.318 8 1159.534 9 1132.782 10 1162.580 Se puede observar de la tabla anterior, que algunas muestras resultan en un peso mayor al valor real de \\(1161\\) g, y otras en valores menores; y si inspeccionáramos cuidadosamente los resultados, podríamos verificar si de hecho el peso promedio en alguna replica es igual al valor de peso verdadero. Cada uno de esos promedios corresponde a un estimador \\(\\hat{\\mu}\\) del verdadero valor medio \\(\\mu\\). Podríamos realizar un histograma de estos valores y veríamos que el verdadero valor esta en el centro de la distribución de valores medios obtenidos de las 10 mil replicas. # Se realiza un grafico de los valores promedios ggplot(virtual_bd_weights, aes(x = `Peso Promedio`)) + geom_histogram(binwidth = 0.1, boundary = 0.4, color = &quot;#213555&quot;) + geom_vline(xintercept = 1161, colour = &quot;white&quot;, linewidth = 1.2) + labs(x = &quot;Peso de los patos negros&quot;, y=&quot;Conteo&quot;, title = &quot;Distribution of 10,000 realizaciones de medidas de peso.&quot;) + theme_light() + theme(panel.grid = element_blank()) Figure 9.1: Distribución muestral de la media \\(\\bar{X}\\) generada por simulación. y calcular el valor promedio de los pesos promedios, que arroja un valor de 1158.51, el cual esta razonablemente en acuerdo con el valor verdadero (el valor real difiere, porque el número de replcias es finito). El gráfico del ejemplo anterior es la distribución muestral del estadísitco \\(\\hat{\\mu}\\). Esta distribución describe la variabilidad de los promedios calculados a partir de las replicas alrededor de la media verdadera \\(\\mu\\). Más adelante (en el capitulo Teoría de Muestreo.) estudiaremos esta distribución, pero en este punto, es importante notar que las propiedades de los estimadores que estudiaremos estan basados en el muestreo repetido hipotético (teoría frecuentista), que permite justificar el comportamiento de estos estimadores: esto es, nos permite derivar y justificar el uso de una distribución de porbabilidad para estos estimadores y, por lo tanto, justificar los resultados de inferencia estadística. "],["ámbito-de-la-inferencia-estadística..html", "9.3 Ámbito de la inferencia Estadística.", " 9.3 Ámbito de la inferencia Estadística. A la inferencia estadística le conciernen 4 tipos de problemas, de los cuales se buscan soluciones que permitan realizar estudios experimentales que lleven a conclusiones relevenates y significativas acerca de un fenómeno en estudio. Estos problemas son: Procesos de recolección de muestras. Hace referencia a la sistematización y correcta cosntrucción de los conjujntos muestras, de forma que sean representativos de la población que se está estudiando, disminuyendo cualquier sesgo en la elección de elementos que conformen la muestra. Estimación puntual y a intervalos. Hace referencia al proceso de hacer inferencia o predicciones acerca de los parámetros de una población, que estan ocultos a nosotros, pero de los cuales queremos precisar su valor a partir de una muestra, tolerando cierto grado de error, que debe cuantificarse. Contraste de hipótesis. Esta hace referencia a un proceso de toma de decisiones acerca de la veracidad estadística de una proposición que se formula como hipótesis de un experimento. Diseño experimental. Se refiere a la forma en la que se planea un experimento para poder realizar inferencias por medio de estimaciones y contrastes de hipótesis, y su correcto establecimiento determina la validez de las conclusiones que se obtienen del proceso de inferencia. En los proximos capitulos estudiaremos cada uno de estos puntos para diferentes posibles experimentos, haciendo mucho emfásis en el contraste de hipótesis. "],["teoría-de-muestreo..html", "10 Teoría de Muestreo.", " 10 Teoría de Muestreo. La teoría de muestreo lidia con el problema de construir un conjunto muestra a partir de un conjunto población. La validez de las conclusiones que se establecen sobre la población dependen de si la muestra se seleccionó de tal forma que representa a la población correctamente. En este punto, necesitamos definiciones precisas de población y muestra: Población: colección de individuos o unidades en la que estamos interesados, y que tiene una ley o distribución de probabilidad asociada. Se denota el tamaño poblacional (el número total de individuos) como \\(N\\). Muestra: es una colección de individuos o unidades tomados de una población. Se denota el tamaño de la muestra (el número total de individuos recolectados de la población) como \\(n\\). En general, \\(n &lt; N\\). Una muestra se puede recolectar de una poblacion por medio de un censo o por muestreo aleatorio. El primero se refiere a un muestreo exhaustivo de todos los individuos que conforman la población. El segundo hace referencia a un método que asegura la recolección aleatoria de elementos del conjunto población para construir un conjunto muestra más pequeño. Dado que la población tiene asociada una ley de probabilidad, se especifica para esta los parámetros de esa ley de probabilidad, y es sobre estos que haremos inferencias, por medio de estimadores. Hasta este punto, henos estado hablando sobre estimadores como valores que calculamos para tratar de saber el verdadero valor de una propiedad o caracteristica, y hemos hablado de parametros como esos valores verdaderos que estan ocultos a nosotros. Definimos ahora con precisión estos términos: Un parámetro es un valor numérico que logra resumir la información sobre una población. En casi todos los casos, este valor es completamente desconocido, pero se busca conocer su valor. Un estimador es un estadístico (es decir, tiene una distribución asociada), un valor numérico que ayuda a resumir la información sobre una muestra y se usa para estimar un parámetro poblacional desconocido. Notación. De forma general, se denota como \\(\\theta\\) al parámetro (o \\({\\boldsymbol\\theta}\\), en notación vectorial, si es más de un parámetro). Pero dependiendo de la ley de probabilidad, se puede especificar otra notación particular. Por ejemplo, es usual llamar a la media poblacional con la letra \\(\\mu\\), a la varianza poblacional con la letra \\(\\sigma^2\\), a las proporciones poblacionales con la letra \\(\\pi\\), entre otras. Si la media poblacional es el único parámetro desconocido, entonces \\(\\theta = \\mu\\). Pero si ambos, media y varianza son desconocidos, se resumen los parámetros con la notacion vectorial usual: \\[{\\boldsymbol\\theta} = \\begin{pmatrix} \\mu \\\\ \\sigma^2 \\end{pmatrix}\\] Ya nos hemos encontrado antes con estimadores y parámetros. En los capítulos de distribuciones de probabilidad (capítulos blah blah blah) definimos leyes de probabilidad discreta y continua de interés, y definimos allí los parámetros de los que dependen. Por ejemplo, para una distribución normal, el parámetro media poblacional se denota como \\(\\mu\\) y la desviación estándar poblacional se define como \\(\\sigma\\). Es usual hacer uso de letras griegas para definir parámetros poblacionales, pero por simplicidad, algunas veces se hace uso de notaciones alfanuméricas usuales. Por ejemplo, para la distribución binomial, la probabilidad de éxito la denotamos como \\(p\\). Sin embargo, también es usual hacer uso de la notación \\(\\pi\\) para describir este parámetro, de forma que no haya confusiones al trabajar con notación de probabilidades. Por otro lado, ya hemos encontrado antes estiamdores usuales: en el capítulo Estadística descriptiva. definimos el promedio, la varianza, la desviación estándar, y otras propiedades con que resumir la información contenida en los datos. Estos son ejemplos de estimadores. Se pueden denotar de dos formas: Usando la letra griega correspondiente al parámetro poblacional, pero colocandole un sombrero. Por ejemplo, podríamos denotar al estimador de la media poblacional \\(\\mu\\) como \\(\\hat{\\mu}\\), al estimador de la desviación estándar poblacional \\(\\sigma\\) como \\(\\hat{\\sigma}\\), y así sucesivamente. Usando letras alfanuméricas para los estimadores. Por ejemplo, al estimador de la media poblacional \\(\\mu\\) es usual denotarlo como \\(\\bar{X}\\), al estimador de la desviación estándar poblacional \\(\\sigma\\) como \\(S\\), y así sucesivamente. En los capítulos siguientes haremos lo posible por adherirnos a la notación de sombreritos donde sea conveniente, pero en algunos casos haremos uso de la notación usual por conveniencia. El contexto se hace claro dependiendo del caso, y se aclarará donde sea necesario en los lugares donde pueda ser causa de confusión. "],["estimación..html", "10.1 Estimación.", " 10.1 Estimación. La estimación consiste en hacer inferencias o predicciones sobre los parámetros de una población, los cuales están ocultos a nostoros, usando la información contenida en una muestra. La estimación puede ser de dos tipos: Estimación puntual: una estimación puntual es un estimador que se calcula a partir de una sola muestra particular. Este tiene tres propiedades esenciales: debe ser i) eficiente, ii) consistente, y iii) suficiente. Estimación por intervalos: son un par de estimadores que definen los límites de un intervalo, que se calculan a partir de una muestra, y se espera que contenga el parámetro que esta siendo estimado. Los estimadores, sea cuales sean, deben construirse. La construcción de estos debe ser tal que estos tengan varias propiedades deseables para hacer inferencia: no deben estar sesgados, lo cual se garantiza al emplear métodos de muestreo que permitan tomar muestras aleatorias. Además, deben elegirse de tal forma que sean de mínima varianza, y deben ser consistentes, esto es, a medida que aumente el esfuerzo de muestreo, el estimador debe ir acercandose cada vez más al valor del parámetro. Cualquier proceso de estimación comienza con la suposición de un modelo estocástico que se asume correcto (como se discute en el capítulo [Infererncia estadística.]) una vez recolectada la muestra. Este modelo es una función de densidad \\(f(x; \\theta)\\) que resume la forma en la que se generan las observaciones (la notación hace referencia a que \\(f\\) es una función de las observaciones \\(x\\), dados los parámetros \\(\\theta\\)). En capítulos anteriores hemos dicho que si una v. a. \\(X\\) sigue una distribución \\(f(x)\\), se escribe \\(X \\sim f(x; \\theta)\\). Cuando recolectamos una muestra, tenemos \\(n\\) observaciones \\(x_1, x_2, \\ldots, x_n\\) de una variable alestoria \\(X\\) en estudio. Sin embargo, resulta más conveniente entender a las observaciones como generadas por \\(n\\) variables aleatorias: \\(X_1\\) genera la observación \\(x_1\\), \\(X_2\\) genera la observación \\(x_2\\), \\(\\ldots\\), \\(X_n\\) genera la observación \\(x_n\\), donde cada v. a. se distribuye con la misma ley de probabilidad \\(f(x, \\theta)\\). En este caso, se dice que las \\(n\\) v. a. se distribuyen identicamente. Si además, estas variables son independientes entre sí, se dicen que las \\(n\\) v. a. son independiente e identicamente distribuidas, que se abrevia como iid, y se escribe: \\[X_i \\overset{iid}{\\sim} f(x_i; \\theta)\\text{ para }i = 1, 2, \\ldots, n\\] que se lee como: las variable aleatorias son independientes e identicamente distribuidas como \\(f(x; \\theta)\\). Ahora, si bien cada variable aleatoria tiene su ley de probabilidad, la muestra en su totalidad tiene una ley de probabilidad que se deriva de la ley de probabilidad de las v. a. individuales. La función de probabilidad conjunta de las \\(n\\) v. a. iid viene dada por: \\[f(X_1, \\ldots, X_n; \\theta) = f(x_1; \\theta) f(x_2; \\theta)\\ldots f(x_n; \\theta)\\] Ejemplo. Digamos que se recolecta una muestra de \\(n\\) observaciones generadas por las v. a. \\(X_1, \\ldots, X_n\\) que son iid como \\(N(\\mu, \\sigma^2)\\). Entonces la función de probabilidad conjunta es: \\[\\begin{aligned} f(x_1, x_2, \\ldots, x_n; \\mu, \\sigma^2) &amp;= f(x_1; \\mu, \\sigma^2) f(x_2; \\mu, \\sigma^2)\\ldots f(x_n; \\mu, \\sigma^2) \\\\ &amp;= \\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_1 - \\mu)^2}{2\\sigma^2}}\\right)\\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_2 - \\mu)^2}{2\\sigma^2}}\\right)\\ldots\\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_n - \\mu)^2}{2\\sigma^2}}\\right) \\\\ &amp;= \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\sum_{i=1}^n\\frac{(x_i - \\mu)^2}{2\\sigma^2}} \\end{aligned}\\] donde se usó las propiedades de producto de potencia con igual base. Al expandir la potencia y aplicar propiedades de sumatoria (tarea sencilla que puede verificar usted mismo) se obtiene que: \\[f(x_1, x_2, \\ldots, x_n; \\mu, \\sigma^2) = \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\frac{(\\bar{X} - \\mu)^2}{2\\sigma^2}}\\] y esta es la función de distribución conjunta para la muestra de tamaño \\(n\\) recolectada. El enfasis que se hace es que la muestra tiene una ley de probabilidad asociada que resulta de las suposiciones iniciales del modelo, y, por lo tanto, la inferencia es dependiente de estas suposiciones. También se trata de aclarar la notación que usaremos y que se encuentra frecuentemente en textos de estadística. "],["distribución-muestral-de-un-estimador..html", "10.2 Distribución muestral de un estimador.", " 10.2 Distribución muestral de un estimador. En el capítulo anterior construimos la distribución del estimador del peso promedio de una muestra de patos negros de tamaño \\(n\\) tomada de forma aleatoria. Dijimos que esta distribución de las medias obtenidas de cada una de esas muestras hipotéticas es conocida como distribución muestral y que esta muestra la variabilidad del estimador o los posibles valores que este puede tomar. Ahora profundizaremos en las propiedades de esta distribución que la hacen útil en inferencia estadística. La variabilidad de la distribución muestral depende del número de observaciones que componen a la población, del número de observaciones que componen a la muestra, y del procedimiento usado para tomar la muestra de la población. Esta variabilidad se puede cuantificar en una cantidad llamada error estándar, y a medida que aumenta el tamaño de la muestra tomada, menor es el error estándar. Esto se resume en el siguiente resultado: \\[SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\] donde \\(\\sigma\\) es la desviación estándar poblacional. Este resultado es cierto sin importar si la distribución subyacente de la población es normal o no. El valor de la desviación estándar poblacional esta oculto a nosotros, de la misma forma que lo esta la media poblacional, y por lo tanto, también se debe estimar a partir de la muestra. Y, al igual que con la media poblacional, diferentes muestran arrojaran diferentes posibles valores del estimador, y esta variabilidad se puede resumir en la distribución muestral de la desviación estándar. Así mismo, como con otro parámetros podemos obtener estimadores de muestras, entonces estos estimadores siempre tienen asociado una distribución muestral para describir su variabilidad, y esta última se cuantifica usando la medida de error estándar. Ejemplo. Volvamos al ejemplo de muestreo de patos negros para medir su peso total. Realizaremos simulaciones similares a la realizada antes, pero en lugar de repetir el experimento de tomar muestras miles de veces, el experimento de toma de muestra se hace una sola vez. Solo modificaremos el tamaño de la muestra obtenida de la población, de forma que no solo tomemos muestras de tamaño \\(50\\), sino que realizaremos la simulación usando muestras de tamaño \\(20\\), \\(100\\) y \\(1000\\) también. Los resultados se muestran en la figura 10.1. Figure 10.1: Dependencia de la distribución muestral con el tamaño de la muestra recolectada. Vemos que en el gráfico correspondiente a la muestra más pequeña, la variabilidad es bastante grande y la mayoría de los datos se concentran alrededor de la verdadera media \\(\\mu = 1161\\) kg. Pero se observan datos atípicos con frecuencias altas que sesgan mucho la forma de la distribución. Al aumentar el tamaño de la muestra recolectada, se puede observar que el sesgo va desapareciendo y las observaciones atípicas son cada vez menos frecuentes. De esta forma, podemos ver que el tamaño de la muestra afecta el calculo de cualquier estimador, haciendo que este este más o menos desviado del verdadero valor dependiendo del \\(n\\). Ésto es fácil de cuantificar si calculamos la desviación estándar en cada simulación (que se muestra en cada uno de los gráficos de la figura anterior) y luego calculamos el error estándar, como se muestra en el siguiente fragmento de código de R: size sample_size mean std_dev std_error small 20 1152.663 102.67452 22.958720 medium 50 1160.893 95.38529 13.489517 large 100 1183.214 112.05658 11.205658 very large 1000 1162.796 96.45836 3.050281 En la tabla vemos que nuestro estimador de la media y la desviación estándar se acercan cada vez más al valor real de \\(1161\\) kg y \\(98\\) kg, y que el error estándar es cada vez más pequeño a medida que aumenta el tamaño de la muestra. Esto quiere decir que, a medida que aumentamos el tamaño de la muestra, nuestros estimadores se acercan cada vez más al valor real, y se hacen cada vez más precisos. Este resultado se resume en uno de los teoremas más importantes de la teoría de probabilidades y estadística, la ley de los grandes números. La ley de los grandes números. Sea \\(X_1,X_2, \\ldots\\) una sucesión infinita de variables aleatorias independientes e idénticamente distribuidas con media finita \\(\\mu\\). Entonces, cuando \\(n \\rightarrow \\infty\\), \\[\\frac{1}{n}\\sum_{i=1}^{n}X_i \\rightarrow \\mu\\] en donde la convergencia se verifica en el sentido casi seguro (ley fuerte) y también en probabilidad (ley débil). Lo que dice el teorema es que, a medida que la muestra de la cual calculamos el estimador se hace más grande, el error en nuestra medida irá decreciendo más y más hasta converger al verdadero valor del parámetro (la convergencia casi segura y en probabilidad son formas de convergencia de una serie infinita de v. a. definidas en terminos de la probabilidad de que la convergencia se de es segura y de la probabilidad de las desviaciones tan pequelñas como se quiera del estimador y el parámetro es nula, respectivamente. Para más detalles, consulte @lehmann1999elements o @rincon2014introduccion). Aquí un comentario pertinente sobre la notación. Si bien usamos la letra griega \\(\\mu\\) en el teorema, misma letra que usamos para denotar la media poblacional de una distribución normal, no debemos pensar que el teorema solo es cierto oara este parámetro. En el teorema, se usa \\(\\mu\\) como notación más amplia de un parámetro verdadero cualquiera. Por ejemplo, si la variable aleatoría \\(X\\) son desviaciones estándar de la media, entonces el parámetro \\(\\mu\\) es el resltado de promediar todas las desviaciones estándar de todas las posibles muestras reclectadas de tamaño \\(n\\), este promedio es \\(\\sigma\\) y el teorema se escribiría: \\[\\frac{1}{n}\\sum_{i=1}^{n}X_i \\rightarrow \\sigma\\] "],["teorema-del-límite-central-tlc..html", "10.3 Teorema del Límite Central (TLC).", " 10.3 Teorema del Límite Central (TLC). Este teorema es el más importante de los que verémos en este libro. Este establce la distribución muestral de estadísticos construidos a partir de estimadores calculados de una muestra, y usa la ley de los grandes números para verificar que la ley de probabilidad del estadístico converge a una normal estándar. Sea \\(X_1, X_2, \\ldots\\) una sucesión infinita de variables aleatorias independientes e idénticamente distribuidas, con media \\(\\theta\\) y error estándar \\(SE(\\theta)\\). Entonces la función de distribución de la variable aleatoria: \\[Z_n = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)}\\] tiende a la función de distribución normal estándar cuando \\(n\\) tiende a infinito. El TLC establece que el estadístico \\(Z_n\\) tendrá una distribución normal estándar sin importar la distribución de las v. a. \\(X_1, X_2, \\ldots, X_n\\), para valores grnades de \\(n\\), aunque esto no implica que la muestra tendrá una forma de campana. En general, a mayor sea la muestra, más cercana esta estará de la verdadera distribución poblacional. Ahora, desde un punto de vista práctico, este teorema se cumple cuando \\(n \\ge 30\\), de forma que no se necesitan muestras demasiado grandes para poder justificar la normalidad al realizar inferencia. Cuando \\(n &lt; 30\\), la distribución muestral es más variables, haciendo que los estimadores de \\(\\theta\\) y \\(SE(\\theta)\\) sean menos precisos, y por lo tanto es mejor utilizar la distribución \\(t\\)-Student con \\(n - 1\\) grados de libertad. Ejemplo. Simulemos un conjunto de variables aleatoria \\(Z_n\\) para distintos valores de \\(n = 5, 10, 20, 30, 50, 100, 500, 1000\\). Para ellos, imaginemos un experimento donde se siembran \\(n\\) semillas y se registra despues de un tiempo, si la semilla germina o no. Denotamos un exito como la semilla no germnina, que se sabe tiene probabilidad de exito de \\(0{,}2\\). Con esta información, simulamos \\(1000\\) replicas del experimento para cada \\(n\\), y construimos un histograma sobre el cual dibujamos una curva de la función de densidad normal estándar para comparar. el gráfico muestra indudablemente que a medida que el \\(n\\) crece, el histograma de la distribución muestral se aproxima cada vez más a una normal estándar, y que el ajuste siempre es mejor cuando \\(n \\ge 30\\), y para valores menores a este, el ajuste no es tan bueno. Este ejemplo busca convencerlo de que el TLC es válido y aplicable a la hora de realizar inferenias. Pero también enfatiza la importancia de elegir un tamaño muestral aadecuado para que la suposición de normalidad tenga sentido de \\(Z_n\\) tenga sentido. En los próximos capítulos estaremos usando este teorema continuamente cuando derivemos la distribución muestral de los estadísitcos que cosntruiuremos para realizar estimaciones y contrastar hipóteis. Corrección por población finita. Para cualquier población estadística que consiste de \\(N\\) unidades, se define la corrección de población finita como: \\[1 - f = 1 - \\frac{n}{N}\\] Solo tiene importancia en poblaciones pequeñas, en las que \\(n &gt; 0{,}05\\times N\\). Modifica los estimadores de la desviación estandar. "],["necesidad-de-especificar-una-muestra..html", "10.4 Necesidad de especificar una muestra.", " 10.4 Necesidad de especificar una muestra. La especificación de una muestra requiere de varios pasos particulares, pero de forma invariable esta sujeta a la pregunta que se busca responder al realizar un experimento, y a la cantidad de esfuerzo y dinero que se puede invertir para realizar dicho experimento. El proceso de diseño y toma de muestra no es puramente matemático: Los objetivos del experimento deben especificarse, y esto involucra especificar variables que podrían ser importantes, el equipo de muestreo a usar, cuanto esfuerzo se puede invertir, … Toda esta información, es la que ayuda a elegir un método de estimación a usar. En este punto, se hace la pregunta: ¿Qué tan grande debe ser la muestra que voy a tomar? El tamaño de la muestra es muy importante para asegurar la representatividad de la población que se esta muestreando, pero también lo es para poder evaluar el efecto de un tratamiento al realizar un experimento (en, por ejemplo, diseños experimentales factoriales o por bloques, o análisis tan sencillos como pruebas \\(t\\)). La elección del tamaño de la muestra requiere de la especificación de: La prueba a utilizar. El nivel de significancia mínimo a usar (tradicionalmente, 1%, 5% o 10%). El tamaño del efecto a contrastar. La potencia deseada para la prueba (usualmente, 80%). La potencia tiene como valor el que su consideración obliga al investigador a pensar en términos de la fuerza de los efectos que su experimento es probable produzca. Aquí, la información a priori comienza a ser de vital importancia. Particularmente en el contraste de hipótesis El trabajo del investigador no es demostrar que un tratamiento no produce el mismo efecto que el control, es demostrar la efectividad del tratamiento. El tamaño de la muestra a usar tiene que ser tal que: * Permita determinar si un efecto dado (su magnitud) puede interpretarse como suficientemente confiable o válido como para que la comunidad científica acepte una hipótesis. * Permita determinar (o se determina tal que) que tan probable es que los datos de un estudio resulten en una significancia estadística antes de que el estudio se haya llevado a acabo. No solo es profesionalmente autodestructivo el diseñar experimentos que no tengan una alta probabilidad de éxito, sino que no es ético el hacerlo por la simple razón de que se consumen recursos escasos (monetarios, de esfuerzo o tiempo). "],["diseño-de-muestreo..html", "10.5 Diseño de muestreo.", " 10.5 Diseño de muestreo. El elegir un correcto método de muestreo es vital para obtener información representativa de la población. Se hace importante el diseño del muestreo: en cuanto al método (sistemático o aleatorio) y en cuanto al número de muestras. Muestreo aleatorio simple. Muestreo aleatorio estratificado. Muestreo adaptativo. Muestreo sistemático. Consideraciones. Debemos primero establecer de forma explícita la población estadística. Se debe especificar la unidad de muestreo. Luego se selecciona una muestra con algún plan particular. Cualquier estadístico asume que la muestra sigue los principios del muestreo probabilístico: Se define un conjunto de muestras distintas \\(S_1\\), \\(S_2\\), \\(\\ldots\\) en el que cierta unidad de muestreo especifica es asignado a \\(S_1\\), otro a \\(S_2\\), y así sucesivamente. A cada muestra \\(S_i\\) ( \\(i=1, 2, \\ldots\\) ) se le asigna una probabilidad de ser seleccionada. Se selecciona una de las muestras por la probabilidad adecuada, usando una tabla de números aleatorios. Claro, el muestreo puede no ser probabilístico: Muestreo de solo unidades accesibles. Muestreo influenciado por sesgos sensoriales. Influencia de prejuicios u otras subjetividades al muestrear unidades típicas El uso de solo voluntarios. 10.5.1 Muestreo Aleatorio Simple. 10.5.2 Muestreo Aleatorio Estratificado. La población de \\(N\\) individuos se subdivide en \\(h\\) subpoblaciones o estratos que no se solapen, de forma que \\(N = N_1 + N_2 + \\ldots + N_L\\). Cada estrato es entonces muestreado por separado, obteniéndose muestras \\(n_1\\), \\(n_2\\), \\(\\ldots\\), \\(n_L\\). La estratificación es recomendable cuando: * Se requieren estimadores de medias y varianzas separadamente para cada estrato. * La probabilidad de seleccionar una muestra varía de un área a otra. * Se necesita mayor precisión de un estimador. * La organización administrativa del equipo lo ve conveniente. ¿Cómo construir los estratos? * No deben exceder más de 6 estratos. * Basado en conocimiento a priori. * Basado en una variable a medir o controlar. * Las muestras pueden decidir estratificarse luego del proceso de recolección. 10.5.3 Muestreo Adaptativo. Hace uso de datos recolectados para realizar decisiones sobre el esfuerzo de muestreo. Puede ser de dos tipos: i) Muestreo adaptativo aglomerado (clusters): se realiza un muestreo aleatorio simple, como se describió antes. Si una o más unidades de muestreo tienen una muestra de interés, se seleccionan unidades de muestreo adicionales en la vecindad de estas. ii) Muestreo adaptativo aglomerado estratificado: se sigue el procedimiento descrito en i), pero sobre estratos definidos como se explica antes. 10.5.4 Muestreo Sistemático. Se usa principalmente por su simplicidad, y también para poder realizar muestreos uniformemente espaciados (en tiempo y espacio), que pueden tratarse como aleatorios, sin sesgo alguno. Se debe cuidar por la presencia de variaciones periódicas. "],["proceso-de-muestreo..html", "10.6 Proceso de Muestreo.", " 10.6 Proceso de Muestreo. Identificación del conjunto Población. Determinación del tamaño de nuestro conjunto muestral. Proporcionar un medio para la base de la selección de muestras del medio Población. Selección de muestras del medio utilizando una de las muchas técnicas de muestreo como el muestreo aleatorio simple, sistemático o estratificado. Verificar si el conjunto de muestra formado contiene elementos que realmente coinciden con los diferentes atributos del conjunto de población, sin grandes variaciones entre ellos. Comprobación de errores o estimaciones inexactas en el conjunto de muestras formadas, que pueden o no haber ocurrido. El conjunto que obtenemos después de realizar los pasos anteriores en realidad contribuye al conjunto de muestra. "],["teoría-de-estimación..html", "11 Teoría de Estimación.", " 11 Teoría de Estimación. La estimación consiste en realizar predicciones o inferencias sobre los parámetros de una distribución usando la información contenida en la muestra. Fromalmente Dada una variable aleatoria \\(X\\) cuya ley de probabilidad depende de un parámetro \\(\\theta\\), un estadístico \\(g(X)\\) se dice es estimador de \\(\\theta\\) si, para cualquier valor observado de \\(x \\in X\\), \\(g(x)\\) se considera un estimado de \\(\\theta\\). Esta definción se puede escribir de otra forma, como: Dadas las observaciones de variables aleatorias \\(X_1, X_2, \\ldots, X_n\\) identica e independientemente distribuidas (iid) con función de distirbución \\(F(x\\vert\\theta)\\), se estima \\(\\theta\\). En la primera definicón anterior, \\(g(x)\\) es una función de la muestra (esto es, de las observaciones realizadas de la v. a. \\(X\\)). La definción puede ser un poco dificil de comprender, pero podemos revisarla poco a poco usando un ejemplo. Ejemplo. Digamos que se realiza un muestreo aleatorio simple de una población cuya función de densidad es una v. a. normal de media \\(\\mu\\) y varianza finita \\(\\sigma^2\\). Digamos que se quiere construir un estimador de la media poblacional. Ya sabemos que el mejor estimador de la meda poblacional es \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\), y por lo tanto se tiene que \\(g(x) = \\bar{X}\\). De aquí en adelante, nos centraremos en construir estadísticos, además de usar los estimadores usuales ya conocidos. Pero antes, veamos las propiedades que tinene un buen estimador. "],["propiedades-de-un-estimador..html", "11.1 Propiedades de un estimador.", " 11.1 Propiedades de un estimador. Cualquier estimador que se precie de ser un buen estimador debe cumplir con 3 propiedades deseables para hacer inferencia. El estimador debe ser insesgado: un estimador se dice es insesgado cuando la esperanza de su estadístico es igual al valor del parametro siendo estimado. Se escribe el sesgo \\(B(\\hat{\\theta})\\) como: \\[B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta\\] Por ejemplo, podemos calcular la esperanza del estadístico \\(\\bar{X}\\) como \\(E[\\bar{X}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]\\). Usando las propiedades \\(E[cX] = cE[X]\\) y \\(E[\\sum_i X] = \\sum_iE[X]\\), donde \\(c\\) es una constante, se tiene que: \\[E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i]\\] Pero como \\(E[X] = \\mu\\) (el valor esperado de una v. a. es su media) entonces: \\[\\frac{1}{n}\\sum_{i=1}^n E[X_i] = \\frac{1}{n}\\sum_{i=1}^n\\mu_X = \\mu\\] y vemos que el estadísitico \\(\\bar{X}\\), que estima \\(\\mu\\) tiene sesgo nulo. Por lo tanto, \\(\\bar{X}\\) es un buen estimador de la media. El estimador debe ser eficiente: un estimador \\(g(x)\\) se dice que es eficiente si de todos los posibles estiamdores, \\(g(x)\\) tiene la mínima varianza posible. Más formalmente, si \\(\\hat{X_1}\\) y \\(\\hat{X_2}\\) son ambos estimadores insesgados de \\(X\\), entonces se dice que \\(\\hat{X_1}\\) es un estimador más eficiente de \\(X\\) que \\(\\hat{X_2}\\), si \\(\\sigma^2_{\\hat{X_1}} &lt; \\sigma^2_{\\hat{X_2}}\\). Figure 11.1: Ejemplo gráfico de estimadores: a) sesgado y no eficiente; b) insesgado pero no eficiente; c) sesgado y eficiente; d) insesgado y eficiente. El estimador debe ser consistente: se dice que un estimador \\(g(x)\\) es consistente si este se aproxima a al parámetro \\(\\theta\\) cuando el esfuerzo de meustreo se hace mayor. Formalmente: sea \\(X_1, X_2, \\ldots, X_n\\) variables aleatorias iid que se usan para obtener un estimador \\(\\hat{\\theta}\\) de \\(\\theta\\). Se dice que \\(\\hat{\\theta}\\) es un estimador consistente si converge en probabilidad a \\(\\theta\\), esto es: \\[\\lim\\limits_{n \\to \\infty} P\\left[\\vert\\hat{\\theta} - \\theta\\vert\\ge\\varepsilon\\right] = 0\\] El último límite se puede modificar para comprender mejor la propiedad de consistencia. Para ello, se puede usar la desigualdad de Chebyshev \\[P\\left[\\vert\\hat{\\theta} - \\theta\\vert\\ge\\varepsilon\\right] \\le \\frac{\\sigma^2_\\theta}{\\varepsilon^2}, \\varepsilon &gt; 0\\] y luego, tomando limites a ambos lados, podemos escribir la propiedad de consustencia como: \\[\\lim\\limits_{n \\to \\infty} \\sigma^2_\\hat{\\theta} = 0\\] Entonces, un estimador es consistente, cuando la varianza de este cae cero cuando aumentamos el esfuerzo de muestreo. Dicho de otra forma, el estimador es consistente cuando se acerca más al verdadero valor del parámetro cuando \\(n\\rightarrow\\infty\\). Ahora podemos proceder a estudiar los estimadores puntuales y por intervalos. "],["estimación-puntual..html", "11.2 Estimación puntual.", " 11.2 Estimación puntual. Cuando un investigador realiza un experimento, por lo general, solo toma una muestra represntativa de tamaño \\(n\\) de la población de interés y calcula estimadores que le permitan describir los datos obtenidos y relalizar inferencias. El investigador no se moelsta en realizar el experiemnto varias veces (no es como las simulaciones, donde podiamos realizar repeticiones tantas como quisieramos. En la realidad, no se tiene el esfuerzo, la aneergía o los emdios apra realizar multiples repeticiones de un experimento). En estos casos se usan estimadores puntuales para poder realizar inferencias basados en solo una repetición del experimento. Un estimador puntual de un parámetro \\(\\theta\\), es solo un valor \\(\\hat{\\theta}\\) de un estadístico \\(\\hat{\\Theta} = g(X)\\). Para aclarar la notación, \\(\\hat{\\Theta}\\) es el conjunto de todos los posibles valores del estadístico, y \\(\\hat{\\theta}\\) es un elemento de ese conjunto particular, calculado a partir de una muestra. Veamos algunos ejemplos. Ejemplo. Un experimento que busca evaluar la reacción de saltamontes a estímulos visuales o acústicos, en los que midieron el tiempo de reacción a estos antes del vuelo, encontraron que el tiempo de reacción promedio a estímulos acústicos es \\(\\bar{X}_a = 108{,}05\\) segundos, y a estímulos visuales es \\(\\bar{X}_v=87{,}19\\) segundos. Estos dos valores son estimadores puntuales de las medias poblacionales \\(\\mu_a\\) y \\(\\mu_v\\). El siguiente, es un ejemplo que trata de enseñar como realizar estimaciones puntuales en diseños estratificados. Ejemplo. Siniff y Skoog (1964) realizaron un muestreo aleatorio estratificado de una manada de caribúes de Nelchina en Alaska. Para ello, se establecieron 6 estratos (basados en estudios preliminares de la densidad relativa de los caribúes), y seleccionaron de manera aleatoria una muestra en cada uno de tamaño \\(n_i\\) ( \\(i=A, B, C, D, E, F\\) ), cada una de unidades muestrales de 4 millas cuadradas, obteniendose los datos mostrados en la tabla. Se desea saber el tamaño total de la población de caribúes. Estrato Tamaño del Estrato (\\(S\\)) Tamaño de muestra (\\(N_h\\)) Número promedio de Caribúes Varianza A 400 98 24.1 5575 B 30 10 25.6 4064 C 61 37 267.6 347556 D 18 6 179.0 22798 E 70 39 293.7 123578 F 120 21 33.2 9795 Total 699 211 NA NA Para poder conocer un estimado del tamaño poblacional total \\(\\hat{N}\\), se necesita primero de un estimado del número promedio de caribúes por unidad de muestreo. \\[ \\begin{aligned} \\bar{X}_{ST} &amp;= \\frac{\\sum_{h=1}^L N_h \\bar{x}_h}{N} \\\\ &amp;= \\frac{400\\times24{,}1 + 30\\times25{,}6 + 61\\times267{,}6 + \\ldots}{699} \\\\ &amp;= 77{,}96\\text{ caribúes milla}^{-2} \\end{aligned} \\] y se puede calcular la densidad de toda la población usando el total de millas cuadradas que conforman los estratos: \\[\\hat{N} = S \\times \\bar{X}_{ST} = 699\\text{ milla}^2 \\times 77{,}96\\text{ caribúes milla}^{-2} = 54.597\\text{ caribúes}\\] Sabemos entonces, que el estimado del número de caribúes es \\(\\hat{N} = 54.597\\text{ caribúes}\\). Sin embargo, aun necesitamos cuantificar la incertidumbre asociada a esta estimación. Podemos calcular la varianza de \\(\\bar{X}_{ST}\\) como: \\[Var(\\bar{X}_{ST}) = \\sum_{i=1}^L\\left[ \\frac{W_h^2 S_h^2}{n_h}(1 - f_h) \\right]\\] donde \\(W_h = N_h / N\\) es el ponderado del estrato y \\(f_h = n_h / N_h\\). Usando los datos de la tabla: \\[Var(\\bar{X}_{ST}) = \\left[ \\frac{0{,}572^2 5575}{98} \\right]\\left(1 - \\frac{98}{400}\\right) + \\left[ \\frac{0{,}043^2 4064}{10} \\right]\\left(1 - \\frac{10}{30}\\right) + \\ldots = 69{,}83\\] de forma que la varianza del tamaño de la población de caribúes es \\(69{,}83 \\times 699^2 = 34.105.734\\), y la desviación estándar es \\(\\sqrt{34.105.734} = 5.840\\) caribúes. Entonces el estimador buscado, con su medida de incertidumbre, es \\[54.597 \\pm 5.840 \\text{ caribúes}\\] El siguiente ejemplo, es uno donde se construye un estadístico a partir de otro que tiene una ley de probabilidad prespecificada. De esta forma, podemos facilitar la obtención de una distribución muestral asociada al nuevo estadistico que permita ontener medidas de probabilidad asociada a valores observados particulares. Ejemplo. Digamos que tenemos una estimador puntual que queremos evaluar, digamos, la media calculada \\(\\bar{X}\\) de una muestra de tamaño \\(n\\), en cuanto a la probabilidad de ocurrencia de este. El TLC nos indica que este estimador se distribuye normalmente (si conocemos la varianza poblacional o si el \\(n\\) es lo suficientemente grande como para asumir que conocemos la varianza poblacional lo suficientemente bien). Escribimos entonces el estimador puntual \\[\\hat{Z} = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\] Como vimos en la sección [distribucion-normal], este se puede usar para encontrar valores de probabilidad asociado a obtener un valor de a lo sumo \\(\\bar{X}\\) como: \\[P(X \\le \\bar{X}) = P(Z \\le\\hat{Z})\\] 11.2.1 Construcción de estadísitcos para inferencia. El ejemplo anterior es muy importante. Nos dice que si podemos asumir una distribución para una variable aleatoria, entonces podemos construir un estadístico con el cual facilitar la obtención de medidas de probabilidad. Esto nos da una forma sencilla de encontrar probabilidades asociadas a un estimador particular calculado a partir de una muestra, y así, poder obtener medidas de incertidumbre que nos permitan derivar conclusiones adecuadas sobre los estimadores. Estadístico sobre la media Los estadísticos sobre la media los podemos escribir usando el TLC como base para realizar inferencia. Si tenemos un conjunto de v. a. \\(X_1, X_2, \\ldots, X_n\\) independientes e identicamente distribuidas como \\(f(\\theta)\\), de las cuales se hacen las observaciones \\(x_1, x_2, \\ldots, x_n\\), de la cual estimamos el valor \\(\\hat{\\theta}\\), entonces podemos construir un estadístico sobre \\(\\theta\\) como: \\[\\frac{\\hat{\\theta} - \\theta}{SE(\\theta)}\\] Este estadístico lo podemos entender al darnos cuenta de dos cosas: El uso de la diferencia \\(\\hat{\\theta} - \\theta\\) sirve como una medida de similitud entre el estimador \\(\\hat{\\theta}\\) y el valor real del parámetro \\(\\theta\\): valores grandes inidcan que ambos son menos aprecidos entre sí, mientras que valores pequeños de esta diferencia indican que el estimador y el parámetro son más similares entre sí. De igual forma, el signo de la diferencia nos dice la dirección en la que cae el estimador: valores positivos indican que se tienen valores por encima del valor del parámetro, mientras que un signo negativo indica que el valor estimado cae por debajo del verdadero valor del parámetro. Al dividir la diferencia entre el error estándar \\(SE(\\theta)\\), lo que se hace es estandarizar la diferencia. De esta forma, las diferencias las podemos entender como desviaciones estándar, esto es, a cuantas desviaciones estándar el estimador cae del parámetro. En este caso, se asume que el error estándar es conocido, de forma que el estadístico sigue una distribución normal estándar (según el TLC), y se escribe: \\[\\hat{Z} = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)} \\sim N(0, 1)\\] La ecuación anterior también es válida aun si tebnemos que calcular el error estándar de los datos, siempre y cuando el tamaño de la muestra recolectada para estimar \\(SE(\\theta)\\) sea lo suficientemente grande como para asegurarnos de que sabemos su valor con una exactitud adecuada. Si, por otro lado, no conocemos el verdadero valor de \\(SE(\\theta)\\) y la muestra de donde estimamos a este es muy pequeña, entonces debemos asumir que este es una variable aletoria más. La distribución muestral del nuevo estadístico la podemos obtener notando que: \\[SE(\\hat{\\theta}) = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\] donde \\(\\hat{\\sigma}^2 \\sim \\chi^2(n - 1)\\), y por la proposición final en la sección [distribución-\\(t\\)-student], entonces el estadístico sigue una distribución \\(t\\)-Student y se escribe como: \\[\\hat{t} = \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\sim t(n - 1)\\] Con este esquema general, podremos realizar inferencias con respecto a la media y otros parámetros, como veremos en la siguiente sección. Pero antes, veamos como construir un estadístico sobre la varianza. Estadístico sobre la varianza Antes usamos la diferencia entre el estimador y el parámetro para construir un estadístico que nos dijera que tan similares son. Este argumento funciona bien con estadísticos como la media, ya que este corresponde a una medida de locación, y la lejanía de dos locaciones (numericamente hablando) nos permite entender que tan similares son (ve la figura ??). Con las varianzas, el argumento de la diferncia no es tan intuitivo. Si queremos saber si una varianza es mayor o menor que otra, resulta más intuitivo verificar qaue tanto mayor (o menor) es la dispersión de una poblacion con respecto a otra. Esto apunta al uso de cocientes entre varianzas. Si tenemos un conjunto de v. a. \\(X_1, X_2, \\ldots, X_n\\) independientes e identicamente distribuidas como \\(f(\\theta)\\), de las cuales se hacen las observaciones \\(x_1, x_2, \\ldots, x_n\\), de la cual estimamos la varianza \\(S^2\\), entonces podemos construir un estadístico sobre \\(S^2\\) como: \\[\\frac{(n - 1) S^2}{\\sigma^2}\\] la cual sabemos, por la proposición que vimos al final de la sección [distribucion-ji-cuadrada] sabemos se distribuye como una distribución \\(\\chi^2\\) con \\(n - 1\\) grados de libertad, por lo que podemos escrbir: \\[X^2 = \\frac{(n - 1) S^2}{\\sigma^2} \\sim \\chi^2(n - 1)\\] Se tiene entonces que: Si la muestra proviene de la misma población, el valor esperado de la varianza \\(E[S^2]\\) será \\(\\sigma^2\\) y el valor esperado del cociente será \\(E[(n - 1) S^2 / \\sigma^2] = \\frac{(n - 1)}{\\sigma^2} E[S^2] = n - 1\\). Este valor resulta que coprresponde a la media de la distribución \\(\\chi^2\\) con \\(n - 1\\) grados de libertad. Si la muestra proviene de una población con un varianza menor, entonces el valor esperado \\(E[S^2]\\) es menor que \\(\\sigma^2\\), por lo que el cociente \\(\\frac{(n - 1)}{\\sigma^2} E[S^2] &lt; n - 1\\). Si la muestra proviene de una población con un varianza mayor, entonces el valor esperado \\(E[S^2]\\) es mayor que \\(\\sigma^2\\), por lo que el cociente \\(\\frac{(n - 1)}{\\sigma^2} E[S^2] &gt; n - 1\\). Los casos anteriores corresponden a lo que esperaríamos a la larga (si repitieramos muchas veces el experimento). Pero hay que entender, que al hacer el experimento y recolectar una muestra, el valor etimado de \\(S^2\\) puede ser menor o mayor que \\(\\sigma^2\\) solo por efecto del azar. Podemos calcular entonces que tan probable es que el valor sea tan grande como el encontrado usando la distribución muestral, \\(P(\\chi^2 \\ge X^2)\\). Ahora, el procedimiento anterior es útil cuando queremos verificar si la variansza calculada de una muestra, corresponde con la varianza conocida para la población de donde se tomo la muestra. Pero podriamos querer comparar dos poblaciones distintas, para verificar si sus varianzas son las mismas. En este caso, supongamos que las varianzas de las poblaciones son \\(\\sigma_1^2\\) y \\(\\sigma_2^2\\), cuyos estimadores respectivos son \\(S_1^2\\) y \\(S_2^2\\), calculados a partir de muestras de tamaño \\(n_1\\) y \\(n_2\\), respectivamente. Podemos usar una parte de estadístico que construimos antes para cada una de las varianzas: \\[\\frac{(n_i - 1) S_i^2}{\\sigma_i^2}\\] para \\(i = 1\\) y \\(2\\), que sabemos corresponden a una v. a. que siguen una distribución \\(\\chi^2\\) con \\(n_i - 1\\) grados de libertad. Entonces, podemos usar la proposición final de la sección [distribucion-f] para verificar que \\[\\frac{S_1^2 / \\sigma_1^2}{S_2^2 / \\sigma_2^2} = \\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2}\\] sigue una distribución \\(F\\) con \\(n_1 - 1\\) y \\(n_2 - 1\\) grados de libertad, y se puede escribir: \\[\\hat{F} = \\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2} \\sim F(n_1 -1, n_2 - 1)\\] El valor esperado para esta distribución es ligeramente mayor a uno para tamaños de muestra muy pequeño, y es aproximadamente de uno para tamaños de muestra grande. De forma que: si el valor es mayor o menor a uno, podríamos hablar sobre varianzas que no son iguales, y por tanto, las muestras son obtenidas de poblaciones distintas. De nuevo, esto es en un sentido estadístico: la muestra puede resultar en proporciones de varianzas distintas a uno solo por azar. Podemos calcular que tan probable es que el valor sea tan grande como el encontrado usando la distribución muestral, \\(P(F \\ge \\hat{F})\\). Usando este esquema general (los estadísticos construidos), podemos cuantificar la incertidumbre con respecto a un parámetro en problemas de inferencia como sigue en la siguiente sección. "],["estimación-por-intervalos..html", "11.3 Estimación por Intervalos.", " 11.3 Estimación por Intervalos. Una estimación por intervalo de un parámetro \\(\\theta\\) es un intervalo de la forma \\(\\hat{\\theta}_L &lt; \\theta &lt; \\hat{\\theta}_U\\), donde \\(\\hat{\\theta}_L\\) y \\(\\hat{\\theta}_U\\) (los límites inferior y superior del intervalo) dependen del valor del estimador \\(\\hat{\\Theta}\\) para una muestra específica, y también de la distribución de muestreo de \\(\\hat{\\Theta}\\). Al intervalo \\(\\hat{\\theta}_L &lt; \\theta &lt; \\hat{\\theta}_U\\) se le llama intervalo de confianza, y su longitud es un indicador de la precisión de una estimación puntual. Figure 11.2: Intervalo de confianza para una variable aleatoria. El área sombreada corresponde al valor de probabilidad asociada al intervalo. Hay que hacer énfasis en algo muy importante: los límites del intervalo son estimados a partir de la muestra, por lo que son v. a. y no se pueden entender como parámetros fijos. Esto es, son estimadores de \\(\\Theta_L\\) \\(\\Theta_U\\). Esto implica que cualquier medida de probabilidad asociada al intervalo se hace en terminos de los límites, y no del parámetro sobre el cual se construye. El razonamiento de la construcción de intervalos de confianza es utilizar la distribución muestral de \\(\\hat{\\Theta}\\) para determinar los límites del intervalo de tal manera que: \\[P(\\hat{\\Theta}_L &lt; \\theta &lt; \\hat{\\Theta}_U) = 1 - \\alpha, \\quad 0 &lt; \\alpha &lt; 1\\] para un valor prespecificado de \\(\\alpha\\). Decimos que hay una probabilidad de \\(1 - \\alpha\\) de que el intervalo contenga al verdadero valor del parámetro \\(\\theta\\), con una confianza de \\(100(1 - \\alpha)\\)%. Al valor de \\(\\alpha\\) se le conoce como nivel de significancia y es uno de los parámetros más importantes que estudiaremos, dada su importancia en la especificación del tamaño de muestras al momento de diseñar experimentos. Este valor expresa el grado de incertidumbre que esperamos a la larga sobre la veracidad del intervalo que construimos, y, por lo tanto, sobre las conclusiones que derivamos del mismo. En general, el valor de \\(\\alpha\\) se suele especificar como \\(0{,}1\\), \\(0{,}05\\) o \\(0{,}01\\) dependiendo de que tanta incertidumbre estamos dispuestos a ceder en cuanto a nuestras conclusiones y las consecuencias que derivan de presentar conclusiones equivocadas. Por ejemplo, digamos que un médico construye un intervalo de confianza para un estudio en el que se busca probar la eficacia de cierto fármaco en curar una enfermedad. Si el investigador eligiera un nivel de significancia de \\(0{,}1\\), entonces el intervalo construido es del \\(100(1 - 0{,}1)\\% = 90\\%\\) de confianza, el cual puede parecer bastante grande. Sin embargo, la incertidumbre asociada puede ser demasiado grande si notamos que el hecho de equivocarse significaría posiblemente la aparición de efectos secundario graves sobre el paciente e incluso, la muerte de una persona. Es por ello que, dependiendo del estudio que estemos realizando, se debe escoger un nivel de confianza adecuado para asegurar que nuestras conclusiones no resulten en la toma de decisiones que puedan tener consecuencias negativas importantes. El teorema del límite central se hace muy importante para la construcción de estadísticos cuya ley de probabilidad es conocida, tal como se estudio en la sección anterior. 11.3.1 Inferencia sobre la media. Ahora veremos unos ejemplos de como usar ese formalismo para construir intervalos de confianza para distintos casos particulares. El primero de nuestros ejemplos es sobre la construcción de intervalos de confianza para una muestra para la cual se conoce la varianza poblacional. Ejemplo. Cuando 14 estudiantes de segundo año de medicina del Bellevue Hospital midieron la presión sanguínea de la misma persona, obtuvieron los siguientes resultados: \\(138, 130, 135, 140, 120, 125, 120, 130, 130, 144, 143, 140, 130\\), y \\(150\\) mmHg. Suponiendo que se sabe que la desviación estándar poblacional es de \\(10\\) mmHg, construya un estimado de un intervalo de confianza del 95% de la media poblacional. De manera ideal, ¿cuál debe ser el intervalo de confianza en esta situación? Solución. Una manera de construir un estadístico es estableciendo una expresión que nos diga cuanto se desvía nuestro estimador del valor real. Esto, ya vimos, lo podemos lograr usando una diferencia estandarizada: \\[\\hat{Z} = \\frac{\\hat{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{133.93 - \\mu}{10 / \\sqrt{14}} \\sim N(0, 1)\\] Entonces podemos construir un intervalo de confianza del 95% (esto indica que \\(0{,}95 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}05\\)) como: \\[ \\begin{aligned} P(z_{\\alpha/2} &lt; Z &lt; z_{1 - \\alpha/2}) &amp;= P\\left(-z_{1 - \\alpha/2} &lt; \\frac{133.93 - \\mu}{10 / \\sqrt{14}} &lt; z_{1 - \\alpha/2}\\right) = 0{,}95 \\\\ &amp;= P\\left(-z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; 133.93 - \\mu &lt; z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ &amp;= P\\left(-133.93 - z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; - \\mu &lt; -133.93 + z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ &amp;= P\\left(133.93 - z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; \\mu &lt; 133.93 + z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ \\end{aligned} \\] Tal que el intervalo es: \\[133.93 - z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; \\mu &lt; 133.93 + z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\] Como \\(\\alpha=0{,}05\\), entonces \\(\\alpha/2=0{,}025\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución normal, o usando qnorm(.975, 0, 1) en R. En este caso, \\(z_{1 - \\alpha/2} = 1.96\\), de forma que: \\[128.69 \\text{ mmHg} &lt; \\mu &lt; 139.17 \\text{ mmHg}\\] Podemos observar que el intervalo ocupa valores de presión sanguínea que podriamos considerar alta, indicando que el paciente sufre de hipertensión, de lo cual podriamos estar seguros con un 95% de confianza. En el ejemplo anterior, usamos lo aprendido en la seccion anterior sobre estadísticos sobre la media. Pero esto supone que la distribución subyacente de los datos es normal, lo cual hace obvio preguntar ¿cómo sé que mis datos son normales? Podemos tratar de ver que tan bien se ajustan nuestros datos a nuestro supuesto de normalidad, evaluando un gráfico QQ (cuantil-cuantil). Este gráfico muestra en el eje horizontal la distribución teorica (la normal estándar) y en el eje vertical, la distribución de las observaciones. Si la distribución de las observaciones fuera normal, los puntos observados en la gráfica caerían excatamente sobre la recta central (esta corresponde a el caso teorico de esperado si la data fuera normal realmente). De otro modo, si la distribución subyacente no es normal, los puntos se desviaran más de la recta central. La froma como estos se desvían de la recta puede ayudar a indicar cual es la distribución subyacente, o darnos cuenta de observaciones anormales o atípicas. El siguiente es un gráfico QQ para los datos de presión sanguínea: Figure 11.3: Gráfico QQ para los datos de presión sanguínea del ejemplo del texto. Se observa en el gráfico que las observaciones que caen por debajo de la media se desvían más de la normalidad que aquellas por encima o alrededor de la media. Esto nos indica que estos valores pueden resultar ser atípicos, lo cual podría corresponder bien con la conclusión de que es bastante probable que el paciente parece tener hipertensión (lo cuál hace bastante extraño obtener valores de presión menores a 128 mmHg). Ahora, ya dijimos que el TLC es aplicable a estadísticos que son estimadores de parámetros que corresponden a distribuciones otras que la normal. Los siguientes sirven de ejemplos de inferencia sobre parámetros de una distribución Poisson y Binomial, respectivamente. Ejemplo. Unos nutricionistas especializados en dieta canina han estado evaluando la efectividad de cierta dieta (con un componente nutricional especial) como medida para controlar la presencia de garrapatas en mascotas cuidadas bajo las mismas condiciones. Para ello, se seleccionaron y asignaron al azar \\(20\\) caninos a dos grupos, 10 en cada grupo. A uno se le administró la nueva dieta, y el otro se alimento con la misma dieta, pero sin el compuesto antiectoparásitos dado al pirmer grupo (este sirve como grupo control para verifiar si existen diferencias). Después de un tiempo apropiado, se midió en los canino el número de garrapatas por individuo. Los datos son los siguientes: Control: \\(22, 17, 15, 7, 12, 16, 12, 14, 20, 13\\) Tratamiento: \\(4, 5, 10, 7, 2, 2, 6, 10, 7, 2\\) Se desea saber si hay un cambio en el número promedio de garrapatas registrado en los caninos debido a la dieta. Solución. La variable aleatoria se trata de un conteo por unidad de muestreo, que ya hemnos estudiado corresponde bien a una ley de probabilidad Poisson. Para cada grupo, se puede calcular el valor promedio observado de garrapatas por individuos, obteniendo \\(\\hat{\\lambda}_C = 14{,}8\\) y \\(\\hat{\\lambda}_T = 5{,}5\\) (el cual recordamos corresponde también a la varianza). Podemos obtener entonces intervalos del 95% de confianza para ambos parámetros siguiendo el procedimiento del ejemplo anterior. Construimos el estadístico: \\[\\hat{Z} = \\frac{\\hat{\\lambda}_j - \\lambda}{\\sqrt{\\hat{\\lambda}_j/n}}\\] donde \\(j = C\\) o \\(j = T\\), dependiendo del grupo. Y luego, aplicando TLC, sabemos que \\(\\hat{Z}\\) sigue una distribcuión normal estándar. Sin embargo, el tamaño de la muestra es bastante pequeña por lo que es conveniente usar la distribución \\(t\\)-Student para compensar esta falta de certeza en el valor del parámetro. De forma que intervalo del 95% de confianza para el \\(\\lambda_C\\) (el control) es: \\[ \\begin{aligned} 14.8 - t_{9, 1 - \\alpha/2}\\sqrt{\\frac{14.8}{10}} &lt; &amp;\\lambda_C &lt; 14.8 + t_{9, 1 - \\alpha/2}\\sqrt{\\frac{14.8}{10}} \\\\ 12.05 &lt; &amp;\\lambda_C &lt; 17.55 \\end{aligned} \\] y para el tratameinto: \\[3.82 &lt; \\lambda_T &lt; 7.18\\] Puede observar que los intervalos para el número pormedio de garrapatas por perro del control y el tratamiento no se solapan en absoluto: el control parece indicar que hay aproximadamente entre \\(2\\) y \\(3\\) veces más garrapatas en el control que en el tratamiento con la nueva dieta. Esto apoya la conclusión de que la dieta es efectiva en controlar los ectoparásitos es los caninos con un nivel de certeza del 95%. Si queremos ser más explicitos, podriamos incluso construir un intervalo de confianza para la diferencia en el número promedio de garrapatas por canino. Para ello, podemos seguir la construcción del estadístico usada anteriormente reconociendo que ahora \\(\\theta = \\lambda_T - \\lambda_C\\), diferencia que denotaremos como \\(D\\). De esta forma: \\[\\hat{t} = \\frac{\\hat{D} - D}{SE(\\hat{D})} = \\frac{(\\hat{\\lambda}_T - \\hat{\\lambda}_C) - (\\lambda_T - \\lambda_C)}{SE(\\hat{D})}\\] El valor de \\(SE(\\hat{D})\\) se calcula notando que, por los intervalos individuales, la varianza del control parece ser mayor que la del tratamiento. Entonces, podemos usar la teoría de propagación de errores como: \\[SE(\\hat{D}) = \\sqrt{\\frac{\\hat{\\lambda}_T}{10} + \\frac{\\hat{\\lambda}_C}{10}} = 1.425\\] Por lo tanto, escribimos el intervalo para el estadístico como: \\[t_{\\nu, \\alpha/2} &lt; \\frac{-9.3 - (\\lambda_T - \\lambda_C)}{1.425} &lt; t_{\\nu, 1 - \\alpha/2}\\] donde el valor de \\(\\nu\\), los grados de libertad, viene dado por la expresión: \\[\\nu=\\frac{(\\hat{\\lambda}_T/n_T + \\hat{\\lambda}_C/n_C)^2}{[(\\hat{\\lambda}_T/n_T)^2/(n_T - 1) + (\\hat{\\lambda}_C/n_C)^2/(n_C - 1)]} = \\frac{2{,}03^2}{0.277} = 14.88\\] De esta forma, se tiene que el cuantil \\(1 - \\alpha/2\\) de la distribución \\(t\\) con 14.88 grados de libertad es 2.13 y, luego de arreglar los terminos en la expresión del intervalo, se obtiene el intervalo de confianza del 95%: \\[-12.34 &lt; \\lambda_T - \\lambda_C &lt; -6.26\\] Notamos que el intervalo no contiene al cero, por lo que podemos concluir con un 95% de confianza que el número promedio de garrapatas por canino es distinto en el tratamiento y el control. Además, como los límites son ambos negativos, podemos concluir también que el tratamiento con la nueva dieta resulta en un número de garrapatas por canino menor que el encontrado en el control sin la dieta. En el ejemplo anterior debemos reconocer y enfatizar ciertas consideraciones que resultan de hacer inferencias sobre la media de dos muestras independientes. En estos casos, el estadístico se construye de la manera que dijimos al final de la sección anterior, como: \\[\\frac{\\hat{D} - D}{SE(D)} = \\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{SE(\\mu_1 - \\mu_2)}\\] La distribución muestral depende de lo que tomamos como \\(SE(\\mu_1 - \\mu_2)\\). Si conocemos la varianza poblacional, entonces no necesitamos calcular \\(SE(\\mu_1 - \\mu_2)\\), y la distribución muestral es una normal estándar. Si no conocemos la varianza, debemos estimar \\(SE(\\mu_1 - \\mu_2)\\) a partir de los datos. En este caso, debemos tomar decisiones dependiendo de si podemos asumir que las varianzas son iguales o no. Si las varianzas se asumen iguales, entonces \\[SE(\\bar{X}_1 - \\bar{X}_2) = S_{pool}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}, \\quad S_{pool}^2 = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\\] donde \\(S_{pool}^2\\) es la varianza ponderada por los grados de libertad de cada muestra particular. En este caso, la distribución muestral del estadístico es una \\(t\\)-Student con \\(n_1 + n_2 - 2\\) grados de libertad. Si las varianzas se asumen distintas, entonces \\[SE(\\bar{X}_1 - \\bar{X}_2) = \\sqrt{\\frac{S^2_1}{n_1} + \\frac{S^2_2}{n_2}}\\] esto es, el error estándar es la suma de los errores estándar de las muetras por separado. En este caso, la distribución muestral del estadístico es una \\(t\\)-Student cuyos grados de libertad son: \\[\\nu=\\frac{(S_1^2/n_1 + S_2^2/n_2)^2}{[(S_1^2/n_1)^2/(n_1 - 1) + (S_2^2/n_2)^2/(n_2 - 1)]}\\] Es importante que tome en cuenta estas posibilidades como hicimos en el ejemplo anterior: las varianzas no son conocidas y las asumimos distintas, por lo que usamos el caso dos del segundo inciso. Ahora, repasamos un ejemplo que involucra un parámetro de una binomial. Ejemplo. Se hicieron medidas del pico de dos grupos de pinzones terrestres medianos que vivían en la isla de Daphne Major, una de las islas Galápagos, durante una gran sequía en 1977. Un grupo de pinzones murió durante la sequía y otro grupo sobrevivió. Los datos de supervivientes y muertos de un total de \\(36\\) pinzones fueron los siguientes: Machos: \\(20\\) muertos y \\(27\\) supervivientes. Hembras: \\(10\\) muertos y \\(9\\) supervivientes. Se desea saber si hay una diferencia en la proporción de pinzones supervivientes a la sequía con respecto al sexo, es decir, si la porporción de supervivientes machos es la misma que la porporción de supervivientes hembras. Solución. Si definimos la v. a. binomial superviviente como \\(1\\) si el pinzón sobrevivió a la sequia, y \\(0\\) de otro modo, entonces vemos que la variable, para machos y hembras, sigue una distribución binomial cuya probabilidad de éxito (supervivencia) \\(\\pi_i\\) (\\(i = M\\) o \\(H\\), dependiendo de si consideramos machos o hembras, respectivamente) puede estimarse usando la proporción observada de exitos: \\[\\begin{cases} p_M = \\frac{27}{47} &amp; \\text{para los machos} \\\\ p_H = \\frac{9}{19} &amp; \\text{para las hembras} \\end{cases}\\] Anteriormente, ya vimos que la varianza de una binomial es \\(np(1-p)\\), de forma que el error estándar es \\(SE(p) = \\sqrt{p(1-p)/ n}\\). De esta forma tenemos que: \\[SE(p_M) = 0.072\\] y \\[SE(p_H) = 0.115\\] Primero, veamos como lucen intervalos de confianza del 95% para cada una de estas proporciones. Aplicando TLC, sabemos que \\(p\\) tiene una distribución normal, por lo que se escribe el estadístico \\[\\hat{t} = \\frac{p_i - \\pi_i}{SE(p_i)}\\] para machos, \\(i = M\\), y hembras, \\(i= H\\). Note que usamos una distribución \\(t\\), dado que el grupo con el \\(n\\) más pequeño es menor que \\(30\\) (nuestro limite para considerar aplicable el TLC con una normal estándar). De esta forma, procedemos de la manera como ya hemos visto y obtenemos los intervalos (verifique los resultados usted mismo): \\[0.429 &lt; \\pi_M &lt; 0.72\\] y \\[0.233 &lt; \\pi_H &lt; 0.714\\] Notamos dos cosas en los intervalos: La primera es que el intervalo para las hembras es de mayor longitud que el de los machos. Esto es asi, ya que el \\(n\\) usado para estimar los limites del intervalo es mayor en los machos que en las hembras. Esto se traduce en que tenemos una mayor certidumbre sobre el valor del parámetro para los machos que para las hembras. Segundo, notamos que ambos intervalos contienen el \\(0{,}5\\). Esto queire decir, que la proporción observada no se puede considerar distinta de \\(0{,}5\\) con un 95% de confianza. Biologicamente, concluiriamos que la sequía elimino a la mitad de los individuos machos y hembras del grupo de pinzones. Tercero, como ambos intervalos contienen el \\(0{,}5\\), parece posible que estas proporciones no difieran significativamente entre si. El ultimo inciso, lo podemos verificar construyendo un intervalo para la diferencia en supervivencia de los pinzones machos y hembras en un solo intervalo. Primero escribimos la diferencia como \\(\\Delta = \\pi_M - \\pi_H\\), la cual se estima por \\(\\hat{\\Delta} = p_M - p_H\\). De esta forma, podemos escribir el estadístico como: \\[\\frac{\\hat{\\Delta} - \\Delta}{SE(\\hat{\\Delta})} = \\frac{(p_M - p_H) - (\\pi_M - \\pi_H)}{SE(\\hat{\\Delta})}\\] la cual, ya sabemos, se distribuye como una norml estándar. Se tiene que: \\[SE(\\hat{\\Delta}) = \\sqrt{\\frac{p_{M} (1 - p_{M})}{n_M} + \\frac{p_{H} (1 - p_{H})}{n_H}}\\] Introduciendo esta expresión en el estadístico y usando la distribución muestral para el intervalo, obtenemos (verifiquelo!): \\[-0.165 &lt; \\pi_M - \\pi_H &lt; 0.366\\] Noten de inmediato que este intervalo contiene al cero. Es por ello que podemos concluir que, con un 95% de confianza, no hay diferencias en la porporción de hembras y machos supevivientes a la sequía. Los ejemplos anteriores sirven para ver como realizar inferencia por medio de intervalos de confianza en medidas de locación para una y dos muestras independientes. Cuando las muestras son dependientes, como cuando mides antes y despues de aplicar un tratamiento experimental sobre los mismo individuos, el diseño se dice que es de medidas repetidas. En estos casos, se puede usar la diferencia entre ambos estados (antes y después) como um estadístico y tratarlo como si se tratara de una sola muestra (vea el problema 3). Ahora, veremos como realizar inferencia por intervalos de confianza, pero sobre la varianza. 11.3.2 Inferencia sobre la varianza. Como vimos antes, las inferencias sobre la varianza se pueden hacer usando como estadístico la proporción de varianzas escalada por los grados de libertad. Veamos unos ejemplos. Ejemplo. En un estudio de los efectos sobre los bebés que tiene el consumo de cocaína durante el embarazo, se obtuvieron los siguientes datos muestrales de pesos al nacer: \\(n=190\\), \\(\\bar{x} = 2700\\) g, \\(S = 645\\) g (según datos de Cognitive Outcomes of Preschool Children with Prenatal Cocaine Exposure, de Singer et al., Journal of American Medical Association, vol. 291, núm. 20). Utilice los datos muestrales para construir un estimado del intervalo de confianza del 95% para la desviación estándar de todos los pesos al nacer de hijos de madres que consumieron cocaína durante el embarazo. Con base en el resultado, ¿parece que la desviación estándar difiere de la desviación estándar de \\(696\\) g de los pesos al nacer de hijos de madres que no consumieron cocaína durante el embarazo? Solución. Antes ya vismo que un estadistico apropiado para realizar inferencia es: \\[X^2 = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2(n-1)\\] Como vemos, dado que conocemos la distribución muestral, podemos usarla para construir el intervalo como: \\[P(\\chi^2_{\\alpha/2, n-1} &lt; X^2 &lt; \\chi^2_{1-\\alpha/2, n-1}) = 1 - \\alpha\\] Como el intervalo de confianza es del 95%, esto indica que \\(0{,}95 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}05\\). Sustituyendo: \\[ \\begin{aligned} P(\\chi^2_{\\alpha/2, 189} &lt; X^2 &lt; \\chi^2_{1 - \\alpha/2, 189}) &amp;= P\\left(\\chi^2_{\\alpha/2, 189} &lt; \\frac{(189)(645\\text{ g})^2}{\\sigma^2} &lt; \\chi^2_{1 - \\alpha/2, 189}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{1}{\\chi^2_{1 - \\alpha/2, 189 }} &lt; \\frac{\\sigma^2}{(189)(645\\text{ g})^2} &lt; \\frac{1}{\\chi^2_{\\alpha/2, 189}}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{(189)(645\\text{ g})^2}{\\chi^2_{1 - \\alpha/2, 189}} &lt; \\sigma^2 &lt; \\frac{(189)(645\\text{ g})^2}{\\chi^2_{\\alpha/2, 189}}\\right) = 0{,}95 \\end{aligned} \\] Y el intervalo es: \\[\\frac{(189)(645\\text{ g})^2}{\\chi^2_{1 - \\alpha/2, 189}} &lt; \\sigma^2 &lt; \\frac{(189)(645\\text{ g})^2}{\\chi^2_{\\alpha/2, 189}}\\] Como \\(\\alpha=0{,}05\\), entonces \\(\\alpha/2=0{,}025\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución chi-cuadrado, o usando qchisq(.025, 189). En este caso, \\(\\chi^2_{\\alpha/2, 189} = 152.82\\). Se procede de igual manera para el otro cuantil y se obtiene el intervalo: \\[3.4341145\\times 10^{5} &lt; \\sigma^2 &lt; 5.1451145\\times 10^{5}\\] De forma que el intervalo para la desviación estándar se obtiene sacado raíz cuadrada: \\[586.01 \\text{ g} &lt; \\sigma &lt; 717.29 \\text{ g}\\] Ahora, tratemos de responder la pregunta de la investigación ¿difiere la desviación estándar de los pesos de bebés de madres expuestas a cocaína de la desviación estándar de \\(696\\) g de bebés de madres que no se expusieron a esa droga? El intervalo de confianza construido incluye dentro de su longitud el valor de \\(696\\) g, por lo que no podemos decir que la desviación estándar de los pesos de los bebés de madres expuestas a cocaína difiere del valor de \\(696\\) g de manera significativa. Esto significa que la variavilidad de los pesos de bebés es la misma sea que provengan de madres expuestas a cocaína o no. Ahira veamos un ejemplo aplicado a la comparación de varianzas de dos poblaciones distintas. Se midieron los tamaños de \\(30\\) cráneos de gorilas hembras y \\(29\\) machos (datos de O’Higgins, 1989). La varianza de las hembras es \\(39{,}7\\), mientras que la varianza de los machos es \\(105{,}9\\). ¿Son las varianzas de los tamaños de cráneos de machos y hembras iguales? Solución. Más arriba ya vimos que cuando queremos comaprar dos varianzas, podemos usar el cociente entre los estadísticos construidos para estos que siguen una distribución Chi-cuadrado: \\[\\hat{F} = \\frac{\\sigma_H^2 S_M^2}{\\sigma_M^2 S_H^2} \\sim F(n_1 -1, n_2 - 1)\\] de forma que podemos usar esta distribución muestral para construir el intervalo como: \\[P(f_{\\alpha/2, n_1-1, n_2-1} &lt; F &lt; f_{1 - \\alpha/2, n_1-1, n_2-1}) = 1 - \\alpha\\] Si elegimos un nivel de significancia de \\(0{,}05\\), esto indica que \\(1 - 0{,}05 = 0{,}95\\), por lo que el intervalo es de 95% de confianza. Sustituyendo: \\[ \\begin{aligned} P(f_{\\alpha/2, \\nu_1, \\nu_2} &lt; F &lt; f_{1 - \\alpha/2, \\nu_1, \\nu_2}) &amp;= P\\left(f_{0{,}025, 29, 28} &lt; \\frac{105{,}9\\sigma_H^2}{39{,}7\\sigma_M^2} &lt; f_{0{,}975, 29, 28}\\right) = 0{,}95 \\\\ &amp;= P\\left(f_{0{,}025, 29, 28}\\frac{39{,}7}{105{,}9} &lt; \\frac{\\sigma_H^2}{\\sigma_M^2} &lt; f_{0{,}975, 29, 28}\\frac{39{,}7}{105{,}9}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{1}{f_{0{,}975, 29, 28}}\\frac{105{,}9}{39{,}7} &lt; \\frac{\\sigma_M^2}{\\sigma_H^2} &lt; \\frac{1}{f_{0{,}025, 29, 28}}\\frac{105{,}9}{39{,}7}\\right) = 0{,}95 \\end{aligned} \\] Y el intervalo es: \\[\\frac{1}{f_{0{,}975, 29, 28}}\\frac{105{,}9}{39{,}7} &lt; \\frac{\\sigma_M^2}{\\sigma_H^2} &lt; \\frac{1}{f_{0{,}025, 29, 28}}\\frac{105{,}9}{39{,}7}\\] Para la distribución \\(F\\), \\(f_{0{,}025, 29, 28} = 0.47\\). Se procede de igual manera para el otro cuantil y se obtiene el intervalo: \\[1.26 &lt; \\frac{\\sigma_M^2}{\\sigma_H^2} &lt; 5.63\\] ¿Qué nos dice el intervalo sobre las varianzas de los cráneos de gorilas machos y hembras? Si las varianszas fueran iguales, entonces el cociente de las varianzas sería igual a \\(1\\). Notamos que el \\(1\\) no se encuentra dentro del intervalo, por lo que podemos decir, con un \\(95\\)% de confianza, que la varinaza de los cráneos de gorilas machos y hembras son distintas. No solo eso, sino que además, como los límites del intervalo son mayores a \\(1\\), entonces la varianza de los cráneos de los gorilas machos es mayor que la varianza de los cráneos de gorilas hembras. "],["ejercicios.-1.html", "11.4 Ejercicios.", " 11.4 Ejercicios. Control del plomo en el aire. A continuación se listan las cantidades de plomo medidas (en microgramos por metro cúbico o \\(\\mu g\\) \\({m}^{-3}\\)) en el aire: \\(5{,}40, 1{,}10, 0{,}42, 0{,}73, 0{,}48\\), y \\(1{,}10\\). La Environmental Protection Agency estableció un estándar de calidad del aire para el plomo de \\(1{,}5\\) \\(\\mu g\\) \\({m}^{-3}\\). Las medidas que se presentan abajo se registraron en el edificio 5 del World Trade Center en diferentes días, inmediatamente después de la destrucción causada por los ataques terroristas del 11 de septiembre de 2001. Después del colapso de los dos edificios hubo una gran preocupación por la calidad del aire. Utilice los valores dados para construir un estimado del intervalo de confianza del 95% para la cantidad media de plomo en el aire. ¿Hay algo en este conjunto de datos que sugiera que el intervalo de confianza tal vez no sea muy bueno? Explique. Se ha realizado un estudio sobre la velocidad en vuelo de diversas especies de pájaros. El propósito era comparar las velocidades del pelícano pardo y el ostrero americano. Se cronometró una muestra de \\(9\\) pajaros pardos y \\(12\\) pajaros ostreros, volando con el viento de costado con una velocidad de viento de \\(5\\) a \\(8\\) millas h\\({}^{-1}\\), y se obtuvo que el pájaro pardo vuela, en promedio, a \\(26{,}05 \\pm 6{,}34\\) millas h\\({}^{-1}\\), y el ostrero a \\(30{,}19 \\pm 3{,}20\\) millas h\\({}^{-1}\\). Construya un intervalo de confianza del 95% para la proporción de varianzas. Un farmaco aun más eficiente contra ectoparásitos ha sido desarrollado. Al momento de realizar el experimento para probar su efectividad en caninos, solo se contaban con \\(14\\) perros. Entonces, en lugar de dividir al conjunto en dos grupos, se decidió que cada perro fuera su propio control, midiendo el numero de garrapatas antes de darles el medicamento y despues de pasar un tiempo con el mismo, observando el cambio en el numero de garrapatas de cada individuo. Los resultados se muestran en la tabla a continuacion. Construya un intervalo de confianza apropiado y uselo para determinar si el nuevo medicamento es capaz de disminuir el número de ectoparásitos en caninos. Antes Despues 13 8 14 10 18 6 14 7 13 8 25 9 19 8 15 9 17 6 14 13 10 10 15 8 12 10 12 10 Se tiene un lote de semillas certificadas que se usaran en un ensayo de eficacia de un biocontrolador que tiene propiedades de potenciacion del crecimiento. Previo al ensayo, se desea determinar el porcentaje de germinacion del lote, para asi tener información suficiente para diseñar el ensayo de eficacia. Para esto se decide sembrar 30 semillas en bolsas de vivero con tierra comun, regandolas con agua cada día por 15 días, resultando en un procentaje de germinacion del 70%. Construya un intervalo de confianza del 95% para el porcentaje de germinación. Se tiene otro lote de semillas más recientes, cuyo porcentaje de germinación fue del 88% luego de un ensayo con 25 semillas. Realice un intervalonde confianza para la diferencia entre el procentaje de germinacion del nuevo lote con respecto al lote viejo. Concluya sobre qué semillas usaría para el ensayo de eficacia y por qué. Luego de varios meses de investigación y modificaciones al proceso de producción, usted a logrado aumentar la cantidad de proteína celular obtenida de biorreactores sumergidos de A. oryzae, donde inicialmente se tenia un rendimiento de \\(8 \\pm 3\\) kg L\\({}^{-1}\\) (basado en una muestra de 3 procesos seguidos durante esos meses de pruebas). Medidas obtenidas para el nuevo proceso (\\(n=3\\)) muestran un incremento en la producción a \\(11{,}5 \\pm 5\\) kg L\\({}^{-1}\\) (asuma que las medidas de dispersión reportada corresponden a desviaciones estándar). Construya IC95% para las desviaciones estándar de ambos procesos y comente sobre los resultados ¿Cree usted adecuado el adoptar el nuevo prpceso productivo? Construya un IC95% para la proporción de varianzas de ambos procesos y comente los resultados ¿Cuáles cree usted serían los pasos siguientes a seguir para aumentar la producción de proteína celular si se desea adoptar el segundo proceso? Un investigador busca poder inducir la formación de callos a partir de semillas de moringa para la obtención de metabolitos secundarios de interés farmaceutico. El trabajo para poder obtener un primer biorrecator piloto de callos es largo y requiere de la caracterizacion del proceso de inducción. Uno de los pasos requiere el contabilizar el número de aberraciones cromosómicas por célula que aparecen como consecuencia del tratamiento con los factores de crecimiento (FC) usados. Luego de inducir la callogénesis usando dos concnetraciones de FC (una alta y otra baja), se obtuvieron los siguientes resultados: Control: 9,5,4,6,4,3,5,2,4. FC Baja: 12,8,5,7,9,6,9,6,10,6,8. FC Alta: 13,15,7,16,15,11,6,8,15,8. Construya un intervalo de confianza para la diferencia de cada tratamiento de FC con respecto al control. Si se busca controlar el número de aberraciones cromosómicas que aparecen, ¿Qué tratamiento usaría usted? Siguiendo con el ejemplo de los grupos de pinzones de la isla Daphne Major sobrevivientes y muertos durante la sequía de 1977, se registraron las masas corporales de estos, tanto para machos como para hembras. Los resultados, presentados como medias \\(\\pm\\) desviación estándar, se muestran en la tabla a continuación. Utilice los datos para construir intervalos de confianza apropiados para verificar si hay una diferencia en la masa corporal de los pinzones que sobrevivieron con respecto a los que no sobrevivieron, tanto para machos como para hembras. En caso de encontrar diferencias, ¿por qué cree que las hay? De un significado biológico a sus resultados. Estado Sexo n Media SD Superviviente Macho 27 17.63 1.66 Superviviente Hembra 9 17.06 1.83 Muerto Macho 20 16.12 1.42 Muerto Hembra 10 15.60 1.24 "],["introducción-al-contraste-de-hipótesis..html", "12 Introducción al Contraste de Hipótesis.", " 12 Introducción al Contraste de Hipótesis. Sirve como método que facilite el proceso de toma de decisión en base a datos recolectados de una población. Su finalidad es producir conclusiones sobre la población partiendo de una hipótesis particular. Se parte de una conjetura inicial sobre el sistema en estudio, esta determina la forma de . Se necesitan de datos experimentales. La hipótesis estadística es una aseveración o conjetura respecto a una o más poblaciones que se estudian. Puede ser verdadera o no. Una decisión tomada en base al contraste, esta plagada de incertidumbre. Debe ser formalizada en un planteamiento matemático concreto. "],["nos-podemos-equivocar.html", "12.1 Nos podemos equivocar…", " 12.1 Nos podemos equivocar… La decisión tomada esta enlazada a una medida de incertidumbre, por lo que es posible cometer errores en las conclusiones. Cualquier afirmación estadística, por tanto, debe ser establecida en conjunto con una medida de que tan seguros estamos de que la decisión tomada es correcta. La razón principal es la dependencia con la muestra usada para refutar o ratificar la hipótesis planteada. Ejemplo. Se examinó la influencia del fármaco succinilcolina sobre los niveles de circulación de andrógenos en la sangre. Se obtuvieron muestras de sangre de venados salvajes inmediatamente después de recibir una inyección intramuscular de succinilcolina con dardos de un rifle de caza. Treinta minutos después se obtuvo una segunda muestra de sangre y después los venados fueron liberados. Los niveles de andrógenos de 15 venados al momento de la captura y 30 minutos más tarde, medidos en nanogramos por mililitro (ng \\({\\text{mL}}^{-1}\\)), se presentan en la tabla. Conc. Adrogenos (ng/mL) Venado Al inyectar 30 min después 1 2.76 7.02 2 5.18 3.10 3 2.68 5.44 4 3.05 3.99 5 4.10 5.21 6 7.05 10.26 7 6.60 13.91 8 4.79 18.53 9 7.39 7.91 10 7.30 4.85 11 11.78 11.10 12 3.90 3.74 13 26.00 94.03 14 67.48 94.03 15 17.04 41.70 De los métodos de estimación sabemos que la cantidad de andrógenos al momento de la inyección es de \\(11.81 \\pm 16.64\\). Mientras que la concentración de andrógenos 30 minutos después de la inyección fue de \\(21.65 \\pm 30.92\\). Podemos construir un intervalo de confianza para las diferencias entre la concentración de andrógenos antes y después, para cada venado. \\[-0.38 &lt; D &lt; 20.08\\] El planteamiento de un sistema de experimentación comienza con una conjetura sobre lo que se desea estudiar (en el ejemplo anterior, se desea saber si la succinilcolina es capaz de disminuir los niveles de andrógenos en la sangre). Nos preocuparemos por obtener una muestra de tamaño \\(n\\) que esté descrita por los valores \\(x_1, x_2, \\ldots, x_n\\) de una variable aleatoria \\(X\\). Suponemos que cada valor es independiente de los demás. Por tanto, podemos conceptualizar estos valores como una secuencia \\(X_1, X_2, \\ldots, X_n\\) de variables aleatorias independientes e idénticamente distribuidas, cada una de las cuales tiene la misma distribución que \\(X\\). \\[ \\begin{aligned} \\text{Conjetura}: &amp; \\text{ La succinilcolina modifica la concentración de andrógeno en venados.} \\\\ \\text{No hay cambio}: &amp; \\text{ La succinilcolina no cambia la concentración de andrógeno en venados.} \\end{aligned} \\] La formalización comienza con el establecimiento de un conjunto de hipótesis posibles al que llamamos \\(\\Theta\\); y luego, se seleccionan dos hipótesis \\(\\Theta_0 \\subseteq \\Theta\\) y \\(\\Theta_1 \\subseteq \\Theta\\) tales que: \\[\\Theta_0 \\cup \\Theta_1 = \\Theta, \\qquad \\Theta_0 \\cap \\Theta_1 = \\emptyset\\] Al conjunto \\(\\Theta_0\\) se le conoce como hipótesis nula, \\(H_0\\); y al conjunto \\(\\Theta_1\\) se le conoce como hipótesis alternativa, \\(H_1\\); y se escribe: \\[ \\begin{aligned} H_0: &amp; \\Theta_0 \\subseteq \\Theta \\\\ H_1: &amp; \\Theta_1 \\subseteq \\Theta \\end{aligned} \\] Además, si se hace un contraste para obtener una conclusión con respecto a un parámetro \\(\\theta\\), el conjunto \\(\\Theta_0\\) debe contener la conjetura de que no hay un cambio en el valor esperado del parámetro (más específicamente, en la desviación esperada). En el ejemplo sobre el efecto de la succinilcolina sobre la concentración de andrógeno en venados, vimos que la diferencia en la concentración de andrógenos antes y después de la inyección de succinilcolina sirve como variable aleatoria para realizar inferencias. De forma que podemos definir el conjunto de todas las posibles hipótesis como \\(\\Theta = \\{D \\in\\mathbb{R}\\}\\). Entonces: \\[ \\begin{aligned} H_0: &amp; \\{D \\in \\mathbb{R}\\vert D = 0\\} \\\\ H_1: &amp; \\{D \\in \\mathbb{R}\\vert D \\ne 0\\} \\end{aligned} \\] \\[ \\begin{aligned} H_0: &amp; D = 0 \\\\ H_1: &amp; D \\ne 0 \\end{aligned} \\] La forma de las hipótesis planteadas para un experimento es algo subjetiva, dado que su proposición depende de lo que sabe el investigador y su experiencia. Las condiciones de los conjuntos asociados a cada hipótesis pueden ser más precisos, dando lugar a pruebas unilaterales. Por ejemplo: \\[ \\begin{aligned} H_0: &amp; D \\le 0 \\\\ H_1: &amp; D &gt; 0 \\end{aligned} \\] La hipótesis a probar es la hipótesis nula. Esta es la que se toma como cierta, como la proposición de que no hay cambio alguno, y debemos utilizar la información disponible como evidencia para respaldar nuestra conjetura alternativa. La hipótesis nula nunca se acepta, solo no se rechaza. No es que la condición de igualdad no se mantenga, sino que la información que se tiene no es capaz de refutarla. "],["estadístico-de-prueba..html", "12.2 Estadístico de Prueba.", " 12.2 Estadístico de Prueba. El estadístico de prueba se construye a partir de la información que se tiene del parámetro poblacional \\(\\theta\\), es decir, usando el estimador \\(\\hat{\\theta}\\) calculado a partir de la muestra de tamaño \\(n\\). Para desviaciones con respecto a un valor promedio: \\[\\hat{Z} = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)} \\sim N(0,1), \\qquad \\hat{T} = \\frac{\\hat{\\theta} - \\theta}{\\hat{SE(\\hat{\\theta})}} \\sim t(n-1)\\] Para contrastes de hipótesis sobre la varianza: \\[\\hat{\\chi}^2 = \\frac{(n-1) Var(\\hat{\\theta})}{Var(\\theta)} \\sim \\chi^2(n-1), \\qquad \\hat{F} = \\frac{Var(\\hat{\\theta_1})}{Var(\\hat{\\theta_2})} \\sim F(n_1 - 1, n_2 - 1)\\] Ejemplo. Siguiendo con nuestro ejemplo de la concentración de andrógenos luego de una inyección de succinilcolina, podemos trabajar con las diferencias \\(d_i\\) (pata \\(i=1, \\ldots, n\\)) entre la concentración de andrógenos antes y 30 min después de la inyección del metabolito. Cada \\(d_i\\) es la realización de una variable aleatoria \\(D_i\\), cuya ley de probabilidad es la misma para todo \\(i=1,\\ldots, n\\). Podemos construir un estadístico basado en estas diferencias para verificar si de verdad hay un cambio en la concentración de andrógenos: \\[\\bar{d} - D, \\text{ que para el caso particular, } D=0 \\text{ por lo que queda } \\bar{d} - 0\\] \\[\\frac{\\bar{d} - 0}{\\hat{S}_d/\\sqrt{n}} = 0.14 \\sim t(n - 1)\\] Veamos otro ejemplo. Digamos que en lugar de un contraste sobre la media, se busca un contraste sobre la mediana para el mismo estudio de la succinilcolina. En este caso, se esperaría que la cantidad de diferencias negativas y positivas fuera la misma, si la distribución de probabilidad fuera la misma entre ambas. ¿Cómo construyo in estadístico en este caso? El razonamiento para construir el estadístico en este caso, es darnos cuenta que si hay un efecto del tratamiento, uno esperaría encontrar diferencias grandes (en valor absoluto) entre las concentraciones de andrógenos antes y despúes. Si asignaramos rangos al valor absoluto de cada diferencia, de tal forma que la diferencia más pequeña en magnitud (en valor absoluto) tenga una puntuación de 1, la siguiente diferencia más pequeña una puntuación de 2, y asi susecivamente hasta la \\(n\\)-ésima diferencia; entonces la oración anterior se convierte en que esperaríamos que los rangos de mayor magnitud se distribuyeran preferencialmente sobre las diferencias de magnitud positiva. ¿Qué esperaríamos si no hubiese un efecto de la succinilcolina sobre las concentraciones de andrógenos? Los rangos (de magnitud grande o pequeña) se distribuirían de manera más o menos aleatoria entre las diferencias positivas y negativas. \\[T^+ = \\sum_{i=1}^n R_i\\delta_i\\qquad \\delta_i = \\begin{cases}1, &amp;\\text{ si }Z_i &gt; 0 \\\\ 0, &amp; \\text{ de otra forma.}\\end{cases} \\text{ donde } Z_i = Y_i - X_i\\] Valores grandes de \\(W\\) son indicativos de que hay un efecto en el tratamiento y que hay un cambio en la concentración de andrógenos en la sangre de venados. "],["pz-ge-hatz.html", "12.3 \\[P(Z \\ge \\hat{Z}) = ???\\]", " 12.3 \\[P(Z \\ge \\hat{Z}) = ???\\] 12.3.1 Región crítica Al definir las hipótesis y calcular un estadístico de prueba, se define un conjunto de valores que puede tomar este último que permiten tomar una decisión sobre la hipótesis que la evidencia esta apoyando. Se puede escoger un valor crítico para el cual se puede definir la región crítica (que no es más que un subconjunto de valores del parámetro \\(\\theta\\)), la cual permite rechazar la hipótesis nula si el parámetro \\(\\theta\\) cae en esa región (pertenece al subconjunto). Ejemplo. Ya construimos un intervalo de confianza que nos sirve para delimitar las regiones (definir los subconjuntos) separadas por un valor de \\(d\\) crítico, dejando una región dentro del cual podemos decir que contiene el verdadero valor del parámetro \\(D\\): La región crítica es solo el complemento del intervalo de confianza: "],["el-p-valor-como-criterio-de-decisión..html", "12.4 El P-valor como criterio de decisión.", " 12.4 El P-valor como criterio de decisión. También es posible asociar un valor de probabilidad específica a obtener un estadístico tan grande como el calculado usando la función de distribución acumulada. Ejemplo. anteriormente, calculamos que el estadístico \\(\\hat{t}\\) calculado para las diferencias entre las concentraciones de andrógenos al momento y 30 minutos después de la inyección fue 0.14. Solo debemos calcular la probabilidad de obtener el valor de probabilidad acumulada para un valor de \\(t\\) tan grande como el calculado: \\[1 - P(t \\ge 0.14) = 0.4462\\] "],["beta-ptextrechazar-h_0-vert-h_1.html", "12.5 \\[1 - \\beta = P(\\text{Rechazar }H_0 \\vert H_1)\\]", " 12.5 \\[1 - \\beta = P(\\text{Rechazar }H_0 \\vert H_1)\\] 12.5.1 Errores de Decisión. Estado Real \\(H_0\\text{ cierta.}\\) \\(H_0\\text{ falsa.}\\) No rechazar \\(H_0\\) Decisión Correcta Error Tipo II Rechazar \\(H_0\\) Error Tipo I Decisión Correcta Ejemplo. Antiguos estudios muestran que el germicida DDT puede acumularse en el cuerpo. En 1965 la concentración media de DDT en las partes grasas del cuerpo de las personas en Estados Unidos fue de \\(9\\) ppm. Se espera que, como resultado de estrictos controles, esta concentración haya decrecido. \\[ \\begin{aligned} H_0: &amp; \\mu \\ge 9\\text{ ppm} \\\\ H_1: &amp; \\mu &lt; 9 \\text{ ppm} \\end{aligned} \\] Si rechazamos \\(H_0\\) siendo esta cierta… Si fallamos en rechazar \\(H_0\\), siendo esta falsa… 12.5.2 Pruebas paramétricas y no paramétricas. Las pruebas paramétricas definen claramente la escala de medida de la variable aleatoria, asi como también hacen suposiciones sobre la distribución poblacional y los parámetros de la misma (generalmente, una distribución normal). Las pruebas no paramétricas son aquellas que hacen suposiciones menos restrictivas sobre la escala sobre la cual es posible medir la variable aleatoria, y sobre la distribución poblacional, de la cual “no se asume nada sobre los parámetros”. La elección de la prueba adecuada es aquella que use mejor la información contenida en la muestra. Pero el desconocimiento del fenómeno o la distribución subyacente tiene un papel importante en la decisión de peso al elegir entre información y creencia. 12.5.3 Elección de la prueba estadistica. La elección de una prueba no viene determinada solo por la potencia de la misma: La manera como se obtuvo la muestra, la naturaleza de la población de la que se obtuvo la muestra, las hipotesis a probar, y el tipo de medición o escala que se empleó para la variable implicada. "],["referencias..html", "Referencias.", " Referencias. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
