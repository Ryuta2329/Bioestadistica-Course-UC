[["index.html", "Prefacio", " Prefacio Esta es el sitio web para la primera edición de Bioestadística, publicado en Julio de 2023. Este libro es el resultado de la preparación del material de los cursos de Bioestadística I y Bioestadística II dados en la Universidad de Carabobo, y los cuales tuve la suerte de dictar durante el primer periodo académico de la universidad en 2023. En este libro tú aprenderas "],["introducción..html", "1 Introducción.", " 1 Introducción. "],["teoría-de-probabilidades..html", "2 Teoría de Probabilidades.", " 2 Teoría de Probabilidades. "],["combinatoria..html", "3 Combinatoria.", " 3 Combinatoria. "],["otros-teoremas-de-probabilidades..html", "4 Otros Teoremas de Probabilidades.", " 4 Otros Teoremas de Probabilidades. "],["estadística-descriptiva..html", "5 Estadística Descriptiva.", " 5 Estadística Descriptiva. Un conjunto de datos se puede describir usando: Medidas de tendencia central: estas son valores que describen el centro alrededor del cual el conjunto de observaciones se distribuye. De esta forma, nos permite describir donde se localizan la mayoría de las observaciones. Son tres: Media: describe la localización media de las observaciones. Mediana: es el valor que distribuye las observaciones de tal forma que 50% de estas quedan por encima de ella, y el otro 50% por debajo. Moda: describe la posición de la (o las) observación (observaciones) más frecuente(s). Medidas de posición: vienen definidos por los cuantiles de una distribución. Un cuantil \\(C_p\\) se define como el valor que deja por debajo de si \\(p\\times100\\)% de las observaciones. Por ejemplo, el cuantil \\(C_{0{,}3}\\) es aquel valor que deja por debajo de si \\(30\\)% de las observaciones. Nos ayuda a describir la posición que una observación ocupa dentro del dominio sobre el cual se distribuyen los datos. Algunos ejemplos son: Los cuartiles: (\\(C_{0{,}25i}\\) con \\(i = 1,2,3\\)) que distribuyen las observaciones en \\(4\\) partes, Los deciles (\\(C_{0{,}1i}\\) con \\(i = 1,2,\\ldots,10\\)), que dividen la distribución en \\(10\\) partes, y Los percentiles (\\(C_{0{,}01i}\\) con \\(i = 1,2,\\ldots, 99, 100\\)), que dividen la distribución en \\(100\\) partes. Medidas de dispersión: estas son medidas de que tan variables son las observaciones. Sirven para describir la dispersión de las observaciones en su dominio y alrededor de su centro. Pueden ser: Rango o Recorrido: es la diferencia entre el valor máximo y el valor mínimo observado. Nos dice que tan amplio es el dominio ocupado por las observaciones, o que tan amplio es el intervalo sobre el cual se distribuyen todas las observaciones. Rango intercuartílico: es la diferencia entre el tercer cuartil y primer cuartil, y por lo tanto, describe la amplitud del intervalo que contiene a un \\(50\\)% de las observaciones. Varianza: es una medida de las diferencias cuadráticas promedio de las observaciones con respecto a la media (sumatoria de cuadrados promedio). Sirve como una medida de cuán variable es un conjunto de datos, dado que a mayor son las desviaciones de la media, más grande es la varianza. Desviación estándar: es una medida de la distancia promedio de las observaciones con respeto a la media. Al igual que antes para la varianza, la desviación estándar sirve como medida de variabilidad con respecto al centro, dado que a mayor la distancia de las observaciones a la media, mayor será la desviación estándar. Coeficiente de variación: es el valor proporcional de la desviación estándar con respecto a la media (Desviación estándar / Media). Esta sirve como medida de dispersión relativa, dado que permite comparar distribuciones basado en cuán distantes, en promedio, de la media están las observaciones, basados en el tamaño relativo de esta con respecto a la media. Medidas de forma: estas nos ayudan a describir la simetría y ensanchamiento de la distribución de las observaciones. Estas son: Asimetría: es un coeficiente cuyo valor nos permite decir si las observaciones se encuentran acumuladas a la derecha, o la izquierda de la distribución (a esto se le llama sesgo). Curtosis: esta describe que tan amplio es el pico de la distribución de observaciones, permitiéndonos decir si se trata de una colina amplía o de un pico estrecho. "],["datos-no-agrupados..html", "5.1 Datos no agrupados.", " 5.1 Datos no agrupados. Para comprender los conceptos de la estadística descriptiva en datos sin agrupar, usamos el conjunto de datos obtenidos del experimento que busca evaluar la reacción de saltamontes a estímulos visuales o acústicos, que midieron el tiempo de reacción a estos antes del vuelo: A estímulos acústicos: \\(86, 102, 103, 99, 108, 100, 118, 108, 109, 113, 114, 107, 107, 117, 120, 101, 126, 109, 106\\). A estímulos visuales: \\(72, 95, 73, 99, 71, 90, 102, 97, 71, 75, 80, 70, 100, 104, 81, 103, 101, 103, 77, 78, 89\\). 5.1.1 Medidas de Tendencia central. El promedio se define como: \\[\\bar{X} = \\frac{\\sum_{i=1}^k f_ix_i}{n}\\] dónde \\(x_i\\) es la \\(i\\)-ésima observación, y \\(f_i\\) es la frecuencia absoluta de la observación (cuantas veces se repite \\(x_i\\) entre el número de observaciones), y la sumatoria se hace sobre los \\(k\\) observaciones únicas (las observaciones repetidas no se toman en cuenta, ya se están tomando en cuenta al multiplicar por \\(f_i\\) el valor observado repetido). Para el tiempo de reacción a estímulos acústicos se calcularía entonces: \\[\\begin{aligned} \\bar{X}_a &amp;= \\frac{86+102+103+99+2\\times108+100+118+109+113+114+2\\times107+117+120+101+126+109+106}{19} \\\\ &amp;= 108{,}05 \\text{ segundos} \\end{aligned}\\] Procediendo de igual forma para el tiempo de reacción a estímulos visuales se obtiene \\(\\bar{X}_v=87{,}19\\) segundos (¡verifícalo!). La mediana para datos agrupados se consigue siguiendo los siguientes pasos: Se ordenan de menor a mayor las observaciones. Se asignan índices a los datos ordenados desde \\(1\\) a \\(n\\): al menor dato se le asigna el índice \\(1\\), al siguiente el \\(2\\), y así sucesivamente. Se calcula el índice de posición de la mediana como \\((n + 1)/2\\) si \\(n\\) es impar. Si \\(n\\) es par se calculan los índices de posición \\((n-1)/2\\) y \\((n+1)/2\\). La mediana es entonces: \\[M = \\begin{cases} x_{(n + 1)/2}, &amp; \\text{ si }n\\text{ es impar}. \\\\ \\frac{x_{(n-1)/2} + x_{(n+1)/2}}{2}, &amp; \\text{ si }n\\text{ es par.} \\end{cases}\\] Para el caso del tiempo de reacción a estímulos acústicos, tenemos que los datos ordenados son: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, 102_{(5)}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, 108_{(10)}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;114_{(15)}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] dónde el subscrito entre paréntesis corresponde al índice dado al dato. Cómo \\(n=19\\) es impar, se calcula el índice de posición \\((n+1)/2 = 10\\). Ubicamos en los datos ordenados la observación con el índice \\(10\\): \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, 102_{(5)}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, \\color{red}{108_{(10)}}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;114_{(15)}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] y este valor corresponde a la mediana, de forma que: \\[M =108 \\text{ segundos}\\] Se puede proceder de igual forma con el tiempo de reacción a estímulos visuales y obtener \\(M = 89\\text{ segundos}\\) (¡verifícalo!). La moda para datos no agrupados es la observación con la máxima frecuencia registrada (la frecuencia con la mayor magnitud): \\[Moda(\\{x_i\\}_{i=1}^n) = \\{x_i \\vert f_{x_i} = max(\\{f_{x_1}, f_{x_2}, \\ldots, f_{x_n}\\})\\}\\] (la notación \\(\\{x_i\\}_{i=1}^k\\) y \\(\\{x_1, x_2, \\ldots, x_n\\}\\) son equivalentes, es decir, \\(\\{x_i\\}_{i=1}^n = \\{x_1, x_2, \\ldots, x_n\\}\\)). Para calcular la moda se siguen los siguientes pasos: Se ordenan los datos de menor a mayor. Se calculan las frecuencias absolutas \\(f_{x_i}\\) contando el número de veces que la magnitud de una observación se repite. La moda es el valor que más se repita. Si hay más de un valor con la misma frecuencia absoluta, entonces la moda es un conjunto de todos los valores con la misma frecuencia absoluta. En el caso del tiempo de reacción a estímulos acústicos, los datos ordenados son: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(1)}, 100_{(1)}, 101_{(1)}, 102_{(1)}, 103_{(1)}, 106_{(1)}, 107_{(2)}, 108_{(2)}, \\\\ &amp;109_{(2)}, 113_{(1)}, 114_{(1)}, 117_{(1)}, 118_{(1)}, 120_{(1)}, 126_{(1)} \\end{aligned}\\] donde se muestra como subscrito en paréntesis la frecuencia absoluta \\(f_{x_i}\\). Como la \\(max(\\{f_{x_i}\\}) = 2\\), que es la frecuencia absoluta de \\(107, 108\\) y \\(109\\) todos en segundos, por lo que \\(Moda(\\{x_i\\}_{i=1}^n) = \\{107, 108, 109\\}\\) y los datos son multimodales. En el caso de estímulos visuales, \\(max(\\{f_{x_i}\\}) = 2\\) para \\(71\\) y \\(103\\) todos en segundos, por lo que \\(Moda(\\{x_i\\}_{i=1}^n) = \\{71, 103\\}\\) segundos (¡verifícalo!). A continuación se muestra un resumen de los estadísticos de tendencia central calculados para los tiempos de reacción acústico y visual, usando R: acoustic &lt;- c(86, 102, 103, 99, 108, 100, 118, 108, 109, 113, 114, 107, 107, 117, 120, 101, 126, 109, 106) visual &lt;- c(72, 95, 73, 99, 71, 90, 102, 97, 71, 75, 80, 70, 100, 104, 81, 103, 101, 103, 77, 78, 89) # Para el calculo de la moda f_a_acoustic &lt;- table(acoustic) mode_acoustic &lt;- names(f_a_acoustic)[which(f_a_acoustic == max(f_a_acoustic))] f_a_visual &lt;- table(visual) mode_visual &lt;- names(f_a_visual)[which(f_a_visual == max(f_a_visual))] tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Media = c(mean(acoustic), mean(visual)), Mediana = c(median(acoustic), median(visual)), Moda = c(paste(mode_acoustic, collapse=&quot;, &quot;), paste(mode_visual, collapse=&quot;, &quot;)) ) %&gt;% kbl() Estimulo Media Mediana Moda Acústico 108.05263 108 107, 108, 109 Visual 87.19048 89 71, 103 donde se muestra que: Para los tiempos de reacción a estímulos acústicos, la media y la mediana coinciden bastante, y aunque la moda consiste de tres elementos, la media y la mediana caen dentro de ese conjunto, lo cual indica que la distribución consiste de un solo pico simétrico. Para los tiempos de reacción a estímulos visuales, la mediana se desvía ligeramente de la media, indicando que la distribución es algo simétrica, pero como las modas están alejadas en promedio unas 16 unidades de la media/mediana, y unas 30 unidades entre sí, podemos decir que la distribución consiste de dos picos. Lo anterior implica que los saltamontes responden de forma única a estímulos acústicos, pero a los estímulos visuales una parte de la población de saltamontes responde rápidamente y la otra parte no tan rápido. 5.1.2 Medidas de posición. Para los datos no agrupados, los cuantiles se pueden calcular siguiendo los siguientes pasos: Se ordenan los datos de menor a mayor. Se asignan índices a las posiciones de cada observación. Se calcula la posición de los cuantiles usando: \\[C_i = \\begin{cases} \\frac{n\\times i}{d}, \\text{ si }n\\text{ es par.} \\\\ \\frac{(n+1)\\times i}{d}, \\text{ si }n\\text{ es impar.} \\end{cases}\\] Ubicamos las observaciones \\(X_{C_i}\\) correspondientes a cada cuantil. Si \\(C_i\\) no es un entero, se calcula el promedio \\(x_{C_i} = (x_{(C_i - 0{,}5)} + x_{(C_i + 0{,}5)}) / 2\\). dónde la \\(i\\) es un entero que corresponde al \\(i\\)-ésimo cuantil (ve más abajo el ejemplo), y la \\(d\\) representa en cuántas partes queremos dividir la distribución. Por ejemplo, si quisiéramos calcular los cuartiles, tendríamos que dividir la distribución en cuatro partes estableciendo \\(d=4\\), y las \\(i\\) tendrían valores de \\(1, 2\\) y \\(3\\) para los cuartiles \\(Q_1, Q_2\\) y \\(Q_3\\), respectivamente (cambiamos la notación de \\(C_i\\) a \\(Q_i\\) debido a que así se denotan usualmente en otros libros de texto y recursos). Para los tiempos de reacción ante estímulos acústicos ya se tienen los datos ordenados antes, junto con índices. Para calcular los cuartiles, como \\(n=19\\) es impar, calculamos \\(Q_i = (19 + 1)\\times i/4 = 5i\\), por lo que: \\[\\{Q_1, Q_2, Q_3\\} = \\{5, 10, 15\\}\\] Buscamos las observaciones en las posiciones dadas por el conjunto anterior: \\[\\begin{aligned} &amp;86_{(1)}, 99_{(2)}, 100_{(3)}, 101_{(4)}, \\color{red}{102_{(5)}}, 103_{(6)}, 106_{(7)}, 107_{(8)}, 107_{(9)}, \\color{red}{108_{(10)}}, 108_{(11)}, 109_{(12)}, 109_{(13)}, 113_{(14)}, \\\\ &amp;\\color{red}{114_{(15)}}, 117_{(16)}, 118_{(17)}, 120_{(18)}, 126_{(19)} \\end{aligned}\\] Por lo tanto, los cuartiles son: \\[\\{x_{Q_1}, x_{Q_2}, x_{Q_3}\\} = \\{102, 108, 114\\}\\] Notamos dos cosas: la primera es que la mediana y el cuartil 2 coinciden como se esperaba. Segundo, si vemos los datos ordenados con los cuartiles en rojo, vemos que entre cada cuartil hay exactamente 4 datos, es decir, la distribución se dividió en cuatro partes, cada una con exactamente la misma cantidad de datos. Para los tiempos de reacción a estímulos visuales, se puede encontrar que \\(\\{Q_1, Q_2, Q_3\\} = \\{5{,}5; 11; 16{,}5\\}\\). Esta vez, los cuartiles son decimales, por lo que podemos usar el valor promedio de las observaciones entre las observaciones adyacentes. Por ejemplo, para \\(Q_1 = 5{,}5\\), se ubican las observaciones \\(x_5 = 73\\) y \\(x_6 = 75\\) (que son los enteros adyacentes a \\(5{,}5\\)) y calculamos el promedio \\((73 + 75) / 2 = 74\\). Realizamos el mismo procedimiento para el tercer cuartil y entonces \\(\\{x_{Q_1}, x_{Q_2}, x_{Q_3}\\} = \\{74; 89; 100{,}5\\}\\) (¡Verifica los resultado!). Ejercicio. Calcula para los datos de tiempos de reacción de saltamontes a estímulos acústicos y visuales, los deciles (\\(d=10\\)) y percentiles (\\(d = 100\\)). A continuación, se resumen los estadísticos de posición en conjunto con los de tendencia central para los tiempos de reacción (añadimos por conveniencia el mínimo y máximo valor registrado): tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Min = c(min(acoustic), min(visual)), Q_1 = c(102, 74), Media = c(mean(acoustic), mean(visual)), Mediana = c(median(acoustic), median(visual)), Q_3 = c(114, 100.5), Max = c(max(acoustic), max(visual)), Moda = c(paste(mode_acoustic, collapse=&quot;, &quot;), paste(mode_visual, collapse=&quot;, &quot;)) ) %&gt;% kbl() Estimulo Min Q_1 Media Mediana Q_3 Max Moda Acústico 86 102 108.05263 108 114.0 126 107, 108, 109 Visual 70 74 87.19048 89 100.5 104 71, 103 donde podemos ver que la distribución de datos en el grupo que recibió estímulos acústicos es más equitativa, ya que los cuartiles se distancian uno del otro en \\(6\\) unidades, mientras que los cuartiles de los datos de estímulos visuales se distancian \\(15\\) y \\(11{,}5\\) unidades, por lo que hay más datos hacia tiempos de reacción altos, y no tanto en tiempos de reacción bajos. Esto ayuda a enfatizar la asimetría pequeña de la distribución de estímulos visuales, y la simetría de la de estímulos acústicos. 5.1.3 Medidas de Dispersión. El recorrido o rango se define como la diferencia con respecto al valor máximo y el mínimo: \\[R = x_{max} - x_{min}\\] Para los tiempos de reacción se tiene que para estímulos acústicos \\(R_a = 126 - 86 = 40\\) segundos, y para estímulos visuales \\(R = 104 - 70 = 34\\) segundos. Esto quiere decir, que el tiempo de reacción a estímulos visuales ocupa una mayor cantidad de posibles observaciones, lo cual lo hace más variable la respuesta que la respuesta a estímulos acústicos. El rango intercuartílico (\\(IQR\\)) es un recorrido pero tomado desde el primer cuartil al tercer cuartil: \\[IQR = Q_3-Q_1\\] Para los tiempos de reacción a estímulos acústicos \\(IQR = 114 -102 = 12\\) segundos, mientras que para estímulos visuales \\(IQR = 100{,}5 - 74 = 26{,}5\\) segundos. Esto quiere decir que el 50% de las observaciones típicas para la respuesta a estímulos acústicos caen en un intervalo más pequeño comparado con la respuesta típica a estímulos visuales, haciendo el tiempo de reacción más consistente en el primer caso. La varianza se define como: \\[S^2 = \\frac{\\sum_{i=1}^k f_i(x_i - \\bar{X})^2}{n - 1}\\] es decir, a cada observación le quitamos el valor medio, y al resultado lo elevamos al cuadrado. Luego, sumamos los resultados y dividimos entre \\(n-1\\). Para los datos de tiempos de reacción a estímulos acústicos, se tiene que las diferencias con respecto a la media son: \\[\\begin{aligned} &amp;-22{,}05; -6{,}05; -5{,}05; -9{,}05; -0{,}05; -8{,}05; 9{,}95; -0{,}05; 0{,}95; \\\\ &amp;4{,}95; 5{,}95; -1{,}05; -1{,}05; 8{,}95; 11{,}95; -7{,}05; 17{,}95; 0{,}95; -2{,}05 \\end{aligned}\\] donde las desviaciones más grandes son aquellas que se alejan más de la media. Elevando al cuadrado permite eliminar los signos, de forma que al sumar no se cancelen los terminos, se obtiene: \\[\\begin{aligned} &amp;486{,}32; 36{,}63; 25{,}53; 81{,}95; 0; 64{,}84; 98{,}95; 0; 0{,}9; 24{,}48; \\\\ &amp;35{,}37; 1{,}11; 1{,}11; 80{,}06; 142{,}74; 49{,}74; 322{,}11; 0{,}9; 4{,}21 \\end{aligned}\\] Luego, sumando estos valores se obtiene: \\[S^2 = \\frac{1456{,}95}{19 - 1} = 80{,}94\\text{ s}^2\\] Para los tiempos de reacción a estímulos visuales se tiene \\(S^2= 167{,}16\\text{ s}^2\\) (verifícalo!). La desviación estándar es: \\[S = \\sqrt{S^2}\\] Y sirve como medida de la distancia promedio de las observaciones con respecto a la media. Usando el resultado anterior, se tiene para los tiempos de reacción a estímulos acústicos \\(S = 8{,}997\\) segundos, y para los tiempos de reacción a estímulos visuales \\(S = 12{,}929\\) segundos. Esto quiere decir que el \\(68{,}2\\)% de las observaciones típicas para la respuesta a estímulos acústicos caen a una distancia de una \\(\\sim9\\) segundos de la media, mientras que para los estímulos visuales caen a unos \\(\\sim13\\) segundos de la media, haciendo más variable la respuesta a estímulos visuales. La última medida de variación importante es el coeficiente de variación, \\(CV\\), que se define como: \\[CV = \\frac{S}{\\bar{X}}\\] Y nos dice que tan grande es la desviación estándar con respecto a la media. Para una distribución normal este valor es \\(\\sim0{,}3\\) (\\(30\\)% en valor porcentual). Para los tiempos de reacción se tiene que para estímulos acústicos \\(CV = 8{,}997/ 108{,}05 = 0{,}0833\\) (\\(\\sim8{,}3\\)%), y para estímulos visuales \\(CV = 12{,}929 / 87{,}19 = 0{,}1483\\) (\\(\\sim14{,}8\\)%). Esto quiere decir, que el tiempo de reacción a estímulos visuales tiene una variación mayor (de aproximadamente 6% mayor) que la respuesta a estímulos acústicos, dado que la desviación estándar representa una mayor proporción de la magnitud de la media. Resumimos los estadísticos de dispersión a continuación: tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Rango = c(max(acoustic) - min(acoustic), max(visual) - min(visual)), IQR = c(114 - 102, 100.5 - 74), Varianza = c(var(acoustic), var(visual)), Std.Dev = c(sd(acoustic), sd(visual)), CV = c(sd(acoustic) / mean(acoustic), sd(visual) / mean(visual)) * 100 ) %&gt;% kbl() Estimulo Rango IQR Varianza Std.Dev CV Acústico 40 12.0 80.94152 8.996751 8.326267 Visual 34 26.5 167.16190 12.929111 14.828581 5.1.4 Medidas de Forma. El coeficiente de asimetría es una medida de forma que busca cuantificar la simetría de una distribución. Se calcula como: \\[A = \\frac{\\sum_{i=1}^n(x_i - \\bar{X})^3}{n S^3}\\] Para una distribución simétrica, se esperaría obtener el mismo número de diferencias \\((x_i - \\bar{X})^3\\) negativas como positivas, y al sumar la magnitud de todas las diferencias negativas, esta sería de igual magnitud que la suma de todas las diferencias positivas, por lo que \\(A = 0\\) si la distribución es simétrica (aunque lo contrario no es cierto: el que \\(A = 0\\) no asegura que la distribución sea simétrica). La asimetría puede ser positiva o negativa dependiendo de la dirección del sesgo. Para distribuciones sesgadas hacia la derecha, \\(A &gt; 0\\). Para distribuciones sesgadas hacia la izquierda, \\(A &lt; 0\\). Se puede calcular la sumatoria de cubos para los tiempos de reacción a estímulos acústicos y visuales, y obtener el coeficiente de asimetría, que para las población de saltamontes sometida a estímulos acústicos \\(A = -0{,}23\\), y para la que fue sometida a estímulos visuales \\(A = -0{,}01\\). Esto muestra que los tiempos de reacción a estímulos visuales es solo muy ligeramente asimétrica, pero que la de tiempos de reacción a estímulos acústicos es in poco más sesgada hacia la izquierda (aunque no tan apreciablemente). La curtosis se define como: \\[K = \\frac{\\sum_{i=1}^n(x_i - \\bar{X})^4}{n S^4} - 3\\] lo cual nos dice algo sobre la forma como se concentran los datos alrededor de la media. Para la distribución normal (que es simétrica y se usa como estándar de comparación) se puede verificar que \\(K = 0\\). Se dice entonces que la distribución: * Es platicúrtica si \\(K &lt; 0\\), y esto implica que las distribuciones extremas son más probables con respecto a la normal. * Es mesocúrtica si \\(K=0\\), y las observaciones se distribuyen más como una normal. * Es leptocúrtica si \\(K &gt; 0\\), y entonces las observaciones tienden a aglomerarse alrededor de la media más de lo que ocurre en la distribución normal. Para las población de saltamontes sometida a estímulos acústicos \\(K = 0{,}166\\), y para la que fue sometida a estímulos visuales \\(K = -1{,}79\\). Esto muestra que los tiempos de reacción a estímulos acústicos se asemeja a una distribución mesocúrtica y los tiempos de reacción a estímulos visuales es platicúrtica, indicando una distribución que tiene colas pesadas, siendo la de estímulos visuales mucho más dispersa alrededor de la media (esto tiene sentido dado nuestro descubrimiento de que se trata de una distribución multimodal). tibble( Estimulo = c(&quot;Acústico&quot;, &quot;Visual&quot;), Asimetria = c(1 / 19, 1 / 21) * c(sum((acoustic - mean(acoustic)) ** 3) / sd(acoustic) ** 3, sum((visual - mean(visual)) ** 3) / sd(visual) ** 3), Curtosis = c(1 / 19, 1 / 21) * c(sum((acoustic - mean(acoustic)) ** 4) / sd(acoustic) ** 4, sum((visual - mean(visual)) ** 4) / sd(visual) ** 4) - 3 ) %&gt;% kbl() Estimulo Asimetria Curtosis Acústico -0.2299108 0.1659499 Visual -0.0100155 -1.7877228 Ejercicio Para el siguiente conjunto de datos de pesos de individuos de una población, separados en Machos (M) y Hembras (H), realice un análisis descriptivo. # Datos Diferencias Peso de Machos y hembras ungrouped &lt;- tibble( Sex = unlist(strsplit(&quot;HMHMHMHMMHMMHHHMHMMHMHMMHMMHHMHMMHMMHMHMMHMHMMHHMM&quot;, &quot;&quot;)), Peso = c(98.5, 150.6, 108.3, 159.4, 162.6, 122.5, 118.5, 167.5, 170.5, 120.4, 177.5, 186.5, 115.4, 115.5, 52.5, 157.6, 134.4, 148.5, 131.5, 143.4, 145.6, 108.6, 155.5, 110.6, 154.5, 183.5, 191.5, 128.6, 135.4, 195.4, 137.5, 205.6, 190.7, 120.5, 188.7, 176.3, 118.5, 158.5, 116.5, 161.4, 165.5, 142.6, 164.6, 120.5, 170.4, 195.5, 132.5, 129.5, 215.6, 176.5) ) "],["datos-agrupados..html", "5.2 Datos Agrupados.", " 5.2 Datos Agrupados. Para comprender los conceptos de la estadística descriptiva en datos agrupados, usamos el conjunto de datos obtenidos del experimento que busca evaluar los niveles de glicemia (en mg dL\\({}^{-1}\\)) en 25 pacientes, cuyos resultados fueron: \\[75, 82, 90, 95, 101, 112, 121, 132, 140, 97, 84, 90, 96, 102, 114, 121, 138, 87, 91, 96, 104, 123, 89, 93, 99\\] Como vimos antes, estos datos se pueden agrupar en clases que denotan intervalos (que pueden ser continuos o aparentes) en los cuales caen las observaciones con cierta frecuencia absoluta, como se muestra a continuación: glicemia &lt;- tribble( ~Lim_rel_inf, ~Lim_rel_sup, ~`Marca de Clase`, ~`f_i`, ~`fr_i`, ~`F_i`, ~`Fr_i`, 74.5, 86.5, 80.5, 3, 12 / 100, 3, 12 / 100, 86.5, 98.5, 92.5, 10, 40 / 100, 13, 52 / 100, 98.5, 110.5, 104.5, 4, 16 / 100, 17, 68 / 100, 110.5, 122.5, 116.5, 4, 16 / 100, 21, 84 / 100, 122.5, 134.5, 128.5, 2, 8 / 100, 23, 92 / 100, 134.5, 146.5, 140.5, 2, 8 / 100, 25, 100 / 100) glicemia %&gt;% kbl() Lim_rel_inf Lim_rel_sup Marca de Clase f_i fr_i F_i Fr_i 74.5 86.5 80.5 3 0.12 3 0.12 86.5 98.5 92.5 10 0.40 13 0.52 98.5 110.5 104.5 4 0.16 17 0.68 110.5 122.5 116.5 4 0.16 21 0.84 122.5 134.5 128.5 2 0.08 23 0.92 134.5 146.5 140.5 2 0.08 25 1.00 5.2.1 Medidas de Tendencia Central. El promedio se define como: \\[\\bar{X} = \\frac{\\sum_{i=1}^k f_ic_i}{n}\\] dónde \\(c_i\\) es la \\(i\\)-ésima marca de clase, y \\(f_i\\) es la frecuencia absoluta asociada a la marca de clase (cuantas observaciones \\(c_i\\) contiene dentro de los límites del intervalo). Para las marcas de clases de los índices de glicemia medidos se calcularía entonces: \\[\\bar{X} = \\frac{3\\times80{,}5 + 10\\times92{,}5 + 4\\times104{,}5 + 4\\times116{,}5 + 2\\times128{,}5 + 2\\times140{,}5}{25}=103{,}54 \\text{ g dL}^{-1}\\] La mediana para datos agrupados se consigue siguiendo los siguientes pasos: Se calcula el índice de posición de la mediana como \\(n/2\\). Se localiza la clase que identifica el intervalo mediano (aquel que contiene la mediana) al buscar la primera clase cuya frecuencia absoluta acumulada sea igual o mayor a \\(n/2\\). La mediana es entonces: \\[M = L_i + \\left(\\frac{n/2 - F_{i-1}}{f_i}\\right)\\cdot a_i\\] donde \\(L_i\\) es el límite inferior del intervalo mediano, \\(F_{i-1}\\) es la frecuencia absoluta acumulada de la clase anterior, \\(f_i\\) es la frecuencia absoluta de la clase que contiene la mediana, y \\(a_i\\) es la amplitud de los intervalos. Para el caso de los índices de glicemia, se tiene que \\(n/2 = 25/2 =12{,}5\\). La primera clase con \\(F_i \\ge 12{,}5\\) es la segunda clase (\\(F_i = 13\\)). Inspeccionando la tabla de datos agrupados podemos calcular entonces: \\[M = 86{,}5 + \\left(\\frac{12{,}5 - 3}{10}\\right)\\cdot 12 = 97{,}9\\text{ g dL}^{-1}\\] La moda se calcula, para datos agrupados, siguiendo estos pasos: Se busca la clase modal (aquella que contiene la moda) determinando cuál de ellas tiene la mayor frecuencia absoluta. Se calcula la moda como: \\[Moda(\\{c_i\\}_{i=1}^k) = L_i + \\left(\\frac{f_i - f_{i-1}}{(f_i - f_{i-1}) + (f_i - f_{i+1})}\\right)\\cdot a_i\\] Para el caso de los índices de glicemia, la clase con la mayor frecuencia absoluta es la segunda clase, por lo que: \\[Moda(\\{c_i\\}_{i=1}^k) = 86{,}5 + \\left(\\frac{10 - 3}{(10 - 3) + (10 - 4)}\\right)\\cdot 12 = 92{,}96\\text{ g dL}^{-1}\\] Notamos que los resultados sobre los datos de frecuencia muestran que \\(Moda &lt; M &lt; \\bar{X}\\), lo cual nos indica que la distribución esta sesgada a la derecha. 5.2.2 Medidas de posición. Para los cuantiles de datos agrupados, se sigue un procedimiento similar al de datos sin agrupar, identificando primero las clases que contiene los cuantiles. Para ello, se sigue el procedimiento: Calcula \\(\\frac{n\\times i}{d}\\) donde \\(i\\) representa el \\(i\\)-ésimo cuantil, y \\(d\\) el número de partes en las que se desea dividir la distribución. Se busca en la tabla de datos agrupados la clase cuya frecuencia absoluta acumulada sea mayor o igual a \\(\\frac{n\\times i}{d}\\). Este será la clase cuantílica. Se calcula el cuantil como: \\[C_i = L_i + \\left(\\frac{f_i - f_{i-1}}{f_i}\\right)\\cdot a_i\\] Digamos que queremos calcular los cuartiles de los datos de índice de glicemia. En este caso, calculamos para \\(i = 1,2,3\\) los valores de \\(i\\times n / 4\\), los cuales son \\(6{,}25, 12{,}5,\\) y \\(18{,}75\\). Calculamos los cuartiles: para el primer cuartil, el resultado muestra que el cuartil se encuentra en la segunda clase, por lo que: \\[Q_1 = 86{,}5 + \\left(\\frac{10 - 3}{10}\\right)\\cdot 12 = 95{,}7\\text{ g dL}^{-1}\\] Para el segundo cuartil nos damos cuenta que el método arroja \\(Q_2 = Q_1\\), lo cual es un error (asegúrese de verificar este resultado, ¿por qué sucede esto?). La razón de esto es que la resolución de los datos no permite la estimación de los cuantiles dado el tamaño de la muestra (vea la discusión más adelante en la siguiente sección). En este caso, recordamos que \\(Q_2 = M\\), y usamos el valor de la mediana calculado anteriormente: \\[Q_2 = 97{,}9\\text{ g dL}^{-1}\\] Para el tercer cuartil, vemos que la clase que contiene el cuartil es la cuarta clase, por lo que: \\[Q_3 = 110{,}5 + \\left(\\frac{21 - 17}{21}\\right)\\cdot 12 = 112{,}79\\text{ g dL}^{-1}\\] De los resultados anteriores, podemos notar que la distancia entre el primer y segundo cuartil es un orden de magnitud menor que la distancia entre el segundo y tercer cuartil, indicando que las observaciones en la tercera parte de la distribución tienen una mayor dispersión, y que en la segunda parte de la distribución las observaciones se aglomeran. Esto refuerza la intuición obtenida antes con las medidas de tendencia central que la distribución está sesgada hacia la derecha. 5.2.3 Medidas de dispersión. El recorrido (o rango) y el rango intercuartílico (IQR) se calculan igual que antes para datos sin agrupar. Sin embargo, la definición de varianza la modificamos para usar las marcas de clase, y no las observaciones: \\[S^2 = \\frac{\\sum_{i=1}^k f_i(c_i - \\bar{X})^2}{n - 1}\\] A partir de esta, es posible calcular la desviación estándar y el coeficiente de variación tal como se definieron para datos sin agrupar. Se tiene que \\(R = 140 - 75 = 65\\) g dL\\({}^{-1}\\) y \\(IQR = 112{,}79 - 95{,}7 = 17{,}09\\) g dL\\({}^{-1}\\), los cuales nos indican que el 50% de las observaciones solo se encuentran ocupando aproximadamente un \\(4\\)% del dominio posible de las observaciones. La varianza es \\(S^2 = 311{,}04\\) (g dL\\({}^{-1}\\))\\({}^2\\), y \\(S = 17{,}64\\) g dL\\({}^{-1}\\) con \\(CV = 0{,}1703\\). Esto nos indica que la distribución de los datos parece no ser tan variable, pero esto puede ser engañoso ya que sabemos que la distribución esta sesgada. 5.2.4 Medidas de forma. Para el cálculo del coeficiente de asimetría y curtosis, se procede al igual que antes para datos sin agrupar, pero usamos las marcas de clases en lugar de las observaciones para realizar el cálculo. El coeficiente de asimetría se calcula como: \\[A = \\frac{\\sum_{i=1}^kf_i(c_i - \\bar{X})^3}{nS^3}\\] y la curtosis como: \\[A = \\frac{\\sum_{i=1}^kf_i(c_i - \\bar{X})^4}{nS^4} - 3\\] Para los índices de glicemia resumidos en la tabla de datos agrupados, obtenemos \\(A = 0{,}661\\) y \\(K = 2{,}32\\). Esto nos indica que la distribución es ligeramente sesgada hacia la derecha (como ya parecíamos intuir de las otras medidas) y leptocúrtica. Esto indica que el sesgo observado es resultado de observaciones atípicas. Media &lt;- sum(glicemia$f_i * glicemia$`Marca de Clase`) / 25 Std.Dev &lt;- sqrt(sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 2) / 24) tribble(~Media, ~`Desv. Est.`, ~Asimetria, ~Curtosis, Media, Std.Dev, sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 3) / (25 * Std.Dev ** 3), sum(glicemia$f_i * (glicemia$`Marca de Clase` - Media) ** 4) / (25 * Std.Dev ** 4) ) %&gt;% kbl() Media Desv. Est. Asimetria Curtosis 103.54 17.63633 0.6609389 2.32162 Ejercicio. En un estudio del síndrome de Down, se examinaron 180 niños afectados y la siguiente tabla da la distribución de frecuencias para el cociente intelectual (IQ) de los niños. Complete la tabla de datos agrupados, añadiendo las columnas que hagan falta, y determine las medidas de tendencia central, los cuartiles, deciles y percentiles, las medidas de dispersión y de forma. Discuta los resultados. tibble( Clase = c(1, 2, 3, 4, 5, 6, 7, 8, 9), `Límites de clase` = c(&quot;10.5 a 20.5&quot;, &quot;20.5 a 30.5&quot;, &quot;30.5 a 40.5&quot;, &quot;40.5 a 50.5&quot;, &quot;50.5 a 60.5&quot;, &quot;60.5 a 70.5&quot;, &quot;70.5 a 80.5&quot;, &quot;80.5 a 90.5&quot;, &quot;90.5 a 100.5&quot;), `Marca de clase` = c(15.5, 25.5, 35.5, 45.5, 55.5, 65.5, 75.5, 85.5, 95.5), `Frecuencia f_i` = c(4, 34, 0, 70, 43, 19, 7, 2, 1) ) %&gt;% kbl() Clase Límites de clase Marca de clase Frecuencia f_i 1 10.5 a 20.5 15.5 4 2 20.5 a 30.5 25.5 34 3 30.5 a 40.5 35.5 0 4 40.5 a 50.5 45.5 70 5 50.5 a 60.5 55.5 43 6 60.5 a 70.5 65.5 19 7 70.5 a 80.5 75.5 7 8 80.5 a 90.5 85.5 2 9 90.5 a 100.5 95.5 1 "],["distribuciones-de-probabilidad..html", "6 Distribuciones de Probabilidad.", " 6 Distribuciones de Probabilidad. Hasta este punto, hemos definido ya lo que es una variable aleatoria y cómo podemos usar esta variable para definir probabilidades en intervalos de números reales. Estas funciones se pueden definir para obtener información de las características de las variables aleatorias. Las funciones usadas son la función de distribución (o función de densidad probabilística) y la función de distribución (o función de distribución acumulada). "],["función-de-probabilidad..html", "6.1 Función de Probabilidad.", " 6.1 Función de Probabilidad. Esta función es la que no da información sobre la probabilidad de un evento cualquiera dentro del espacio probabilístico. El establecer la definición de la función de probabilidad se debe hacer para los casos en los que se tienen variables aleatorias discretas, y para los casos en los que se tienen variables aleatorias continuas. Caso discreto. Sea una v.a. \\(X\\) que toma valores \\(x_0, x_1, \\ldots\\) con probabilidades \\(p_0 = P(X = x_0), p_1 = P(X = x_1), \\ldots\\) (esta es una lista infinita, pero numerable, de probabilidades asignadas a los posibles valores de \\(X\\)). Se define entonces la función de probabilidad discreta como: \\[f(x) = \\begin{cases} P(X = x), &amp; \\text{ si }x = x_0, x_1, \\ldots \\\\ 0, &amp; \\text{ de otra forma.} \\end{cases}\\] Notamos que una función definida de esta forma, cumple con todos los axiomas de Kolmogorov y es, por lo tanto, una medida de probabilidad. De lo que se tiene: \\[\\begin{align} f(x) &amp;\\ge 0 \\\\ \\sum f(x) &amp;= 1 \\end{align}\\] De esta forma, podemos definir la probabilidad de un evento cualquiera como: \\[P(X \\in A) = \\sum_{x \\in A} f(x)\\] Esto es así ya que, como vimos, \\(A\\) estaría formado por la unión de eventos disjuntos, cuya probabilidad es la sumatoria de las probabilidades individuales. Ejemplo. Sea \\(X\\) una v.a. que toma los valores \\(1,2,3\\) con probabilidades \\(0{,}3, 0{,}5, 0{,}2\\), respectivamente. Definimos la función de probabilidad como: \\[f(x) = \\begin{cases} 0{,}3, &amp; \\text{ si }x = 1 \\\\ 0{,}5, &amp; \\text{ si }x = 2 \\\\ 0{,}2, &amp; \\text{ si }x = 3 \\\\ 0, &amp; \\text{ de otra forma.} \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = 1:3, y=c(.3, .5, .2))) + geom_segment(aes(x = 1:3, y = rep(0, 3), xend = 1:3, yend = c(.3, .5, .2)), linewidth = 2, linetype = &quot;dashed&quot;, colour = &quot;gray75&quot;) + geom_point(size = 3) + geom_hline(yintercept = 0, linewidth = 3) + geom_point(aes(y = rep(0, 3)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;f(x)&quot;) + xlab(&quot;x&quot;) + theme_light(base_size = 14) + theme(panel.grid = element_blank()) a partir de la cual podemos calcular la probabilidad de cualquier evento, como por ejemplo \\(P(X\\ge2) = P(X=2) + P(X=3) + P(X=4) + \\ldots = 0{,}5 + 0{,}2 + 0 + \\ldots = 0{,}7\\) o \\(P(\\vert X\\vert = 1) = P(X = 1) + P(X = -1) = 0{,}3 + 0 = 0{,}3\\). En el ejemplo anterior vemos que no hubo necesidad de definir un experimento aleatorio para construir una función de probabilidad. Esta libertad nos permite definir arbitrariamente funciones de probabilidad en esquemas genéricos, lo único que se necesita es que obedezcan los axiomas de Kolmogorov. Ejercicio. Una muestra de \\(7\\) semillas contiene \\(2\\) infectadas con una enfermedad. Un agrónomo compra \\(3\\) de las semillas al azar. Si \\(x\\) es el número de unidades defectuosas que compra el agrónomo, calcule la distribución de probabilidad de \\(X\\). Exprese los resultados de forma gráfica como un histograma de probabilidad. Ahora definimos la función de probabilidad para el caso contínuo. Caso continuo. Sea \\(X\\) una v.a. continua. Decimos que \\(f(x)\\) es la función de densidad de la variable \\(X\\) en el intervalo \\([a, b]\\in\\mathbb{R}\\) si se cumple: \\[P(X \\in[a,b]) = \\int_a^b f(x)dx\\] donde \\(f(x)\\) es una función no negativa e integrable en el intervalo \\([a,b]\\) Bajo esta definición, es claro que se cumplen los criterios de Kolmogorov: \\[\\begin{align} f(x) &amp;\\ge 0 \\space \\forall \\space x \\in \\mathbb{R} \\\\ \\int_{-\\infty}^{\\infty} f(x) &amp;= 1 \\end{align}\\] Ejemplo. Sea \\(X\\) una v.a. continua con función de probabilidad definida como: \\[f(x) = \\begin{cases} 3x^2/2, &amp; \\text{ si } -1 &lt; x &lt; 1 \\\\ 0, &amp; \\text{ de otra forma} \\end{cases}\\] cuyo gráfico se muestra a continuación. x &lt;- seq(-1, 1, by=.1) ggplot(NULL, aes(x = x)) + geom_line(aes(y=1.5 * x ** 2), linewidth = 2) + geom_line(aes(x = c(-1.5, -1), y=c(0, 0)), linewidth = 2) + geom_line(aes(x = c(1, 1.5), y=c(0, 0)), linewidth = 2) + geom_point(aes(x=c(-1, 1), y = rep(0, 2)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;f(x)&quot;) + xlab(&quot;x&quot;) + theme_light() a partir de la cual podemos calcular la probabilidad de cualquier evento, como por ejemplo: \\[\\begin{aligned} P(X \\le 1/3) &amp;= \\int_{-\\infty}^{1/3}f(x)dx \\\\ &amp;= \\int_{-\\infty}^{-1}0dx + \\int_{-1}^{1/3}\\frac{3}{2}x^2dx \\\\ &amp;= 0 + \\frac{(1/3)^3}{2} - \\frac{(-1)^3}{2} \\\\ &amp;= \\frac{1}{54} + \\frac{1}{2} \\\\ &amp;= \\frac{14}{27} \\end{aligned}\\] Al igual que antes, no hubo necesidad de definir un experimento aleatorio para construir una función de probabilidad continua. Ejercicio. El tiempo que pasa, en segundos, para que un murciélago detecte entre árboles sucesivos a una presa que se mueve a una velocidad dada es una variable aleatoria continua con una función de distribución acumulativa: \\[F(x) = \\begin{cases} 0, &amp; x &lt; 0, \\\\ 1 - e^{-8x}, &amp; x ≥ 0 \\end{cases}\\] Calcule la probabilidad de que el tiempo que pase para que el murciélago detecte entre árboles sucesivos a las presas que exceden una velocidad de escape sea menor de 12 minutos. "],["función-de-distribución..html", "6.2 Función de distribución.", " 6.2 Función de distribución. Sea \\(X\\) una variable aleatoria cualquiera, la función de distribución, denotada como \\(F(x) : \\mathbb{R} \\rightarrow \\mathbb{R}\\) (lo cual se lee como: \\(F(x)\\) toma un número real y lo transformar en otro número real) se define como la probabilidad: \\[F(x) = P(X \\le x)\\] Vemos entonces porque la llamamos también función de probabilidad acumulada, ya que \\(F(x)\\) denota la probabilidad acumulada hasta el valor observado \\(x\\). También notamos que, como las probabilidades son todas mayores o iguales a cero, y que la probabilidad del espacio muestral en su totalidad es \\(1\\), la función de distribución se define entre \\(0\\) y \\(1\\). Al igual que antes, se hace necesario distinguir entre la función de distribución en el caso discreto y en el caso continuo. Caso discreto. Si \\(X\\) es una v.a. discreta con función de distribución \\(f(x)\\), entonces se define: \\[F(x) = \\sum_{t \\le x} f(t)\\] Ejemplo. Para el ejemplo anterior dado para la función de probabilidad de una v.a. discreta, podemos construir la función de distribución considerando todos los intervalos donde la probabilidad se mantenga contante. De esta forma obtenemos: \\[F(x) = \\begin{cases} 0, &amp; x &lt; 1 \\\\ 0{,}3, &amp; 1 \\le x &lt; 2 \\\\ 0{,}8, &amp; 2 \\le x &lt; 3 \\\\ 1, &amp; x \\ge 3 \\\\ \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = 1:3, y=c(.3, .8, 1))) + geom_point(size = 3) + geom_line(aes(x = c(0, 1), y = c(0, 0)), linewidth = 2) + geom_line(aes(x = c(1, 2), y = c(.3, .3)), linewidth = 2) + geom_line(aes(x = c(2, 3), y = c(.8, .8)), linewidth = 2) + geom_line(aes(x = c(3, 4), y = c(1, 1)), linewidth = 2) + geom_point(aes(x = c(1, 2, 3), y = c(0, .3, .8)), size = 3, shape = 21, fill = &quot;white&quot;) + ylab(&quot;F(x)&quot;) + xlab(&quot;x&quot;) + theme_light() Caso continuo. Si \\(X\\) es una v.a. continua con función de distribución \\(f(x)\\), entonces se define: \\[F(x) = \\int_{-\\infty}^x f(t)dt\\] Ejemplo. Se tiene una variable aleatoria \\(X\\) con función de probabilidad dada por: \\(f(x) = \\begin{cases}\\vert x\\vert, &amp; -1 &lt; x &lt; 1 \\\\ 0 &amp; \\text{ de otra forma}\\end{cases}\\). La obtención de la función de distribución se obtiene aplicando la definición en cada intervalo en los que la definición de \\(f(x)\\) cambia. Empezando con el intervalo de \\((-\\infty, -1)\\), se tiene: \\[F(X) = \\int_{-\\infty}^-1 0dx = 0\\] Luego, en el intervalo \\([-1,0)\\) se tiene: \\[F(X) = \\int_{-1}^x -xdx = (1-x^2)/2\\] y así seguimos hasta obtener \\(F(x)\\) en todos los reales: \\[F(x) = \\begin{cases} 0, &amp;x \\le -1 \\\\ (1-x^2)/2, &amp;-1 \\le x &lt; 0 \\\\ (1+x^2)/2, &amp;0 &lt; x \\le 1 \\\\ 1, &amp;x \\ge 1 \\\\ \\end{cases}\\] cuyo gráfico se muestra a continuación. ggplot(NULL, aes(x = c(-1.5, -1), y=c(0, 0))) + geom_line(linewidth = 2) + geom_line(aes(x = seq(-1, 0, by = .1), y = (1 - seq(-1, 0, by = .1) ** 2) / 2), linewidth = 2) + geom_line(aes(x = seq(0, 1, by = .1), y = (1 + seq(0, 1, by = .1) ** 2) / 2), linewidth = 2) + geom_line(aes(x = c(1, 1.5), y = c(1, 1)), linewidth = 2) + ylab(&quot;F(x)&quot;) + xlab(&quot;x&quot;) + theme_light() 6.2.1 Propiedades de la función de distribución. La función de distribución resulta ser muy importante desde el punto de vista matemático, pues siempre puede definirse dicha función para cualquier variable aleatoria y a través de ella quedan representadas todas las propiedades de la variable aleatoria. Cualquier función que cumpla las siguientes propiedades es una función de distribución, sea que tenga o no una variable aleatoria asociada. \\(F(x)\\) está acotada por arriba por \\(1\\), lo cual se puede escribir como \\[\\lim_{x\\rightarrow\\infty} F(x) = 1\\]. Esto es así dado que la probabilidad de todo el espacio muestral es \\(1\\). \\(F(x)\\) está acotada por abajo por \\(0\\), lo cual se puede escribir como \\[\\lim_{x\\rightarrow-\\infty} F(x) = 0\\]. Esto es así dado que las probabilidades son no negativas. \\(F(x)\\) es monótona no decreciente, esto es, si \\(x_1 \\le x_2\\), entonces \\(F(x_1) \\le F(x_2)\\). \\(F(x)\\) es continua por la derecha, lo cual se puede escribir como \\[F(x) = \\lim_{x\\rightarrow x_0^+} F(x)\\]. "],["cálculos-con-funciones-de-probabilidades..html", "6.3 Cálculos con funciones de probabilidades.", " 6.3 Cálculos con funciones de probabilidades. "],["distribuciones-de-probabilidad-de-variables-discretas..html", "7 Distribuciones de probabilidad de variables discretas.", " 7 Distribuciones de probabilidad de variables discretas. "],["distribuciones-de-probabilidad-de-variables-continuas..html", "8 Distribuciones de probabilidad de variables continuas. ", " 8 Distribuciones de probabilidad de variables continuas. "],["distribución-normal..html", "8.1 Distribución Normal.", " 8.1 Distribución Normal. La distribución normal (o gaussiana) es de las distribuciones más importantes que estudiaremos. Esta la usaremos con frecuencia más adelante cuando realicemos inferencias a partir de observaciones realizadas en un experimento. Aparecerá primero en el teorema del límite central (capítulo Teoría de Muestreo.), que es uno de los teoremas más importantes que tiene aplicaciones directas en la práctica. Decimos que una variable aleatoria \\(X\\) tiene una distribución normal si su función de densidad viene dada por: \\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty,\\] donde \\(\\mu, \\sigma^2 \\in \\mathbb{R}\\), con \\(\\sigma^2 &gt; 0\\), son los parámetros de la distribución. Si la variable \\(X\\) se distribuye como normal se escribe: \\[X \\sim N(\\mu, \\sigma^2)\\] La grafica de la función de densidad normal tiene forma de campana, siendo simétrica con respecto a la vertical que pasa por la media \\(\\mu\\), la cual es el centro de la campana. Siendo \\(\\sigma\\) (raíz cuadrada de la varianza \\(\\sigma^2\\)) es la distancia del centro a cualquiera de los puntos de inflexión de la curva (como se muestra en la figura 8.1). Figure 8.1: Función de densidad de una variable aleatoria normal N(\\(\\mu, \\sigma^2\\)). Esta información se resume como: \\[\\begin{aligned} &amp; E(X) = \\mu \\\\ &amp; Var(X) = \\sigma^2 \\end{aligned}\\] La función de distribución viene dada por la integral: \\[F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}dy\\] la cual no tiene primitiva analítica asociada y debe resolverse por métodos numéricos. Es posible obtener valores de probabilidad acumulada en R usando el comando pnorm, el cual da \\(F(x) = P(X \\le x)\\), como se muestra en la figura siguiente. Figure 8.2: Función de distribución acumulada de una variable aleatoria normal N(\\(\\mu, \\sigma^2\\)). Por ejemplo, supongamos que se tiene la variable aleatoria \\(X\\) que representa la longitud del ala de abejas de cierta granja de apicultores. Se sabe, de estudios anteriores, que la longitud media es de \\(1{,}5 \\pm 0{,}73\\) cm. Los mismos estudios preliminares han mostrado que \\(X\\) sigue una distribución normal, por lo que se escribe: \\[X \\sim N(\\mu = 1{,}5, \\sigma^2 = 0.533)\\] Podemos obtener la probabilidad de que una abeja tenga una longitud del ala menor a \\(1\\) cm, \\(P(X \\le 1\\text{ cm})\\) como pnorm(1, 1.5, 0.73) (figura 8.3, a la izquierda). Si queremos la probabilidad de aquellas con una longitud del ala mayor a \\(3\\) cm, \\(P(X \\ge 3\\text{ cm}) = 1 - P(X \\le 3\\text{ cm})\\), que se calcula en R como 1 - pnorm(3, 1.5, 0.73) (figura 8.3, a la derecha). Figure 8.3: Función de densidad de una variable aleatoria normal N(\\(\\mu = 1{,}5, \\sigma^2 = 0{,}532\\)), donde se muestra de forma grafica la probabilidad acumulada \\(P(X \\le 1)\\) (a la izquierda) y \\(P(X \\ge 3)\\) (a la derecha). Notamos, de las figuras del ejemplo, que la probabilidad acumulada se puede entender como el área debajo de la función de densidad para un valor de \\(X\\) observado o menor, y que para encontrar valores acumulados hacia arriba, solo necesitamos usar \\(1 - F(X)\\). Ejercicio. Un investigador informa que unos ratones a los que primero se les restringen drásticamente sus dietas y después se les enriquecen con vitaminas y proteínas vivirán un promedio de \\(40\\) meses. Si suponemos que la vida de tales ratones se distribuye normalmente, con una desviación estándar de \\(6{,}3\\) meses, calcule la probabilidad de que un ratón determinado viva a) más de \\(32\\) meses; b) menos de \\(28\\) meses; c) entre \\(37\\) y \\(49\\) meses. Un caso particular de esta distribución, que es muy útil, es cuando \\(\\mu = 0\\) y \\(\\sigma^2 = 1\\), la cual da lugar a la distribución normal estándar, cuya función de densidad queda: \\[f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}, \\quad -\\infty &lt; x &lt; \\infty,\\] la cual también se denota como \\(\\phi(x)\\). Esto es importante porque significa que siempre es posible transformar una v.a. normal en una estándar por una simple operación: \\[Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\] que se conoce como estandarización. La importancia de este procedimiento es que el cálculo de probabilidades de una variable aleatoria normal se puede reducir al cálculo de probabilidad de una variable aleatoria de distribución normal estándar. Esto es fácil de ver ya que: \\[\\begin{aligned} P(a &lt; X &lt; b) &amp;= P(a - \\mu &lt; X - \\mu &lt; b - \\mu) \\\\ &amp;= P\\left(\\frac{a - \\mu}{\\sigma} &lt; \\frac{C - \\mu}{\\sigma} &lt; \\frac{b - \\mu}{\\sigma}\\right) \\\\ &amp;= P\\left(\\frac{a - \\mu}{\\sigma} &lt; Z &lt; \\frac{b - \\mu}{\\sigma}\\right) \\end{aligned}\\] Se puede demostrar que si \\(X\\) se distribuye como una normal estándar, entonces la variable \\(-X\\) también tiene distribución normal estándar, y: \\[\\Phi(-x) = 1 - \\Phi(x)\\] Definimos los cuantiles de la distribución normal estándar \\(z_\\alpha\\) para cada valor de \\(\\alpha\\) en el intervalo \\((0,1)\\) como aquel para el cual: \\[\\Phi(z_{\\alpha}) = 1 - \\alpha\\] Algunos cuantiles importantes que vale la pena recordar, y que usaremos frecuentemente más adelante son: \\(z_{0{,}9} = 1{,}28\\), \\(z_{0{,}95} = 1{,}64\\), \\(z_{0{,}975} = 1{,}96\\) y \\(z_{0{,}99} = 2{,}33\\). Ahora, mencioanmos una proposición muy útil sobre la suma de dos variables aleatorias normales. Sean \\(X_1\\) y \\(X_2\\) dos variables aleatorias independientes con distribución \\(N(\\mu_1, \\sigma_1^2)\\) y \\(N(\\mu_2, \\sigma_2^2)\\), entonces: \\[X_1 + X_2 \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\] Veamos un ejemplo. Por ejemplo, digamos que un investigador tiene una población de ciertas bacterias en un cultivo puro que se esta estudiando por sus capacidades de producir cierta proteína transmembrana de interés, que sirve como transportador de un metabolito que se desea degradar. Se sabe que esta se produce en este cultivo con una densidad de \\(35 \\pm 4{,}3\\) unidades por centímetro cuadrado por célula. Sin embargo, en un accidente, el investigador mezclo dos cultivos con capacidades distintas de producir la proteína transmembrana. Este segundo cultivo tiene una capacidad menor de producir la proteína que funciona como transportador, haciendola menos efectiva en metabolizar el metabolito, con una densidad de solo \\(11 \\pm 7{,}1\\) unidades por centímetro cuadrado por célula. Si suponemos que la v. a. densidad de la proteína por centímetro cuadrado por célula se distribuye como una normal, entonces \\(X_{\\text{Cultivo 1}}\\sim N(35, 18{,}49)\\) y \\(X_{\\text{Cultivo 2}}\\sim N(11, 50{,}41)\\) en ambos cultivos aislados; y luego del accidente, \\(X_{\\text{Cultivo Mezclado}}\\sim N(46, 68{,}9)\\). "],["distribución-ji-cuadrada..html", "8.2 Distribución Ji-Cuadrada.", " 8.2 Distribución Ji-Cuadrada. Se dice que una variable aleatoria continúa \\(X\\) sigue una distribución Ji-Cuadrada con \\(n\\) grados de libertad (\\(n&gt;0\\)), si su función de densidad viene dada por: \\[f(x) = \\begin{cases} \\frac{1}{\\gamma(n/2)}\\left(\\frac{1}{2}\\right)^{n/2}x^{n/2 - 1}e^{-x/2} &amp;\\text{ si }x &gt; 0 \\\\ 0 &amp;\\text{ en otro caso}\\end{cases}\\] Esta función se distribuye en el intervalo \\((0, \\infty)\\) y su único parámetro son los grados de libertad \\(n\\) que puede ser cualquier valor positivo, aunque la mayoría de las veces tomará solo valores enteros positivos. En la figura 8.4 se muestra esta distribución para \\(n = 1, 2, 5\\), y \\(8\\): a partir de \\(n=3\\) aparece un pico en la función, el cual se desplaza a valores mayores a medida que \\(n\\) aumenta. Figure 8.4: Función de densidad de una variable aleatoria Ji-Cuadrada \\(\\chi^2(n)\\). Si \\(X\\) sigue una distribución Ji-Cuadrada, escribiremos \\[X \\sim \\chi^2(n)\\] Su función de distribución viene dada por: \\[F(x) = \\int_0^{x} \\frac{1}{\\gamma(n/2)} \\left(\\frac{1}{2}\\right)^{n/2}u^{n/2 - 1}e^{-u/2}du\\] cuyo gráfico se muestra a comtinuación para una v. a. \\(X \\sim \\chi^2(n = 8)\\): Figure 8.5: Función de distribución acumulada de una variable aleatoria \\(\\chi^2(n = 8)\\). Es posible obtener valores de probabilidad acumulada en R usando el comando pchisq, el cual da \\(F(x) = P(X \\le x)\\). Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = n \\\\ &amp; Var(X) = 2n \\end{aligned}\\] Antes de ver un ejemplo de calculo de porbabilidades a partir de la función de distribución, es bueno conocer los siguientes resultados. La distribución Ji-cuadrada se puede obtener como resultado de elevar al cuadrado una variable normal estándar. Si \\(X \\sim N (0, 1)\\), entonces: \\[X^2 \\sim \\chi^2(1)\\] Este resultado, junto con la siguiente proposición, nos permitirán entender la distribución de v. a. con distribución Ji-Cuadrada. Sea \\(X\\) y \\(Y\\) dos v. a. independientes con distribución \\(\\chi^2(n)\\) y \\(\\chi^2(m)\\), respectivamente. Entonces: \\[X + Y \\sim \\chi^2(n + m)\\] Este resultado se puede extender a la suma de \\(n\\) variables aleatorias independientes distribuidas como Ji-cuadrado. Esto nos dice que podemos entender cualquier variable aleatoria que sigue una distribución Ji-Cuadrado como una suma de v. a. con la misma distribución pero cada una con un solo grado de libertad, cada una de las cuales se entiende como una v. a. normal estándar al cuadrado. Se nos permite obtener el siguiente resultado que utilizaremos más adelante cuando hablemos de inferencia estadística, y que es tan importante para realizar inferencia sobre la varianza. Sean \\(X_1, \\ldots, X_n\\) variables aleatorias independientes, cada una de ellas con distribución \\(N(\\mu, \\sigma^2)\\). Entonces: \\[\\frac{(n - 1)S^2}{\\sigma^2} \\sim \\chi^2(n - 1)\\] donde \\(S^2 = \\frac{1}{n - 1}\\sum_{i=1}^n(X_i - \\bar{X})^2\\) y \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\). Veamos un ejemplo. Por ejemplo, se sabe que en madres que no consumieron cocaína al nacer, el peso de los bebés nacidos tiene una desviación entándar de \\(\\sigma = 696\\) g. En un estudio de los efectos que tiene el consumo de cocaína sobre los bebés durante el embarazo, se recolectaron datos de \\(n = 190\\) madres consumidoras de cocaína. Por la proposión anterior, sabemos que la variabilidad en el peso de los bebés con respecto al valor conocido se distribuye como una \\(\\chi^2(n - 1)\\). Podemos calcular la probabilidad de que la variabilidad sea la mitad del valor conocido de \\(696\\) g, calculando la variable aleatoria \\((190 - 1) \\frac{(1/2)\\sigma^2}{\\sigma^2} = 189/2 = 94{,}5\\), cuya probabilidad se puede calcular en R como pchisq(94.5, 189) que arroja un valor de 0 (se muestra en la gráfica de la izquierda de la figura 8.6). Si la variabilidad es aproximadamente 8% menor al valor conocido, entonces \\((190 - 1) \\frac{0{,}92\\sigma^2}{\\sigma^2} = 173{,}88\\), entonces se calcula pchisq(173.8, 189) que arroja 0.2222 (que se muestra en la gráfica de la derecha de la figura 8.6) Figure 8.6: Función de densidad de una variable aleatoria \\(\\chi^2(n = 189)\\), donde se muestra de forma grafica la probabilidad acumulada \\(P(X \\le 94{,}5)\\) (a la izquierda) y \\(P(X \\le 173{,}88)\\) (a la derecha). "],["distribución-t-student..html", "8.3 Distribución \\(t\\)-Student.", " 8.3 Distribución \\(t\\)-Student. Se dice que un variable aleatoria continua \\(X\\) sigue una distribución \\(t\\)-Student con \\(n\\) grados de libertad (\\(n&gt;0\\)), si su función de densidad viene dada por: \\[f(x) = \\frac{\\Gamma(\\frac{n + 1}{2})}{\\sqrt{n\\pi}\\Gamma(n/2)}\\left(1 + \\frac{x^2}{n}\\right)^{-(n+1)/2}, \\quad -\\infty &lt; x &lt; \\infty\\] y se escribe: \\[X \\sim t(n)\\] en donde \\(n\\) es un número real positivo, aunque tomaremos principalmente el caso cuando \\(n\\) es entero positivo. Figure 8.7: Función de densidad de una variable aleatoria \\(t\\)-Student \\(t(n)\\), para \\(n = 1\\) (línea sólida), \\(n = 4\\) (línea quebrada) y \\(n = 15\\) (línea punteada). En rojo se muestra la distribución normal estándar. La función de densidad es de campana como la normal, pero con colas más pesadas que esta última, esto es, la probabilidad de obtener una observación extrema es mayor que la probabilidad de esa misma observación proviniendo de una distribución normal. A medida que aumentan los grados de libertad, la amplitud de las colas disminuye y la distribución se aproxima a una normal, y en el límite cuando \\(n\\rightarrow\\infty\\), ambas densidades coinciden. La función de distribución tampoco tiene una expresión sencilla y se escribe como: \\[F(x) = \\int_{-\\infty}^\\infty \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi}\\Gamma(n/2)} \\left(1 + \\frac{u^2}{n}\\right)^{-(n+1)/2} du\\] cuyo gráfico se muestra a continuación para una \\(t\\)-Student de \\(4\\) grados de libertad. Figure 8.8: Función de distribución acumulada de una variable aleatoria \\(t(n = 4)\\). Es posible obtener valores de probabilidad acumulada en R usando el comando pt, el cual da \\(F(x) = P(X \\le x)\\) (vea el ejemplo al final de esta subsección). Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = 0, \\quad n &gt; 1 \\\\ &amp; Var(X) = \\frac{n}{n - 2}, \\quad n &gt; 2 \\end{aligned}\\] Esta distribución resulta cuando se estudian ciertas operaciones entre variables aleatorias. Un resultado que usaremos seguido en inferencia estadística es el siguiente: Si \\(X \\sim N(0,1)\\) y \\(Y \\sim \\chi^2(n)\\) son dos variables aleatorias independientes, entonces: \\[\\frac{X}{\\sqrt{Y/n}} \\sim t(n)\\] También se puede llegar al siguiente resultado: Sean \\(X_1, \\ldots, X_n\\) v. a. independientes, cada una de ellas con distribución normal \\(N(\\mu, \\sigma^2)\\). Entonces: \\[\\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t(n - 1)\\] donde \\(S^2 = \\frac{1}{n - 1}\\sum_{i=1}^n(X_i - \\bar{X})^2\\) y \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\). Veamos un ejemplo. Ejemplo. Un programa que busca probar la eficacia de productos microbiológicos por su capacidad de degradar contaminantes derivados del petroleo (de tal forma que se puedan limpiar los ecosistemas que sufren el efecto del derrames amplios en las costas) es puesto en marcha. Investigaciones iniciales muestran que el producto es capaz de disminuir la cantidad de derivados del petroleo en un promedio de \\(10\\) ppm. Las pruebas del producto, basadas en una muestra de \\(5\\) recolecciones al azar de agua contaminadas y se les aplica el producto y se encuentra una variabilidad en la apcidad de degradación de \\(5{,}2\\) ppm. La proposición anterior entoncs muestra que la variable aleatoria \\[\\frac{\\bar{X} - 10}{5{,}2/\\sqrt{5}} \\sim t(4)\\] Por lo que podemos calcular la probabilidad de que al aplicar el producto se reduzca la contamianción por petroleo a \\(5\\) ppm, introduciendo este valor en la ecuación anterior, obteniendose \\((5 - 10)/5{,}2/\\sqrt{5} = -0{,}4300\\). Luego usamos la función pt como en pt(-0.43, 4) que arroja un valor de 0.3447 (como se ve en la figura 8.9, a la izquierda). Si queremos la probabilidad de que la contaminación solo descienda hasta \\(20\\) ppm como mínimo, entonces se calcula \\((20 - 10)/5{,}2/\\sqrt{5} = 0{,}86\\), cuya probabilidad se puede calcular como 1 - pt(0.86, 4), cuyo valor es 0.7809 (como se ve en la figura 8.9, a la derecha). Figure 8.9: Función de distribución acumulada de una variable aleatoria \\(t(n = 4)\\). "],["distribución-f..html", "8.4 Distribución \\(F\\).", " 8.4 Distribución \\(F\\). Se dice que la variable aleatoria continua \\(X\\) tiene una distribución \\(F\\) de Fisher-Snedecor con \\(a &gt; 0\\) y \\(b &gt; 0\\) grados de libertad si su función de densidad viene dada por: \\[f(x) = \\begin{cases} \\frac{\\Gamma(\\frac{a + b}{2})}{\\Gamma(\\frac{a}{2})\\Gamma(\\frac{b}{2})}\\left(\\frac{a}{b}\\right)^{a/2}x^{a/2 - 1}\\left(1 + \\frac{a}{b}x\\right)^{-(a + b)/2} &amp; \\text{si }x &gt; 0 \\\\ 0 &amp; \\text{de otra forma.} \\end{cases}\\] y se escribe \\[X \\sim F(a, b)\\] La gráfica de \\(f(x)\\) se muestra a continuación. Figure 8.10: Función de densidad de una variable aleatoria \\(F(a, b)\\), para difernetes combinaciones de los parámetros. Es posible obtener valores de probabilidad acumulada en R usando el comando pf, el cual da \\(F(x) = P(X \\le x)\\), que no tiene una expresión sencilla reducida. Para esta distribución, es posible demostrar que: \\[\\begin{aligned} &amp; E(X) = \\frac{b}{b - 2}, \\quad b &gt; 2 \\\\ &amp; Var(X) = \\frac{2b^2(a + b - 2)}{a(b - 2)^2(b - 4)}, \\quad n &gt; 4 \\end{aligned}\\] La distribución \\(F(a, b)\\) aparece como resultado de realizar operciones entre variables aleatorias con distribución Ju-Cuadrada, como se muestra en la siguiente proposición. Sean \\(X\\) y \\(Y\\) dos variables aleatorias independientes con distribución \\(\\chi^2(a)\\) y \\(\\chi^2(b)\\), respectivamente. Entonces: \\[\\frac{X/a}{Y/b} \\sim F(a, b)\\] "],["ejercicios..html", "8.5 Ejercicios.", " 8.5 Ejercicios. Se ha realizado un estudio sobre la velocidad en vuelo de diversas especies de pájaros. El propósito era comparar las velocidades de dos especies de aves relacioandas entre sí, pero que se ha visto tienen tiempos de vuelo similares, pero una de ellas es más variable que la otra dada la longitud mas variable de la envergadura de las alas. Estudios previos muestran que la velocidad de vuelo de la especie con tiempos de vuelo menos variable, volando con el viento de costado con una velocidad de viento de \\(5\\) a \\(8\\) millas h\\({}^{-1}\\), en promedio, es \\(26{,}05 \\pm 3{,}20\\) millas h\\({}^{-1}\\). Si se tomará una muestra de \\(12\\) individuos de esta población, determine la probabilidad de que la variabilidad a) sea mayor a \\(6\\) milla h\\({}^{-1}\\), b) sea menor a \\(1\\) milla h\\({}^{-1}\\), c) este entre \\(2{,}5\\) y \\(4{,}3\\) millas h\\({}^{-1}\\). Un proceso industrial nuevo quiere lograr conseguir producir biocombustible a base de microalgas. Para hacer rentable esto, se requiere conseguir una producción neta de biomasa de microalgas de \\(5{,}6\\) g L\\({}^{-1}\\). a) Determine la probabilidad de lograr alcanzar esta cantidad de biomasa o más, si el proceso actual genera en promedio solo \\(2{,}5 \\pm 0{,}89\\) g L\\({}^{-1}\\), asumiendo que la distribución de bioamsa es una normal. Un cambio en las condiciones de cultivo han logrado aumentar el rendimiento del proceso a \\(4{,}3 \\pm 0{,}78\\) g L\\({}^{-1}\\). Determine la probabilidad de b) alcanzar, al menos, el valor de \\(5{,}6\\) g L\\({}^{-1}\\) bajo este nuevo escenario, c) de lograr una producción de biomasa entre \\(4{,}0\\) y \\(5{,}6\\) g L\\({}^{-1}\\). Se han desarrollado aortas artificiales a base de celulosa que requieren tengan un diametro de interno de \\(20\\) mm para que sea efectiva. A partir de una muestra de \\(5\\) aortas producidas se encuentra que el diametro medio es de \\(18{,}2 \\pm 0{,}9\\) mm. ¿Cuál es la probabilidad de que a) el diámetro sea de, al menos, el valor requerido?, b) el diámetro este entre \\(19\\) y \\(23\\) mm?, c) el diámetro se como mínimo de \\(17\\) mm? La productividad de un proceso de cultivo de papás hidropónico en una planta de producción nueva en Valencia es de \\(27 \\pm 5\\) kg semanales. Calcule la probabilidad de a) que la productividad sea mayor a \\(30\\) kg semanales, b) que la productividad este entre los \\(25\\) y \\(30\\) kg semanales, c) que la producción sea menor a los \\(15\\) kg. Asuma que la productividad semanal sigue una distribución normal. En estudios de herencia, es posible saber si un carácter se hereda de forma autosómica o sexual, simplemente estudiando la progenie de un primer entrecruzamiento, que denominamos F1, verificando si la proporción de individuos que presentan los caracteres siguen las proporciones mendelianas \\(3:1\\). Para ello, se hace uso de la variable aleatoria: \\[X^2 = \\sum_{i=1}^k \\frac{d_i^2}{e_i} = \\sum_{i=1}^k \\frac{(o_i - e_i)^2}{e_i}\\] la cual se distribuye como una chi-cuadrado con \\(k-1\\) grados de libertad, donde \\(k\\) es el número de clases (o fenotipos) expresados por el gen considerado, y \\(e_i\\) es el valor esperado. En un experimento clásico, se contabilizaron el número de semillas rugosas y lisas obtenidas en F1 luego de un entrecruzamiento de semillas lisas y rugosas. El número de semillas lisas contabilizado fue de \\(384\\) semillas lisas, y \\(118\\) semillas rugosas. ¿Cuál es la probabilidad de que el caracter se transmita de forma autosómica? "],["inferencia-estadística..html", "9 Inferencia Estadística.", " 9 Inferencia Estadística. La inferencia es la parte de la estadística que se encarga de formalizar el proceso de estimación y contrastes de hipótesis. Su problema fundamental consiste en poder derivar declaraciones acerca de un fenómeno natural de interés a partir de observaciones realizadas del fenómeno. Las declaraciones de las que se hablan, son declaraciones en un sentido estadístico: esto es, las declaraciones se establecen con cierto grado de veracidad. No son verdades universales, sino que están sujetas a errores. El problema de la inferencia estadística es cuantificar que tan seguro estamos sobre esas declaraciones. La razón de que las declaraciones estén sujetas a variabilidad tiene que ver con las observaciones que realizamos del fenómeno. Las observaciones se usan para realizar inferencias acerca de las características o propiedades particulares del fenómeno en estudio. Estas observaciones no son perfectas y están limitadas a los recursos que posee el investigador para llevarlas a cabo: No son perfectas ya que cualquier medición está sujeta a que tan preciso es el instrumento con el que medimos (eso incluye nuestros sentidos). Además, las observaciones contienen una variabilidad inherente que es debida solo al azar. Esto hace que se tenga cierta incertidumbre al realizar mediciones, que son desviaciones aleatorias del valor real de lo que se está midiendo. Son limitadas dado que no se tiene siempre el dinero, el tiempo, o la energía para recolectar toda la información disponible. Esto hace que no se disponga siempre de toda la información que pueda ayudarnos a estudiar un fenómeno particular, sino que solo un subconjunto de esa información. En conclusión, las observaciones realizadas del fenómeno en cuestión contienen variación aleatoria que hacen imposible el observar directamente la característica o propiedad que se está estudiando, y dado que las declaraciones derivan de estas observaciones, se necesita cuantificar esta variabilidad/incertidumbre, necesitándose así modelos estocásticos para poder tratar con esta variación. Es por ello que se hace necesario de modelos estadísticos con los cuales manejar los datos. Estos modelos se originan de la matemática deductiva (aquellos que comienzan con teorías generales y que, por argumentos lógicos, se llega a conclusiones específicas), pero no necesariamente son los correctos, y esto hace que estén sujetos a incertidumbre. Obtener información (observaciones) no nos permite decir que modelo es el correcto. Simplemente no sabemos: al realizar inferencia y obtener declaraciones de estas, asumimos un modelo correcto y analizamos los datos bajo esta premisa, y toleramos/soportamos la posibilidad de caer en un error debido a una mala elección del modelo. Por ejemplo, al hablar de inferencia en los próximos capítulos, estaremos suponiendo que la distribución subyacente a los datos en una distribución normal. Esta suposición bajo la cual analizamos los datos y hacemos contrastes puede no ser la correcta, por lo que cualquier conclusión que derive de esas pruebas puede ser errada. Podríamos decidir usar otro modelo, otra distribución subyacente a partir de la cual hacer inferencias, pero aun así, este modelo podría no ser correcto de todos modos. Podemos cuantificar que tanto podemos aceptar la suposición de partida, pero estas decisiones también estarían sujetas a incertidumbre. Es un trade-off entre la necesidad de analizar los datos y la probabilidad de caer en un error debido a esa elección de un modelo. Esta incertidumbre de la que hablamos en el último apartado es lo que se conoce como incertidumbre inductiva y es esto lo que hace que los problemas estadísticos sean inductivos: se parte de las observaciones realizadas sobre una característica/propiedad que no podemos observar directamente al realizar un experimento. Es esta incertidumbre la que hace a las declaraciones derivadas de la inferencia, falibles. Para puntualizar, decimos que existen dos tipos de incertidumbre: Incertidumbre estocástica: es aquella que está relacionada a la aleatoriedad de las observaciones, y la capacidad de estas de dar información sobre parámetros fijos. Se puede manejar al aumentar el tamaño del experimento. Incertidumbre inductiva: se debe a que la información es incompleta al elegir un modelo. Aunque la anterior es fácil de manejar, esta no. Puede ser imposible cuantificarla o controlarla. La idea general de la inferencia es poder cuantificar la incertidumbre estocástica y explicar la variabilidad observada en los datos, pero el mecanismo subyacente no es tan importante de explicar. El problema es que la incertidumbre inductiva tiende a incrementar la incertidumbre estocástica, pero siempre podemos realizar análisis hasta tener un razonable control sobre esta última. Esta distinción entre tipos de incertidumbre y el manejo de ambas, es lo que hace que diferentes investigadores puedan llegar a distintas conclusiones. "],["cómo-se-enfrenta-a-la-inferencia-estadística.html", "9.1 ¿Cómo se enfrenta a la Inferencia Estadística?", " 9.1 ¿Cómo se enfrenta a la Inferencia Estadística? Básicamente, las corrientes de pensamiento tienen que ver sobre cómo se plantea la visión de probabilidad: Como la frecuencia esperada a la larga (luego de repetir el experimento muchas veces); o Como una noción subjetiva de incertidumbre. Por ejemplo: Si lanzamos una moneda, tenemos un sentido de incertidumbre acerca del resultado: decimos que la probabilidad de obtener una cara es de \\(0{,}5\\). Ahora, pensemos en el siguiente lanzamiento: ¿podemos decir que la incertidumbre sobre el resultado sigue siendo \\(0{,}5\\)?¿o la probabilidad de \\(0{,}5\\) solo tiene sentido a la larga? Si obtenemos cara durante el primer lanzamiento, la probabilidad de obtener otra cara, dado que el resultado anterior, viene dado por el teorema de Bayes. Sin embargo, el uso de este teorema requiere que especifiquemos la distribución de probabilidad a priori, y es aquí donde está el problema entre los frecuentistas y bayesianos: Los frecuentistas, estresarían el hecho de que lo que importa es la probabilidad a la larga, sin importar cuanta información tengamos a partir de los datos. Por lo que la distribución de probabilidad subyacente es \\(0{,}5\\) para ellos. Los bayesianos podrían concordar con los frecuentistas, pero también podrían inclinarse a darle importancia a la información que se tiene actualmente. Al haber solo un lanzamiento, la distribución binomial asigna una probabilidad a priori distinta de \\(0{,}5\\) para el siguiente lanzamiento. En este curso solo hablaremos de estadística en un sentido frecuentista, para mayor información sobre el enfoque bayesiano puede consultar @gelman1995 (otro enfoque posible es uno intermedio entre ambas posturas, frecuentista y bayesiana, que se basa en la idea de una probabilidad fiduciaria o función de verosimilitud. Para más información sobre este enfoque pueden consultar @pawitan2001all). Las dos problemáticas principales para un frecuentista son: La elección de una distribución a priori apropiada. El desacuerdo sobre el accionar bajo grados de creencias personales. Sin embargo, la incertidumbre inductiva es mayor por varios ordenes de magnitud a cualquier diferencia en la incertidumbre estocástica que resulta de analizar datos siguiendo el criterio de cualquier escuela de pensamiento. "],["frecuentistas-y-muestreo-repetido..html", "9.2 Frecuentistas y muestreo repetido.", " 9.2 Frecuentistas y muestreo repetido. Los frecuentistas, ven la probabilidad como una frecuencia a la larga, suponiendo que un experimento se repite, de forma hipotética, muchas veces. Es decir, se basa en el principio de muestreo repetido bajo las mismas condiciones. Para ellos, cualquier parámetro de importancia es fijo, y no puede tratarse como una variable aleatoria. Se trata de entender la relación entre el estimado de una propiedad que podemos calcular y el valor real de esa propiedad, imaginándonos como podría el resultado cambiar si en lugar de la muestra seleccionada, se hubiese recolectado otra igualmente probable. Imagine que desea conocer el peso promedio de una población de patos negros, que se distribuye como \\(N(1161\\text{ g}, 9604\\text{ g}^2)\\) (no necesariamente usted ssbe qje esta es la fistribución y, en general, no lo ssbe. Solo usamos este conocimiento para poder ralizqr simulaciones en este ejemplo), por lo que se toma una muestra aleatoria de \\(n = 50\\) patos. En esta muestra, encuentra que el peso promedio es \\(\\hat{\\mu} = 1158{,}2\\) g. Este peso promedio es una estimación del peso promedio verdadero de la población de \\(\\mu=1161\\) g. Ahora, supongamos que se realiza este experimento muchas veces, unas 10 mil veces digamos, y en cada repetición calculamos el peso promedio. En R, podemos lograr hacer esto de la siguiente forma: # Se asume media poblacional de 1161 g y desviacion estandar poblacional de 98 g bd_wieghts &lt;- tibble(duck_id = 1:2300, weight = rnorm(2300, 1161, 98)) # Se realizan 10000 muestreos aleatorios de 50 observaciones de la población. virtual_samples &lt;- bd_wieghts %&gt;% rep_sample_n(size = 50, reps = 10000) # A cada replica se le calcula el peso promedio virtual_bd_weights &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(`Peso Promedio` = mean(weight)) virtual_bd_weights %&gt;% head(10) %&gt;% kbl() replicate Peso Promedio 1 1175.754 2 1199.270 3 1163.568 4 1161.798 5 1162.832 6 1140.051 7 1152.834 8 1178.297 9 1175.702 10 1170.830 Se puede observar de la tabla anterior, que algunas muestras resultan en un peso mayor al valor real de \\(1161\\) g, y otras en valores menores; y si inspeccionáramos cuidadosamente los resultados, podríamos verificar si de hecho el peso promedio en alguna replica es igual al valor de peso verdadero. Cada uno de esos promedios corresponde a un estimador \\(\\hat{\\mu}\\) del verdadero valor medio \\(\\mu\\). Podríamos realizar un histograma de estos valores y veríamos que el verdadero valor esta en el centro de la distribución de valores medios obtenidos de las 10 mil replicas. # Se realiza un grafico de los valores promedios ggplot(virtual_bd_weights, aes(x = `Peso Promedio`)) + geom_histogram(binwidth = 0.1, boundary = 0.4, color = &quot;#213555&quot;) + geom_vline(xintercept = 1161, colour = &quot;white&quot;, linewidth = 1.2) + labs(x = &quot;Peso de los patos negros&quot;, y=&quot;Conteo&quot;, title = &quot;Distribution of 10,000 realizaciones de medidas de peso.&quot;) + theme_light() + theme(panel.grid = element_blank()) Figure 9.1: Distribución muestral de la media \\(\\bar{X}\\) generada por simulación. y calcular el valor promedio de los pesos promedios, que arroja un valor de 1164.27, el cual esta razonablemente en acuerdo con el valor verdadero (el valor real difiere, porque el número de replcias es finito). El gráfico del ejemplo anterior es la distribución muestral del estadísitco \\(\\hat{\\mu}\\). Esta distribución describe la variabilidad de los promedios calculados a partir de las replicas alrededor de la media verdadera \\(\\mu\\). Más adelante (en el capitulo Teoría de Muestreo.) estudiaremos esta distribución, pero en este punto, es importante notar que las propiedades de los estimadores que estudiaremos estan basados en el muestreo repetido hipotético (teoría frecuentista), que permite justificar el comportamiento de estos estimadores: esto es, nos permite derivar y justificar el uso de una distribución de porbabilidad para estos estimadores y, por lo tanto, justificar los resultados de inferencia estadística. "],["ámbito-de-la-inferencia-estadística..html", "9.3 Ámbito de la inferencia Estadística.", " 9.3 Ámbito de la inferencia Estadística. A la inferencia estadística le conciernen 4 tipos de problemas, de los cuales se buscan soluciones que permitan realizar estudios experimentales que lleven a conclusiones relevenates y significativas acerca de un fenómeno en estudio. Estos problemas son: Procesos de recolección de muestras. Hace referencia a la sistematización y correcta cosntrucción de los conjujntos muestras, de forma que sean representativos de la población que se está estudiando, disminuyendo cualquier sesgo en la elección de elementos que conformen la muestra. Estimación puntual y a intervalos. Hace referencia al proceso de hacer inferencia o predicciones acerca de los parámetros de una población, que estan ocultos a nosotros, pero de los cuales queremos precisar su valor a partir de una muestra, tolerando cierto grado de error, que debe cuantificarse. Contraste de hipótesis. Esta hace referencia a un proceso de toma de decisiones acerca de la veracidad estadística de una proposición que se formula como hipótesis de un experimento. Diseño experimental. Se refiere a la forma en la que se planea un experimento para poder realizar inferencias por medio de estimaciones y contrastes de hipótesis, y su correcto establecimiento determina la validez de las conclusiones que se obtienen del proceso de inferencia. En los proximos capitulos estudiaremos cada uno de estos puntos para diferentes posibles experimentos, haciendo mucho emfásis en el contraste de hipótesis. "],["teoría-de-muestreo..html", "10 Teoría de Muestreo.", " 10 Teoría de Muestreo. La teoría de muestreo lidia con el problema de construir un conjunto muestra a partir de un conjunto población. La validez de las conclusiones que se establecen sobre la población dependen de si la muestra se seleccionó de tal forma que representa a la población correctamente. En este punto, necesitamos definiciones precisas de población y muestra: Población: colección de individuos o unidades en la que estamos interesados, y que tiene una ley o distribución de probabilidad asociada. Se denota el tamaño poblacional (el número total de individuos) como \\(N\\). Muestra: es una colección de individuos o unidades tomados de una población. Se denota el tamaño de la muestra (el número total de individuos recolectados de la población) como \\(n\\). En general, \\(n &lt; N\\). Una muestra se puede recolectar de una poblacion por medio de un censo o por muestreo aleatorio. El primero se refiere a un muestreo exhaustivo de todos los individuos que conforman la población. El segundo hace referencia a un método que asegura la recolección aleatoria de elementos del conjunto población para construir un conjunto muestra más pequeño. Dado que la población tiene asociada una ley de probabilidad, se especifica para esta los parámetros de esa ley de probabilidad, y es sobre estos que haremos inferencias, por medio de estimadores. Hasta este punto, henos estado hablando sobre estimadores como valores que calculamos para tratar de saber el verdadero valor de una propiedad o caracteristica, y hemos hablado de parametros como esos valores verdaderos que estan ocultos a nosotros. Definimos ahora con precisión estos términos: Un parámetro es un valor numérico que logra resumir la información sobre una población. En casi todos los casos, este valor es completamente desconocido, pero se busca conocer su valor. Un estimador es un estadístico (es decir, tiene una distribución asociada), un valor numérico que ayuda a resumir la información sobre una muestra y se usa para estimar un parámetro poblacional desconocido. Notación. De forma general, se denota como \\(\\theta\\) al parámetro (o \\({\\boldsymbol\\theta}\\), en notación vectorial, si es más de un parámetro). Pero dependiendo de la ley de probabilidad, se puede especificar otra notación particular. Por ejemplo, es usual llamar a la media poblacional con la letra \\(\\mu\\), a la varianza poblacional con la letra \\(\\sigma^2\\), a las proporciones poblacionales con la letra \\(\\pi\\), entre otras. Si la media poblacional es el único parámetro desconocido, entonces \\(\\theta = \\mu\\). Pero si ambos, media y varianza son desconocidos, se resumen los parámetros con la notacion vectorial usual: \\[{\\boldsymbol\\theta} = \\begin{pmatrix} \\mu \\\\ \\sigma^2 \\end{pmatrix}\\] Ya nos hemos encontrado antes con estimadores y parámetros. En los capítulos de distribuciones de probabilidad (capítulos blah blah blah) definimos leyes de probabilidad discreta y continua de interés, y definimos allí los parámetros de los que dependen. Por ejemplo, para una distribución normal, el parámetro media poblacional se denota como \\(\\mu\\) y la desviación estándar poblacional se define como \\(\\sigma\\). Es usual hacer uso de letras griegas para definir parámetros poblacionales, pero por simplicidad, algunas veces se hace uso de notaciones alfanuméricas usuales. Por ejemplo, para la distribución binomial, la probabilidad de éxito la denotamos como \\(p\\). Sin embargo, también es usual hacer uso de la notación \\(\\pi\\) para describir este parámetro, de forma que no haya confusiones al trabajar con notación de probabilidades. Por otro lado, ya hemos encontrado antes estiamdores usuales: en el capítulo Estadística descriptiva. definimos el promedio, la varianza, la desviación estándar, y otras propiedades con que resumir la información contenida en los datos. Estos son ejemplos de estimadores. Se pueden denotar de dos formas: Usando la letra griega correspondiente al parámetro poblacional, pero colocandole un sombrero. Por ejemplo, podríamos denotar al estimador de la media poblacional \\(\\mu\\) como \\(\\hat{\\mu}\\), al estimador de la desviación estándar poblacional \\(\\sigma\\) como \\(\\hat{\\sigma}\\), y así sucesivamente. Usando letras alfanuméricas para los estimadores. Por ejemplo, al estimador de la media poblacional \\(\\mu\\) es usual denotarlo como \\(\\bar{X}\\), al estimador de la desviación estándar poblacional \\(\\sigma\\) como \\(S\\), y así sucesivamente. En los capítulos siguientes haremos lo posible por adherirnos a la notación de sombreritos donde sea conveniente, pero en algunos casos haremos uso de la notación usual por conveniencia. El contexto se hace claro dependiendo del caso, y se aclarará donde sea necesario en los lugares donde pueda ser causa de confusión. "],["estimación..html", "10.1 Estimación.", " 10.1 Estimación. La estimación consiste en hacer inferencias o predicciones sobre los parámetros de una población, los cuales están ocultos a nostoros, usando la información contenida en una muestra. La estimación puede ser de dos tipos: Estimación puntual: una estimación puntual es un estimador que se calcula a partir de una sola muestra particular. Este tiene tres propiedades esenciales: debe ser i) eficiente, ii) consistente, y iii) suficiente. Estimación por intervalos: son un par de estimadores que definen los límites de un intervalo, que se calculan a partir de una muestra, y se espera que contenga el parámetro que esta siendo estimado. Los estimadores, sea cuales sean, deben construirse. La construcción de estos debe ser tal que estos tengan varias propiedades deseables para hacer inferencia: no deben estar sesgados, lo cual se garantiza al emplear métodos de muestreo que permitan tomar muestras aleatorias. Además, deben elegirse de tal forma que sean de mínima varianza, y deben ser consistentes, esto es, a medida que aumente el esfuerzo de muestreo, el estimador debe ir acercandose cada vez más al valor del parámetro. Cualquier proceso de estimación comienza con la suposición de un modelo estocástico que se asume correcto (como se discute en el capítulo [Infererncia estadística.]) una vez recolectada la muestra. Este modelo es una función de densidad \\(f(x; \\theta)\\) que resume la forma en la que se generan las observaciones (la notación hace referencia a que \\(f\\) es una función de las observaciones \\(x\\), dados los parámetros \\(\\theta\\)). En capítulos anteriores hemos dicho que si una v. a. \\(X\\) sigue una distribución \\(f(x)\\), se escribe \\(X \\sim f(x; \\theta)\\). Cuando recolectamos una muestra, tenemos \\(n\\) observaciones \\(x_1, x_2, \\ldots, x_n\\) de una variable alestoria \\(X\\) en estudio. Sin embargo, resulta más conveniente entender a las observaciones como generadas por \\(n\\) variables aleatorias: \\(X_1\\) genera la observación \\(x_1\\), \\(X_2\\) genera la observación \\(x_2\\), \\(\\ldots\\), \\(X_n\\) genera la observación \\(x_n\\), donde cada v. a. se distribuye con la misma ley de probabilidad \\(f(x, \\theta)\\). En este caso, se dice que las \\(n\\) v. a. se distribuyen identicamente. Si además, estas variables son independientes entre sí, se dicen que las \\(n\\) v. a. son independiente e identicamente distribuidas, que se abrevia como iid, y se escribe: \\[X_i \\overset{iid}{\\sim} f(x_i; \\theta)\\text{ para }i = 1, 2, \\ldots, n\\] que se lee como: las variable aleatorias son independientes e identicamente distribuidas como \\(f(x; \\theta)\\). Ahora, si bien cada variable aleatoria tiene su ley de probabilidad, la muestra en su totalidad tiene una ley de probabilidad que se deriva de la ley de probabilidad de las v. a. individuales. La función de probabilidad conjunta de las \\(n\\) v. a. iid viene dada por: \\[f(X_1, \\ldots, X_n; \\theta) = f(x_1; \\theta) f(x_2; \\theta)\\ldots f(x_n; \\theta)\\] Ejemplo. Digamos que se recolecta una muestra de \\(n\\) observaciones generadas por las v. a. \\(X_1, \\ldots, X_n\\) que son iid como \\(N(\\mu, \\sigma^2)\\). Entonces la función de probabilidad conjunta es: \\[\\begin{aligned} f(x_1, x_2, \\ldots, x_n; \\mu, \\sigma^2) &amp;= f(x_1; \\mu, \\sigma^2) f(x_2; \\mu, \\sigma^2)\\ldots f(x_n; \\mu, \\sigma^2) \\\\ &amp;= \\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_1 - \\mu)^2}{2\\sigma^2}}\\right)\\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_2 - \\mu)^2}{2\\sigma^2}}\\right)\\ldots\\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x_n - \\mu)^2}{2\\sigma^2}}\\right) \\\\ &amp;= \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\sum_{i=1}^n\\frac{(x_i - \\mu)^2}{2\\sigma^2}} \\end{aligned}\\] donde se usó las propiedades de producto de potencia con igual base. Al expandir la potencia y aplicar propiedades de sumatoria (tarea sencilla que puede verificar usted mismo) se obtiene que: \\[f(x_1, x_2, \\ldots, x_n; \\mu, \\sigma^2) = \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\frac{(\\bar{X} - \\mu)^2}{2\\sigma^2}}\\] y esta es la función de distribución conjunta para la muestra de tamaño \\(n\\) recolectada. El enfasis que se hace es que la muestra tiene una ley de probabilidad asociada que resulta de las suposiciones iniciales del modelo, y, por lo tanto, la inferencia es dependiente de estas suposiciones. También se trata de aclarar la notación que usaremos y que se encuentra frecuentemente en textos de estadística. "],["distribución-muestral-de-un-estimador..html", "10.2 Distribución muestral de un estimador.", " 10.2 Distribución muestral de un estimador. En el capítulo anterior construimos la distribución del estimador del peso promedio de una muestra de patos negros de tamaño \\(n\\) tomada de forma aleatoria. Dijimos que esta distribución de las medias obtenidas de cada una de esas muestras hipotéticas es conocida como distribución muestral y que esta muestra la variabilidad del estimador o los posibles valores que este puede tomar. Ahora profundizaremos en las propiedades de esta distribución que la hacen útil en inferencia estadística. La variabilidad de la distribución muestral depende del número de observaciones que componen a la población, del número de observaciones que componen a la muestra, y del procedimiento usado para tomar la muestra de la población. Esta variabilidad se puede cuantificar en una cantidad llamada error estándar, y a medida que aumenta el tamaño de la muestra tomada, menor es el error estándar. Esto se resume en el siguiente resultado: \\[SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\] donde \\(\\sigma\\) es la desviación estándar poblacional. Este resultado es cierto sin importar si la distribución subyacente de la población es normal o no. El valor de la desviación estándar poblacional esta oculto a nosotros, de la misma forma que lo esta la media poblacional, y por lo tanto, también se debe estimar a partir de la muestra. Y, al igual que con la media poblacional, diferentes muestran arrojaran diferentes posibles valores del estimador, y esta variabilidad se puede resumir en la distribución muestral de la desviación estándar. Así mismo, como con otro parámetros podemos obtener estimadores de muestras, entonces estos estimadores siempre tienen asociado una distribución muestral para describir su variabilidad, y esta última se cuantifica usando la medida de error estándar. Ejemplo. Volvamos al ejemplo de muestreo de patos negros para medir su peso total. Realizaremos simulaciones similares a la realizada antes, pero en lugar de repetir el experimento de tomar muestras miles de veces, el experimento de toma de muestra se hace una sola vez. Solo modificaremos el tamaño de la muestra obtenida de la población, de forma que no solo tomemos muestras de tamaño \\(50\\), sino que realizaremos la simulación usando muestras de tamaño \\(20\\), \\(100\\) y \\(1000\\) también. Los resultados se muestran en la figura 10.1. Figure 10.1: Dependencia de la distribución muestral con el tamaño de la muestra recolectada. Vemos que en el gráfico correspondiente a la muestra más pequeña, la variabilidad es bastante grande y la mayoría de los datos se concentran alrededor de la verdadera media \\(\\mu = 1161\\) kg. Pero se observan datos atípicos con frecuencias altas que sesgan mucho la forma de la distribución. Al aumentar el tamaño de la muestra recolectada, se puede observar que el sesgo va desapareciendo y las observaciones atípicas son cada vez menos frecuentes. De esta forma, podemos ver que el tamaño de la muestra afecta el calculo de cualquier estimador, haciendo que este este más o menos desviado del verdadero valor dependiendo del \\(n\\). Ésto es fácil de cuantificar si calculamos la desviación estándar en cada simulación (que se muestra en cada uno de los gráficos de la figura anterior) y luego calculamos el error estándar, como se muestra en el siguiente fragmento de código de R: size sample_size mean std_dev std_error small 20 1125.461 107.63021 24.066846 medium 50 1170.217 92.59296 13.094622 large 100 1158.180 104.15617 10.415617 very large 1000 1162.193 96.90811 3.064503 En la tabla vemos que nuestro estimador de la media y la desviación estándar se acercan cada vez más al valor real de \\(1161\\) kg y \\(98\\) kg, y que el error estándar es cada vez más pequeño a medida que aumenta el tamaño de la muestra. Esto quiere decir que, a medida que aumentamos el tamaño de la muestra, nuestros estimadores se acercan cada vez más al valor real, y se hacen cada vez más precisos. Este resultado se resume en uno de los teoremas más importantes de la teoría de probabilidades y estadística, la ley de los grandes números. La ley de los grandes números. Sea \\(X_1,X_2, \\ldots\\) una sucesión infinita de variables aleatorias independientes e idénticamente distribuidas con media finita \\(\\mu\\). Entonces, cuando \\(n \\rightarrow \\infty\\), \\[\\frac{1}{n}\\sum_{i=1}^{n}X_i \\rightarrow \\mu\\] en donde la convergencia se verifica en el sentido casi seguro (ley fuerte) y también en probabilidad (ley débil). Lo que dice el teorema es que, a medida que la muestra de la cual calculamos el estimador se hace más grande, el error en nuestra medida irá decreciendo más y más hasta converger al verdadero valor del parámetro (la convergencia casi segura y en probabilidad son formas de convergencia de una serie infinita de v. a. definidas en terminos de la probabilidad de que la convergencia se de es segura y de la probabilidad de las desviaciones tan pequelñas como se quiera del estimador y el parámetro es nula, respectivamente. Para más detalles, consulte @lehmann1999elements o @rincon2014introduccion). Aquí un comentario pertinente sobre la notación. Si bien usamos la letra griega \\(\\mu\\) en el teorema, misma letra que usamos para denotar la media poblacional de una distribución normal, no debemos pensar que el teorema solo es cierto oara este parámetro. En el teorema, se usa \\(\\mu\\) como notación más amplia de un parámetro verdadero cualquiera. Por ejemplo, si la variable aleatoría \\(X\\) son desviaciones estándar de la media, entonces el parámetro \\(\\mu\\) es el resltado de promediar todas las desviaciones estándar de todas las posibles muestras reclectadas de tamaño \\(n\\), este promedio es \\(\\sigma\\) y el teorema se escribiría: \\[\\frac{1}{n}\\sum_{i=1}^{n}X_i \\rightarrow \\sigma\\] "],["teorema-del-límite-central-tlc..html", "10.3 Teorema del Límite Central (TLC).", " 10.3 Teorema del Límite Central (TLC). Este teorema es el más importante de los que verémos en este libro. Este establce la distribución muestral de estadísticos construidos a partir de estimadores calculados de una muestra, y usa la ley de los grandes números para verificar que la ley de probabilidad del estadístico converge a una normal estándar. Sea \\(X_1, X_2, \\ldots\\) una sucesión infinita de variables aleatorias independientes e idénticamente distribuidas, con media \\(\\theta\\) y error estándar \\(SE(\\theta)\\). Entonces la función de distribución de la variable aleatoria: \\[Z_n = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)}\\] tiende a la función de distribución normal estándar cuando \\(n\\) tiende a infinito. El TLC establece que el estadístico \\(Z_n\\) tendrá una distribución normal estándar sin importar la distribución de las v. a. \\(X_1, X_2, \\ldots, X_n\\), para valores grnades de \\(n\\), aunque esto no implica que la muestra tendrá una forma de campana. En general, a mayor sea la muestra, más cercana esta estará de la verdadera distribución poblacional. Ahora, desde un punto de vista práctico, este teorema se cumple cuando \\(n \\ge 30\\), de forma que no se necesitan muestras demasiado grandes para poder justificar la normalidad al realizar inferencia. Cuando \\(n &lt; 30\\), la distribución muestral es más variables, haciendo que los estimadores de \\(\\theta\\) y \\(SE(\\theta)\\) sean menos precisos, y por lo tanto es mejor utilizar la distribución \\(t\\)-Student con \\(n - 1\\) grados de libertad. Ejemplo. Simulemos un conjunto de variables aleatoria \\(Z_n\\) para distintos valores de \\(n = 5, 10, 20, 30, 50, 100, 500, 1000\\). Para ellos, imaginemos un experimento donde se siembran \\(n\\) semillas y se registra despues de un tiempo, si la semilla germina o no. Denotamos un exito como la semilla no germnina, que se sabe tiene probabilidad de exito de \\(0{,}2\\). Con esta información, simulamos \\(1000\\) replicas del experimento para cada \\(n\\), y construimos un histograma sobre el cual dibujamos una curva de la función de densidad normal estándar para comparar. el gráfico muestra indudablemente que a medida que el \\(n\\) crece, el histograma de la distribución muestral se aproxima cada vez más a una normal estándar, y que el ajuste siempre es mejor cuando \\(n \\ge 30\\), y para valores menores a este, el ajuste no es tan bueno. Este ejemplo busca convencerlo de que el TLC es válido y aplicable a la hora de realizar inferenias. Pero también enfatiza la importancia de elegir un tamaño muestral aadecuado para que la suposición de normalidad tenga sentido de \\(Z_n\\) tenga sentido. En los próximos capítulos estaremos usando este teorema continuamente cuando derivemos la distribución muestral de los estadísitcos que cosntruiuremos para realizar estimaciones y contrastar hipóteis. Corrección por población finita. Para cualquier población estadística que consiste de \\(N\\) unidades, se define la corrección de población finita como: \\[1 - f = 1 - \\frac{n}{N}\\] Solo tiene importancia en poblaciones pequeñas, en las que \\(n &gt; 0{,}05\\times N\\). Modifica los estimadores de la desviación estandar. "],["necesidad-de-especificar-una-muestra..html", "10.4 Necesidad de especificar una muestra.", " 10.4 Necesidad de especificar una muestra. La especificación de una muestra requiere de varios pasos particulares, pero de forma invariable esta sujeta a la pregunta que se busca responder al realizar un experimento, y a la cantidad de esfuerzo y dinero que se puede invertir para realizar dicho experimento. El proceso de diseño y toma de muestra no es puramente matemático: Los objetivos del experimento deben especificarse, y esto involucra especificar variables que podrían ser importantes, el equipo de muestreo a usar, cuanto esfuerzo se puede invertir, … Toda esta información, es la que ayuda a elegir un método de estimación a usar. En este punto, se hace la pregunta: ¿Qué tan grande debe ser la muestra que voy a tomar? El tamaño de la muestra es muy importante para asegurar la representatividad de la población que se esta muestreando, pero también lo es para poder evaluar el efecto de un tratamiento al realizar un experimento (en, por ejemplo, diseños experimentales factoriales o por bloques, o análisis tan sencillos como pruebas \\(t\\)). La elección del tamaño de la muestra requiere de la especificación de: La prueba a utilizar. El nivel de significancia mínimo a usar (tradicionalmente, 1%, 5% o 10%). El tamaño del efecto a contrastar. La potencia deseada para la prueba (usualmente, 80%). La potencia tiene como valor el que su consideración obliga al investigador a pensar en términos de la fuerza de los efectos que su experimento es probable produzca. Aquí, la información a priori comienza a ser de vital importancia. Particularmente en el contraste de hipótesis El trabajo del investigador no es demostrar que un tratamiento no produce el mismo efecto que el control, es demostrar la efectividad del tratamiento. El tamaño de la muestra a usar tiene que ser tal que: * Permita determinar si un efecto dado (su magnitud) puede interpretarse como suficientemente confiable o válido como para que la comunidad científica acepte una hipótesis. * Permita determinar (o se determina tal que) que tan probable es que los datos de un estudio resulten en una significancia estadística antes de que el estudio se haya llevado a acabo. No solo es profesionalmente autodestructivo el diseñar experimentos que no tengan una alta probabilidad de éxito, sino que no es ético el hacerlo por la simple razón de que se consumen recursos escasos (monetarios, de esfuerzo o tiempo). "],["diseño-de-muestreo..html", "10.5 Diseño de muestreo.", " 10.5 Diseño de muestreo. El elegir un correcto método de muestreo es vital para obtener información representativa de la población. Se hace importante el diseño del muestreo: en cuanto al método (sistemático o aleatorio) y en cuanto al número de muestras. Muestreo aleatorio simple. Muestreo aleatorio estratificado. Muestreo adaptativo. Muestreo sistemático. Consideraciones. Debemos primero establecer de forma explícita la población estadística. Se debe especificar la unidad de muestreo. Luego se selecciona una muestra con algún plan particular. Cualquier estadístico asume que la muestra sigue los principios del muestreo probabilístico: Se define un conjunto de muestras distintas \\(S_1\\), \\(S_2\\), \\(\\ldots\\) en el que cierta unidad de muestreo especifica es asignado a \\(S_1\\), otro a \\(S_2\\), y así sucesivamente. A cada muestra \\(S_i\\) ( \\(i=1, 2, \\ldots\\) ) se le asigna una probabilidad de ser seleccionada. Se selecciona una de las muestras por la probabilidad adecuada, usando una tabla de números aleatorios. Claro, el muestreo puede no ser probabilístico: Muestreo de solo unidades accesibles. Muestreo influenciado por sesgos sensoriales. Influencia de prejuicios u otras subjetividades al muestrear unidades típicas El uso de solo voluntarios. 10.5.1 Muestreo Aleatorio Simple. 10.5.2 Muestreo Aleatorio Estratificado. La población de \\(N\\) individuos se subdivide en \\(h\\) subpoblaciones o estratos que no se solapen, de forma que \\(N = N_1 + N_2 + \\ldots + N_L\\). Cada estrato es entonces muestreado por separado, obteniéndose muestras \\(n_1\\), \\(n_2\\), \\(\\ldots\\), \\(n_L\\). La estratificación es recomendable cuando: * Se requieren estimadores de medias y varianzas separadamente para cada estrato. * La probabilidad de seleccionar una muestra varía de un área a otra. * Se necesita mayor precisión de un estimador. * La organización administrativa del equipo lo ve conveniente. ¿Cómo construir los estratos? * No deben exceder más de 6 estratos. * Basado en conocimiento a priori. * Basado en una variable a medir o controlar. * Las muestras pueden decidir estratificarse luego del proceso de recolección. 10.5.3 Muestreo Adaptativo. Hace uso de datos recolectados para realizar decisiones sobre el esfuerzo de muestreo. Puede ser de dos tipos: i) Muestreo adaptativo aglomerado (clusters): se realiza un muestreo aleatorio simple, como se describió antes. Si una o más unidades de muestreo tienen una muestra de interés, se seleccionan unidades de muestreo adicionales en la vecindad de estas. ii) Muestreo adaptativo aglomerado estratificado: se sigue el procedimiento descrito en i), pero sobre estratos definidos como se explica antes. 10.5.4 Muestreo Sistemático. Se usa principalmente por su simplicidad, y también para poder realizar muestreos uniformemente espaciados (en tiempo y espacio), que pueden tratarse como aleatorios, sin sesgo alguno. Se debe cuidar por la presencia de variaciones periódicas. "],["proceso-de-muestreo..html", "10.6 Proceso de Muestreo.", " 10.6 Proceso de Muestreo. Identificación del conjunto Población. Determinación del tamaño de nuestro conjunto muestral. Proporcionar un medio para la base de la selección de muestras del medio Población. Selección de muestras del medio utilizando una de las muchas técnicas de muestreo como el muestreo aleatorio simple, sistemático o estratificado. Verificar si el conjunto de muestra formado contiene elementos que realmente coinciden con los diferentes atributos del conjunto de población, sin grandes variaciones entre ellos. Comprobación de errores o estimaciones inexactas en el conjunto de muestras formadas, que pueden o no haber ocurrido. El conjunto que obtenemos después de realizar los pasos anteriores en realidad contribuye al conjunto de muestra. "],["teoría-de-estimación..html", "11 Teoría de Estimación.", " 11 Teoría de Estimación. La estimación consiste en realizar predicciones o inferencias sobre los parámetros de una distribución usando la información contenida en la muestra. Formalmente Dada una variable aleatoria \\(X\\) cuya ley de probabilidad depende de un parámetro \\(\\theta\\), un estadístico \\(g(X)\\) se dice es estimador de \\(\\theta\\) si, para cualquier valor observado de \\(x \\in X\\), \\(g(x)\\) se considera un estimado de \\(\\theta\\). Esta definición se puede escribir de otra forma, como: Dadas las observaciones de variables aleatorias \\(X_1, X_2, \\ldots, X_n\\) idéntica e independientemente distribuidas (iid) con función de distribución \\(F(x\\vert\\theta)\\), se estima \\(\\theta\\). En la primera definición anterior, \\(g(x)\\) es una función de la muestra (esto es, de las observaciones realizadas de la v. a. \\(X\\)). La definición puede ser un poco difícil de comprender, pero podemos revisarla poco a poco usando un ejemplo. Ejemplo. Digamos que se realiza un muestreo aleatorio simple de una población cuya función de densidad es una v. a. normal de media \\(\\mu\\) y varianza finita \\(\\sigma^2\\). Digamos que se quiere construir un estimador de la media poblacional. Ya sabemos que el mejor estimador de la meda poblacional es \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\), y por lo tanto se tiene que \\(g(x) = \\bar{X}\\). De aquí en adelante, nos centraremos en construir estadísticos, además de usar los estimadores usuales ya conocidos. Pero antes, veamos las propiedades que tiene un buen estimador. "],["propiedades-de-un-estimador..html", "11.1 Propiedades de un estimador.", " 11.1 Propiedades de un estimador. Cualquier estimador que se precie de ser un buen estimador debe cumplir con 3 propiedades deseables para hacer inferencia. El estimador debe ser insesgado: un estimador se dice es insesgado cuando la esperanza de su estadístico es igual al valor del parámetro siendo estimado. Se escribe el sesgo \\(B(\\hat{\\theta})\\) como: \\[B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta\\] Por ejemplo, podemos calcular la esperanza del estadístico \\(\\bar{X}\\) como \\(E[\\bar{X}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]\\). Usando las propiedades \\(E[cX] = cE[X]\\) y \\(E[\\sum_i X] = \\sum_iE[X]\\), donde \\(c\\) es una constante, se tiene que: \\[E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i]\\] Pero como \\(E[X] = \\mu\\) (el valor esperado de una v. a. es su media) entonces: \\[\\frac{1}{n}\\sum_{i=1}^n E[X_i] = \\frac{1}{n}\\sum_{i=1}^n\\mu_X = \\mu\\] y vemos que el estadístico \\(\\bar{X}\\), que estima \\(\\mu\\) tiene sesgo nulo. Por lo tanto, \\(\\bar{X}\\) es un buen estimador de la media. El estimador debe ser eficiente: un estimador \\(g(x)\\) se dice que es eficiente si de todos los posibles estimadores, \\(g(x)\\) tiene la mínima varianza posible. Más formalmente, si \\(\\hat{X_1}\\) y \\(\\hat{X_2}\\) son ambos estimadores insesgados de \\(X\\), entonces se dice que \\(\\hat{X_1}\\) es un estimador más eficiente de \\(X\\) que \\(\\hat{X_2}\\), si \\(\\sigma^2_{\\hat{X_1}} &lt; \\sigma^2_{\\hat{X_2}}\\). Figure 11.1: Ejemplo gráfico de estimadores: a) sesgado y no eficiente; b) insesgado pero no eficiente; c) sesgado y eficiente; d) insesgado y eficiente. El estimador debe ser consistente: se dice que un estimador \\(g(x)\\) es consistente si este se aproxima a al parámetro \\(\\theta\\) cuando el esfuerzo de muestreo se hace mayor. Formalmente: sea \\(X_1, X_2, \\ldots, X_n\\) variables aleatorias iid que se usan para obtener un estimador \\(\\hat{\\theta}\\) de \\(\\theta\\). Se dice que \\(\\hat{\\theta}\\) es un estimador consistente si converge en probabilidad a \\(\\theta\\), esto es: \\[\\lim\\limits_{n \\to \\infty} P\\left[\\vert\\hat{\\theta} - \\theta\\vert\\ge\\varepsilon\\right] = 0\\] El último límite se puede modificar para comprender mejor la propiedad de consistencia. Para ello, se puede usar la desigualdad de Chebyshev \\[P\\left[\\vert\\hat{\\theta} - \\theta\\vert\\ge\\varepsilon\\right] \\le \\frac{\\sigma^2_\\theta}{\\varepsilon^2}, \\varepsilon &gt; 0\\] y luego, tomando límites a ambos lados, podemos escribir la propiedad de consistencia como: \\[\\lim\\limits_{n \\to \\infty} \\sigma^2_\\hat{\\theta} = 0\\] Entonces, un estimador es consistente, cuando la varianza de este cae cero cuando aumentamos el esfuerzo de muestreo. Dicho de otra forma, el estimador es consistente cuando se acerca más al verdadero valor del parámetro cuando \\(n\\rightarrow\\infty\\). Ahora podemos proceder a estudiar los estimadores puntuales y por intervalos. "],["estimación-puntual..html", "11.2 Estimación puntual.", " 11.2 Estimación puntual. Cuando un investigador realiza un experimento, por lo general, solo toma una muestra representativa de tamaño \\(n\\) de la población de interés y calcula estimadores que le permitan describir los datos obtenidos y realizar inferencias. El investigador no se molesta en realizar el experimento varias veces (no es como las simulaciones, donde podíamos realizar repeticiones tantas como quisiéramos. En la realidad, no se tiene el esfuerzo, la energía o los medios para realizar múltiples repeticiones de un experimento). En estos casos se usan estimadores puntuales para poder realizar inferencias basados en solo una repetición del experimento. Un estimador puntual de un parámetro \\(\\theta\\), es solo un valor \\(\\hat{\\theta}\\) de un estadístico \\(\\hat{\\Theta} = g(X)\\). Para aclarar la notación, \\(\\hat{\\Theta}\\) es el conjunto de todos los posibles valores del estadístico, y \\(\\hat{\\theta}\\) es un elemento de ese conjunto particular, calculado a partir de una muestra. Veamos algunos ejemplos. Ejemplo. Un experimento que busca evaluar la reacción de saltamontes a estímulos visuales o acústicos, en los que midieron el tiempo de reacción a estos antes del vuelo, encontraron que el tiempo de reacción promedio a estímulos acústicos es \\(\\bar{X}_a = 108{,}05\\) segundos, y a estímulos visuales es \\(\\bar{X}_v=87{,}19\\) segundos. Estos dos valores son estimadores puntuales de las medias poblacionales \\(\\mu_a\\) y \\(\\mu_v\\). El siguiente, es un ejemplo que trata de enseñar cómo realizar estimaciones puntuales en diseños estratificados. Ejemplo. Siniff y Skoog (1964) realizaron un muestreo aleatorio estratificado de una manada de caribúes de Nelchina en Alaska. Para ello, se establecieron 6 estratos (basados en estudios preliminares de la densidad relativa de los caribúes), y seleccionaron de manera aleatoria una muestra en cada uno de tamaño \\(n_i\\) ( \\(i=A, B, C, D, E, F\\) ), cada una de unidades muestrales de 4 millas cuadradas, obteniéndose los datos mostrados en la tabla. Se desea saber el tamaño total de la población de caribúes. Estrato Tamaño del Estrato (\\(S\\)) Tamaño de muestra (\\(N_h\\)) Número promedio de Caribúes Varianza A 400 98 24.1 5575 B 30 10 25.6 4064 C 61 37 267.6 347556 D 18 6 179.0 22798 E 70 39 293.7 123578 F 120 21 33.2 9795 Total 699 211 NA NA Para poder conocer un estimado del tamaño poblacional total \\(\\hat{N}\\), se necesita primero de un estimado del número promedio de caribúes por unidad de muestreo. \\[ \\begin{aligned} \\bar{X}_{ST} &amp;= \\frac{\\sum_{h=1}^L N_h \\bar{x}_h}{N} \\\\ &amp;= \\frac{400\\times24{,}1 + 30\\times25{,}6 + 61\\times267{,}6 + \\ldots}{699} \\\\ &amp;= 77{,}96\\text{ caribúes milla}^{-2} \\end{aligned} \\] y se puede calcular la densidad de toda la población usando el total de millas cuadradas que conforman los estratos: \\[\\hat{N} = S \\times \\bar{X}_{ST} = 699\\text{ milla}^2 \\times 77{,}96\\text{ caribúes milla}^{-2} = 54.597\\text{ caribúes}\\] Sabemos entonces, que el estimado del número de caribúes es \\(\\hat{N} = 54.597\\text{ caribúes}\\). Sin embargo, aun necesitamos cuantificar la incertidumbre asociada a esta estimación. Podemos calcular la varianza de \\(\\bar{X}_{ST}\\) como: \\[Var(\\bar{X}_{ST}) = \\sum_{i=1}^L\\left[ \\frac{W_h^2 S_h^2}{n_h}(1 - f_h) \\right]\\] donde \\(W_h = N_h / N\\) es el ponderado del estrato y \\(f_h = n_h / N_h\\). Usando los datos de la tabla: \\[Var(\\bar{X}_{ST}) = \\left[ \\frac{0{,}572^2 5575}{98} \\right]\\left(1 - \\frac{98}{400}\\right) + \\left[ \\frac{0{,}043^2 4064}{10} \\right]\\left(1 - \\frac{10}{30}\\right) + \\ldots = 69{,}83\\] de forma que la varianza del tamaño de la población de caribúes es \\(69{,}83 \\times 699^2 = 34.105.734\\), y la desviación estándar es \\(\\sqrt{34.105.734} = 5.840\\) caribúes. Entonces el estimador buscado, con su medida de incertidumbre, es \\[54.597 \\pm 5.840 \\text{ caribúes}\\] El siguiente ejemplo, es uno donde se construye un estadístico a partir de otro que tiene una ley de probabilidad especificada. De esta forma, podemos facilitar la obtención de una distribución muestral asociada al nuevo estadístico que permita obtener medidas de probabilidad asociada a valores observados particulares. Ejemplo. Digamos que tenemos una estimador puntual que queremos evaluar, digamos, la media calculada \\(\\bar{X}\\) de una muestra de tamaño \\(n\\), en cuanto a la probabilidad de ocurrencia de este. El TLC nos indica que este estimador se distribuye normalmente (si conocemos la varianza poblacional o si el \\(n\\) es lo suficientemente grande como para asumir que conocemos la varianza poblacional lo suficientemente bien). Escribimos entonces el estimador puntual \\[\\hat{Z} = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\] Como vimos en la sección [distribucion-normal], este se puede usar para encontrar valores de probabilidad asociado a obtener un valor de a lo sumo \\(\\bar{X}\\) como: \\[P(X \\le \\bar{X}) = P(Z \\le\\hat{Z})\\] 11.2.1 Construcción de estadísticos para inferencia. El ejemplo anterior es muy importante. Nos dice que si podemos asumir una distribución para una variable aleatoria, entonces podemos construir un estadístico con el cual facilitar la obtención de medidas de probabilidad. Esto nos da una forma sencilla de encontrar probabilidades asociadas a un estimador particular calculado a partir de una muestra, y así, poder obtener medidas de incertidumbre que nos permitan derivar conclusiones adecuadas sobre los estimadores. Estadístico sobre la media Los estadísticos sobre la media los podemos escribir usando el TLC como base para realizar inferencia. Si tenemos un conjunto de v. a. \\(X_1, X_2, \\ldots, X_n\\) independientes e idénticamente distribuidas como \\(f(\\theta)\\), de las cuales se hacen las observaciones \\(x_1, x_2, \\ldots, x_n\\), de la cual estimamos el valor \\(\\hat{\\theta}\\), entonces podemos construir un estadístico sobre \\(\\theta\\) como: \\[\\frac{\\hat{\\theta} - \\theta}{SE(\\theta)}\\] Este estadístico lo podemos entender al darnos cuenta de dos cosas: El uso de la diferencia \\(\\hat{\\theta} - \\theta\\) sirve como una medida de similitud entre el estimador \\(\\hat{\\theta}\\) y el valor real del parámetro \\(\\theta\\): valores grandes indican que ambos son menos parecidos entre sí, mientras que valores pequeños de esta diferencia indican que el estimador y el parámetro son más similares entre sí. De igual forma, el signo de la diferencia nos dice la dirección en la que cae el estimador: valores positivos indican que se tienen valores por encima del valor del parámetro, mientras que un signo negativo indica que el valor estimado cae por debajo del verdadero valor del parámetro. Al dividir la diferencia entre el error estándar \\(SE(\\theta)\\), lo que se hace es estandarizar la diferencia. De esta forma, las diferencias las podemos entender como desviaciones estándar, esto es, a cuantas desviaciones estándar el estimador cae del parámetro. En este caso, se asume que el error estándar es conocido, de forma que el estadístico sigue una distribución normal estándar (según el TLC), y se escribe: \\[\\hat{Z} = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)} \\sim N(0, 1)\\] La ecuación anterior también es válida aun si tenemos que calcular el error estándar de los datos, siempre y cuando el tamaño de la muestra recolectada para estimar \\(SE(\\theta)\\) sea lo suficientemente grande como para asegurarnos de que sabemos su valor con una exactitud adecuada. Si, por otro lado, no conocemos el verdadero valor de \\(SE(\\theta)\\) y la muestra de donde estimamos a este es muy pequeña, entonces debemos asumir que este es una variable aleatoria más. La distribución muestral del nuevo estadístico la podemos obtener notando que: \\[SE(\\hat{\\theta}) = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\] donde \\(\\hat{\\sigma}^2 \\sim \\chi^2(n - 1)\\), y por la proposición final en la sección [distribución-\\(t\\)-student], entonces el estadístico sigue una distribución \\(t\\)-Student y se escribe como: \\[\\hat{t} = \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\sim t(n - 1)\\] Con este esquema general, podremos realizar inferencias con respecto a la media y otros parámetros, como veremos en la siguiente sección. Pero antes, veamos cómo construir un estadístico sobre la varianza. Estadístico sobre la varianza Antes usamos la diferencia entre el estimador y el parámetro para construir un estadístico que nos dijera que tan similares son. Este argumento funciona bien con estadísticos como la media, ya que este corresponde a una medida de locación, y la lejanía de dos locaciones (numéricamente hablando) nos permite entender que tan similares son (ve la figura ??). Con las varianzas, el argumento de la diferencia no es tan intuitivo. Si queremos saber si una varianza es mayor o menor que otra, resulta más intuitivo verificar que tanto mayor (o menor) es la dispersión de una población con respecto a otra. Esto apunta al uso de cocientes entre varianzas. Si tenemos un conjunto de v. a. \\(X_1, X_2, \\ldots, X_n\\) independientes e idénticamente distribuidas como \\(f(\\theta)\\), de las cuales se hacen las observaciones \\(x_1, x_2, \\ldots, x_n\\), de la cual estimamos la varianza \\(S^2\\), entonces podemos construir un estadístico sobre \\(S^2\\) como: \\[\\frac{(n - 1) S^2}{\\sigma^2}\\] la cual sabemos, por la proposición que vimos al final de la sección [distribucion-ji-cuadrada] sabemos se distribuye como una distribución \\(\\chi^2\\) con \\(n - 1\\) grados de libertad, por lo que podemos escribir: \\[X^2 = \\frac{(n - 1) S^2}{\\sigma^2} \\sim \\chi^2(n - 1)\\] Se tiene entonces que: Si la muestra proviene de la misma población, el valor esperado de la varianza \\(E[S^2]\\) será \\(\\sigma^2\\) y el valor esperado del cociente será \\(E[(n - 1) S^2 / \\sigma^2] = \\frac{(n - 1)}{\\sigma^2} E[S^2] = n - 1\\). Este valor resulta que corresponde a la media de la distribución \\(\\chi^2\\) con \\(n - 1\\) grados de libertad. Si la muestra proviene de una población con un varianza menor, entonces el valor esperado \\(E[S^2]\\) es menor que \\(\\sigma^2\\), por lo que el cociente \\(\\frac{(n - 1)}{\\sigma^2} E[S^2] &lt; n - 1\\). Si la muestra proviene de una población con un varianza mayor, entonces el valor esperado \\(E[S^2]\\) es mayor que \\(\\sigma^2\\), por lo que el cociente \\(\\frac{(n - 1)}{\\sigma^2} E[S^2] &gt; n - 1\\). Los casos anteriores corresponden a lo que esperaríamos a la larga (si repitiéramos muchas veces el experimento). Pero hay que entender, que al hacer el experimento y recolectar una muestra, el valor estimado de \\(S^2\\) puede ser menor o mayor que \\(\\sigma^2\\) solo por efecto del azar. Podemos calcular entonces que tan probable es que el valor sea tan grande como el encontrado usando la distribución muestral, \\(P(\\chi^2 \\ge X^2)\\). Ahora, el procedimiento anterior es útil cuando queremos verificar si la varianza calculada de una muestra, corresponde con la varianza conocida para la población de donde se tomó la muestra. Pero podríamos querer comparar dos poblaciones distintas, para verificar si sus varianzas son las mismas. En este caso, supongamos que las varianzas de las poblaciones son \\(\\sigma_1^2\\) y \\(\\sigma_2^2\\), cuyos estimadores respectivos son \\(S_1^2\\) y \\(S_2^2\\), calculados a partir de muestras de tamaño \\(n_1\\) y \\(n_2\\), respectivamente. Podemos usar una parte de estadístico que construimos antes para cada una de las varianzas: \\[\\frac{(n_i - 1) S_i^2}{\\sigma_i^2}\\] para \\(i = 1\\) y \\(2\\), que sabemos corresponden a una v. a. que siguen una distribución \\(\\chi^2\\) con \\(n_i - 1\\) grados de libertad. Entonces, podemos usar la proposición final de la sección [distribucion-f] para verificar que \\[\\frac{S_1^2 / \\sigma_1^2}{S_2^2 / \\sigma_2^2} = \\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2}\\] sigue una distribución \\(F\\) con \\(n_1 - 1\\) y \\(n_2 - 1\\) grados de libertad, y se puede escribir: \\[\\hat{F} = \\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2} \\sim F(n_1 -1, n_2 - 1)\\] El valor esperado para esta distribución es ligeramente mayor a uno para tamaños de muestra muy pequeño, y es aproximadamente de uno para tamaños de muestra grande. De forma que: si el valor es mayor o menor a uno, podríamos hablar sobre varianzas que no son iguales, y por tanto, las muestras son obtenidas de poblaciones distintas. De nuevo, esto es en un sentido estadístico: la muestra puede resultar en proporciones de varianzas distintas a uno solo por azar. Podemos calcular que tan probable es que el valor sea tan grande como el encontrado usando la distribución muestral, \\(P(F \\ge \\hat{F})\\). Usando este esquema general (los estadísticos construidos), podemos cuantificar la incertidumbre con respecto a un parámetro en problemas de inferencia como sigue en la siguiente sección. "],["estimación-por-intervalos..html", "11.3 Estimación por Intervalos.", " 11.3 Estimación por Intervalos. Una estimación por intervalo de un parámetro \\(\\theta\\) es un intervalo de la forma \\(\\hat{\\theta}_L &lt; \\theta &lt; \\hat{\\theta}_U\\), donde \\(\\hat{\\theta}_L\\) y \\(\\hat{\\theta}_U\\) (los límites inferior y superior del intervalo) dependen del valor del estimador \\(\\hat{\\Theta}\\) para una muestra específica, y también de la distribución de muestreo de \\(\\hat{\\Theta}\\). Al intervalo \\(\\hat{\\theta}_L &lt; \\theta &lt; \\hat{\\theta}_U\\) se le llama intervalo de confianza, y su longitud es un indicador de la precisión de una estimación puntual. Figure 11.2: Intervalo de confianza para una variable aleatoria. El área sombreada corresponde al valor de probabilidad asociada al intervalo. Hay que hacer énfasis en algo muy importante: los límites del intervalo son estimados a partir de la muestra, por lo que son v. a. y no se pueden entender como parámetros fijos. Esto es, son estimadores de \\(\\Theta_L\\) \\(\\Theta_U\\). Esto implica que cualquier medida de probabilidad asociada al intervalo se hace en términos de los límites, y no del parámetro sobre el cual se construye. El razonamiento de la construcción de intervalos de confianza es utilizar la distribución muestral de \\(\\hat{\\Theta}\\) para determinar los límites del intervalo de tal manera que: \\[P(\\hat{\\Theta}_L &lt; \\theta &lt; \\hat{\\Theta}_U) = 1 - \\alpha, \\quad 0 &lt; \\alpha &lt; 1\\] para un valor prespecificado de \\(\\alpha\\). Decimos que hay una probabilidad de \\(1 - \\alpha\\) de que el intervalo contenga al verdadero valor del parámetro \\(\\theta\\), con una confianza de \\(100(1 - \\alpha)\\)%. Al valor de \\(\\alpha\\) se le conoce como nivel de significancia y es uno de los parámetros más importantes que estudiaremos, dada su importancia en la especificación del tamaño de muestras al momento de diseñar experimentos. Este valor expresa el grado de incertidumbre que esperamos a la larga sobre la veracidad del intervalo que construimos, y, por lo tanto, sobre las conclusiones que derivamos del mismo. En general, el valor de \\(\\alpha\\) se suele especificar como \\(0{,}1\\), \\(0{,}05\\) o \\(0{,}01\\) dependiendo de que tanta incertidumbre estamos dispuestos a ceder en cuanto a nuestras conclusiones y las consecuencias que derivan de presentar conclusiones equivocadas. Por ejemplo, digamos que un médico construye un intervalo de confianza para un estudio en el que se busca probar la eficacia de cierto fármaco en curar una enfermedad. Si el investigador eligiera un nivel de significancia de \\(0{,}1\\), entonces el intervalo construido es del \\(100(1 - 0{,}1)\\% = 90\\%\\) de confianza, el cual puede parecer bastante grande. Sin embargo, la incertidumbre asociada puede ser demasiado grande si notamos que el hecho de equivocarse significaría posiblemente la aparición de efectos secundarios graves sobre el paciente e incluso, la muerte de una persona. Es por ello que, dependiendo del estudio que estemos realizando, se debe escoger un nivel de confianza adecuado para asegurar que nuestras conclusiones no resulten en la toma de decisiones que puedan tener consecuencias negativas importantes. El teorema del límite central se hace muy importante para la construcción de estadísticos cuya ley de probabilidad es conocida, tal como se estudió en la sección anterior. 11.3.1 Inferencia sobre la media. Ahora veremos unos ejemplos de cómo usar ese formalismo para construir intervalos de confianza para distintos casos particulares. El primero de nuestros ejemplos es sobre la construcción de intervalos de confianza para una muestra para la cual se conoce la varianza poblacional. Ejemplo. Cuando 14 estudiantes de segundo año de medicina del Bellevue Hospital midieron la presión sanguínea de la misma persona, obtuvieron los siguientes resultados: \\(138, 130, 135, 140, 120, 125, 120, 130, 130, 144, 143, 140, 130\\), y \\(150\\) mmHg. Suponiendo que se sabe que la desviación estándar poblacional es de \\(10\\) mmHg, construya un estimado de un intervalo de confianza del 95% de la media poblacional. De manera ideal, ¿cuál debe ser el intervalo de confianza en esta situación? Solución. Una manera de construir un estadístico es estableciendo una expresión que nos diga cuanto se desvía nuestro estimador del valor real. Esto, ya vimos, lo podemos lograr usando una diferencia estandarizada: \\[\\hat{Z} = \\frac{\\hat{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{133.93 - \\mu}{10 / \\sqrt{14}} \\sim N(0, 1)\\] Entonces podemos construir un intervalo de confianza del 95% (esto indica que \\(0{,}95 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}05\\)) como: \\[ \\begin{aligned} P(z_{\\alpha/2} &lt; Z &lt; z_{1 - \\alpha/2}) &amp;= P\\left(-z_{1 - \\alpha/2} &lt; \\frac{133.93 - \\mu}{10 / \\sqrt{14}} &lt; z_{1 - \\alpha/2}\\right) = 0{,}95 \\\\ &amp;= P\\left(-z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; 133.93 - \\mu &lt; z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ &amp;= P\\left(-133.93 - z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; - \\mu &lt; -133.93 + z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ &amp;= P\\left(133.93 - z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; \\mu &lt; 133.93 + z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\right) = 0{,}95 \\\\ \\end{aligned} \\] Tal que el intervalo es: \\[133.93 - z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}} &lt; \\mu &lt; 133.93 + z_{1 - \\alpha/2}\\frac{10}{\\sqrt{14}}\\] Como \\(\\alpha=0{,}05\\), entonces \\(\\alpha/2=0{,}025\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución normal, o usando qnorm(.975, 0, 1) en R. En este caso, \\(z_{1 - \\alpha/2} = 1.96\\), de forma que: \\[128.69 \\text{ mmHg} &lt; \\mu &lt; 139.17 \\text{ mmHg}\\] Podemos observar que el intervalo ocupa valores de presión sanguínea que podríamos considerar alta, indicando que el paciente sufre de hipertensión, de lo cual podríamos estar seguros con un 95% de confianza. En el ejemplo anterior, usamos lo aprendido en la sección anterior sobre estadísticos sobre la media. Pero esto supone que la distribución subyacente de los datos es normal, lo cual hace obvio preguntar ¿cómo sé que mis datos son normales? Podemos tratar de ver que tan bien se ajustan nuestros datos a nuestro supuesto de normalidad, evaluando un gráfico QQ (cuantil-cuantil). Este gráfico muestra en el eje horizontal la distribución teórica (la normal estándar) y en el eje vertical, la distribución de las observaciones. Si la distribución de las observaciones fuera normal, los puntos observados en la gráfica caerían exactamente sobre la recta central (esta corresponde a el caso teórico de esperado si la data fuera normal realmente). De otro modo, si la distribución subyacente no es normal, los puntos se desviaran más de la recta central. La forma como estos se desvían de la recta puede ayudar a indicar cuál es la distribución subyacente, o darnos cuenta de observaciones anormales o atípicas. El siguiente es un gráfico QQ para los datos de presión sanguínea: Figure 11.3: Gráfico QQ para los datos de presión sanguínea del ejemplo del texto. Se observa en el gráfico que las observaciones que caen por debajo de la media se desvían más de la normalidad que aquellas por encima o alrededor de la media. Esto nos indica que estos valores pueden resultar ser atípicos, lo cual podría corresponder bien con la conclusión de que es bastante probable que el paciente parece tener hipertensión (lo cual hace bastante extraño obtener valores de presión menores a 128 mmHg). Ahora, ya dijimos que el TLC es aplicable a estadísticos que son estimadores de parámetros que corresponden a distribuciones otras que la normal. Los siguientes sirven de ejemplos de inferencia sobre parámetros de una distribución Poisson y Binomial, respectivamente. Ejemplo. Unos nutricionistas especializados en dieta canina han estado evaluando la efectividad de cierta dieta (con un componente nutricional especial) como medida para controlar la presencia de garrapatas en mascotas cuidadas bajo las mismas condiciones. Para ello, se seleccionaron y asignaron al azar \\(20\\) caninos a dos grupos, 10 en cada grupo. A uno se le administró la nueva dieta, y el otro se alimentó con la misma dieta, pero sin el compuesto antiectoparásitos dado al primer grupo (este sirve como grupo control para verificar si existen diferencias). Después de un tiempo apropiado, se midió en los canino el número de garrapatas por individuo. Los datos son los siguientes: Control: \\(22, 17, 15, 7, 12, 16, 12, 14, 20, 13\\) Tratamiento: \\(4, 5, 10, 7, 2, 2, 6, 10, 7, 2\\) Se desea saber si hay un cambio en el número promedio de garrapatas registrado en los caninos debido a la dieta. Solución. La variable aleatoria se trata de un conteo por unidad de muestreo, que ya hemos estudiado corresponde bien a una ley de probabilidad Poisson. Para cada grupo, se puede calcular el valor promedio observado de garrapatas por individuos, obteniendo \\(\\hat{\\lambda}_C = 14{,}8\\) y \\(\\hat{\\lambda}_T = 5{,}5\\) (el cual recordamos corresponde también a la varianza). Podemos obtener entonces intervalos del 95% de confianza para ambos parámetros siguiendo el procedimiento del ejemplo anterior. Construimos el estadístico: \\[\\hat{Z} = \\frac{\\hat{\\lambda}_j - \\lambda}{\\sqrt{\\hat{\\lambda}_j/n}}\\] donde \\(j = C\\) o \\(j = T\\), dependiendo del grupo. Y luego, aplicando TLC, sabemos que \\(\\hat{Z}\\) sigue una distribución normal estándar. Sin embargo, el tamaño de la muestra es bastante pequeña por lo que es conveniente usar la distribución \\(t\\)-Student para compensar esta falta de certeza en el valor del parámetro. De forma que intervalo del 95% de confianza para el \\(\\lambda_C\\) (el control) es: \\[ \\begin{aligned} 14.8 - t_{9, 1 - \\alpha/2}\\sqrt{\\frac{14.8}{10}} &lt; &amp;\\lambda_C &lt; 14.8 + t_{9, 1 - \\alpha/2}\\sqrt{\\frac{14.8}{10}} \\\\ 12.05 &lt; &amp;\\lambda_C &lt; 17.55 \\end{aligned} \\] y para el tratamiento: \\[3.82 &lt; \\lambda_T &lt; 7.18\\] Puede observar que los intervalos para el número promedio de garrapatas por perro del control y el tratamiento no se solapan en absoluto: el control parece indicar que hay aproximadamente entre \\(2\\) y \\(3\\) veces más garrapatas en el control que en el tratamiento con la nueva dieta. Esto apoya la conclusión de que la dieta es efectiva en controlar los ectoparásitos es los caninos con un nivel de certeza del 95%. Si queremos ser más explícitos, podríamos incluso construir un intervalo de confianza para la diferencia en el número promedio de garrapatas por canino. Para ello, podemos seguir la construcción del estadístico usada anteriormente reconociendo que ahora \\(\\theta = \\lambda_T - \\lambda_C\\), diferencia que denotaremos como \\(D\\). De esta forma: \\[\\hat{t} = \\frac{\\hat{D} - D}{SE(\\hat{D})} = \\frac{(\\hat{\\lambda}_T - \\hat{\\lambda}_C) - (\\lambda_T - \\lambda_C)}{SE(\\hat{D})}\\] El valor de \\(SE(\\hat{D})\\) se calcula notando que, por los intervalos individuales, la varianza del control parece ser mayor que la del tratamiento. Entonces, podemos usar la teoría de propagación de errores como: \\[SE(\\hat{D}) = \\sqrt{\\frac{\\hat{\\lambda}_T}{10} + \\frac{\\hat{\\lambda}_C}{10}} = 1.425\\] Por lo tanto, escribimos el intervalo para el estadístico como: \\[t_{\\nu, \\alpha/2} &lt; \\frac{-9.3 - (\\lambda_T - \\lambda_C)}{1.425} &lt; t_{\\nu, 1 - \\alpha/2}\\] donde el valor de \\(\\nu\\), los grados de libertad, viene dado por la expresión: \\[\\nu=\\frac{(\\hat{\\lambda}_T/n_T + \\hat{\\lambda}_C/n_C)^2}{[(\\hat{\\lambda}_T/n_T)^2/(n_T - 1) + (\\hat{\\lambda}_C/n_C)^2/(n_C - 1)]} = \\frac{2{,}03^2}{0.277} = 14.88\\] De esta forma, se tiene que el cuantil \\(1 - \\alpha/2\\) de la distribución \\(t\\) con 14.88 grados de libertad es 2.13 y, luego de arreglar los términos en la expresión del intervalo, se obtiene el intervalo de confianza del 95%: \\[-12.34 &lt; \\lambda_T - \\lambda_C &lt; -6.26\\] Notamos que el intervalo no contiene al cero, por lo que podemos concluir con un 95% de confianza que el número promedio de garrapatas por canino es distinto en el tratamiento y el control. Además, como los límites son ambos negativos, podemos concluir también que el tratamiento con la nueva dieta resulta en un número de garrapatas por canino menor que el encontrado en el control sin la dieta. En el ejemplo anterior debemos reconocer y enfatizar ciertas consideraciones que resultan de hacer inferencias sobre la media de dos muestras independientes. En estos casos, el estadístico se construye de la manera que dijimos al final de la sección anterior, como: \\[\\frac{\\hat{D} - D}{SE(D)} = \\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{SE(\\mu_1 - \\mu_2)}\\] La distribución muestral depende de lo que tomamos como \\(SE(\\mu_1 - \\mu_2)\\). Si conocemos la varianza poblacional, entonces no necesitamos calcular \\(SE(\\mu_1 - \\mu_2)\\), y la distribución muestral es una normal estándar. Si no conocemos la varianza, debemos estimar \\(SE(\\mu_1 - \\mu_2)\\) a partir de los datos. En este caso, debemos tomar decisiones dependiendo de si podemos asumir que las varianzas son iguales o no. Si las varianzas se asumen iguales, entonces \\[SE(\\bar{X}_1 - \\bar{X}_2) = S_{pool}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}, \\quad S_{pool}^2 = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\\] donde \\(S_{pool}^2\\) es la varianza ponderada por los grados de libertad de cada muestra particular. En este caso, la distribución muestral del estadístico es una \\(t\\)-Student con \\(n_1 + n_2 - 2\\) grados de libertad. Si las varianzas se asumen distintas, entonces \\[SE(\\bar{X}_1 - \\bar{X}_2) = \\sqrt{\\frac{S^2_1}{n_1} + \\frac{S^2_2}{n_2}}\\] esto es, el error estándar es la suma de los errores estándar de las muetras por separado. En este caso, la distribución muestral del estadístico es una \\(t\\)-Student cuyos grados de libertad son: \\[\\nu=\\frac{(S_1^2/n_1 + S_2^2/n_2)^2}{[(S_1^2/n_1)^2/(n_1 - 1) + (S_2^2/n_2)^2/(n_2 - 1)]}\\] Es importante que tome en cuenta estas posibilidades como hicimos en el ejemplo anterior: las varianzas no son conocidas y las asumimos distintas, por lo que usamos el caso dos del segundo inciso. Ahora, repasamos un ejemplo que involucra un parámetro de una binomial. Ejemplo. Se hicieron medidas del pico de dos grupos de pinzones terrestres medianos que vivían en la isla de Daphne Major, una de las islas Galápagos, durante una gran sequía en 1977. Un grupo de pinzones murió durante la sequía y otro grupo sobrevivió. Los datos de supervivientes y muertos de un total de \\(36\\) pinzones fueron los siguientes: Machos: \\(20\\) muertos y \\(27\\) supervivientes. Hembras: \\(10\\) muertos y \\(9\\) supervivientes. Se desea saber si hay una diferencia en la proporción de pinzones supervivientes a la sequía con respecto al sexo, es decir, si la proporción de supervivientes machos es la misma que la proporción de supervivientes hembras. Solución. Si definimos la v. a. binomial superviviente como \\(1\\) si el pinzón sobrevivió a la sequía, y \\(0\\) de otro modo, entonces vemos que la variable, para machos y hembras, sigue una distribución binomial cuya probabilidad de éxito (supervivencia) \\(\\pi_i\\) (\\(i = M\\) o \\(H\\), dependiendo de si consideramos machos o hembras, respectivamente) puede estimarse usando la proporción observada de éxitos: \\[\\begin{cases} p_M = \\frac{27}{47} &amp; \\text{para los machos} \\\\ p_H = \\frac{9}{19} &amp; \\text{para las hembras} \\end{cases}\\] Anteriormente, ya vimos que la varianza de una binomial es \\(np(1-p)\\), de forma que el error estándar es \\(SE(p) = \\sqrt{p(1-p)/ n}\\). De esta forma tenemos que: \\[SE(p_M) = 0.072\\] y \\[SE(p_H) = 0.115\\] Primero, veamos como lucen intervalos de confianza del 95% para cada una de estas proporciones. Aplicando TLC, sabemos que \\(p\\) tiene una distribución normal, por lo que se escribe el estadístico \\[\\hat{t} = \\frac{p_i - \\pi_i}{SE(p_i)}\\] para machos, \\(i = M\\), y hembras, \\(i= H\\). Note que usamos una distribución \\(t\\), dado que el grupo con el \\(n\\) más pequeño es menor que \\(30\\) (nuestro límite para considerar aplicable el TLC con una normal estándar). De esta forma, procedemos de la manera como ya hemos visto y obtenemos los intervalos (verifique los resultados usted mismo): \\[0.429 &lt; \\pi_M &lt; 0.72\\] y \\[0.233 &lt; \\pi_H &lt; 0.714\\] Notamos dos cosas en los intervalos: La primera es que el intervalo para las hembras es de mayor longitud que el de los machos. Esto es así, ya que el \\(n\\) usado para estimar los límites del intervalo es mayor en los machos que en las hembras. Esto se traduce en que tenemos una mayor certidumbre sobre el valor del parámetro para los machos que para las hembras. Segundo, notamos que ambos intervalos contienen el \\(0{,}5\\). Esto quiere decir, que la proporción observada no se puede considerar distinta de \\(0{,}5\\) con un 95% de confianza. Biológicamente, concluiríamos que la sequía elimino a la mitad de los individuos machos y hembras del grupo de pinzones. Tercero, como ambos intervalos contienen el \\(0{,}5\\), parece posible que estas proporciones no difieran significativamente entre sí. El ultimo inciso, lo podemos verificar construyendo un intervalo para la diferencia en supervivencia de los pinzones machos y hembras en un solo intervalo. Primero escribimos la diferencia como \\(\\Delta = \\pi_M - \\pi_H\\), la cual se estima por \\(\\hat{\\Delta} = p_M - p_H\\). De esta forma, podemos escribir el estadístico como: \\[\\frac{\\hat{\\Delta} - \\Delta}{SE(\\hat{\\Delta})} = \\frac{(p_M - p_H) - (\\pi_M - \\pi_H)}{SE(\\hat{\\Delta})}\\] la cual, ya sabemos, se distribuye como una normal estándar. Se tiene que: \\[SE(\\hat{\\Delta}) = \\sqrt{\\frac{p_{M} (1 - p_{M})}{n_M} + \\frac{p_{H} (1 - p_{H})}{n_H}}\\] Introduciendo esta expresión en el estadístico y usando la distribución muestral para el intervalo, obtenemos (verifíquelo!): \\[-0.165 &lt; \\pi_M - \\pi_H &lt; 0.366\\] Noten de inmediato que este intervalo contiene al cero. Es por ello que podemos concluir que, con un 95% de confianza, no hay diferencias en la proporción de hembras y machos supervivientes a la sequía. Los ejemplos anteriores sirven para ver cómo realizar inferencia por medio de intervalos de confianza en medidas de locación para una y dos muestras independientes. Cuando las muestras son dependientes, como cuando mides antes y después de aplicar un tratamiento experimental sobre los mismos individuos, el diseño se dice que es de medidas repetidas. En estos casos, se puede usar la diferencia entre ambos estados (antes y después) como un estadístico y tratarlo como si se tratara de una sola muestra (vea el problema 3). Ahora, veremos cómo realizar inferencia por intervalos de confianza, pero sobre la varianza. 11.3.2 Inferencia sobre la varianza. Como vimos antes, las inferencias sobre la varianza se pueden hacer usando como estadístico la proporción de varianzas escalada por los grados de libertad. Veamos unos ejemplos. Ejemplo. En un estudio de los efectos sobre los bebés que tiene el consumo de cocaína durante el embarazo, se obtuvieron los siguientes datos muestrales de pesos al nacer: \\(n=190\\), \\(\\bar{x} = 2700\\) g, \\(S = 645\\) g (según datos de Cognitive Outcomes of Preschool Children with Prenatal Cocaine Exposure, de Singer et al., Journal of American Medical Association, vol. 291, núm. 20). Utilice los datos muestrales para construir un estimado del intervalo de confianza del 95% para la desviación estándar de todos los pesos al nacer de hijos de madres que consumieron cocaína durante el embarazo. Con base en el resultado, ¿parece que la desviación estándar difiere de la desviación estándar de \\(696\\) g de los pesos al nacer de hijos de madres que no consumieron cocaína durante el embarazo? Solución. Antes ya vimos que un estadístico apropiado para realizar inferencia es: \\[X^2 = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2(n-1)\\] Como vemos, dado que conocemos la distribución muestral, podemos usarla para construir el intervalo como: \\[P(\\chi^2_{\\alpha/2, n-1} &lt; X^2 &lt; \\chi^2_{1-\\alpha/2, n-1}) = 1 - \\alpha\\] Como el intervalo de confianza es del 95%, esto indica que \\(0{,}95 = 1 - \\alpha\\), por lo que \\(\\alpha = 0{,}05\\). Sustituyendo: \\[ \\begin{aligned} P(\\chi^2_{\\alpha/2, 189} &lt; X^2 &lt; \\chi^2_{1 - \\alpha/2, 189}) &amp;= P\\left(\\chi^2_{\\alpha/2, 189} &lt; \\frac{(189)(645\\text{ g})^2}{\\sigma^2} &lt; \\chi^2_{1 - \\alpha/2, 189}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{1}{\\chi^2_{1 - \\alpha/2, 189 }} &lt; \\frac{\\sigma^2}{(189)(645\\text{ g})^2} &lt; \\frac{1}{\\chi^2_{\\alpha/2, 189}}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{(189)(645\\text{ g})^2}{\\chi^2_{1 - \\alpha/2, 189}} &lt; \\sigma^2 &lt; \\frac{(189)(645\\text{ g})^2}{\\chi^2_{\\alpha/2, 189}}\\right) = 0{,}95 \\end{aligned} \\] Y el intervalo es: \\[\\frac{(189)(645\\text{ g})^2}{\\chi^2_{1 - \\alpha/2, 189}} &lt; \\sigma^2 &lt; \\frac{(189)(645\\text{ g})^2}{\\chi^2_{\\alpha/2, 189}}\\] Como \\(\\alpha=0{,}05\\), entonces \\(\\alpha/2=0{,}025\\), y se puede saber el valor del estadístico asociado a este cuantil usando una tabla de distribución chi-cuadrado, o usando qchisq(.025, 189). En este caso, \\(\\chi^2_{\\alpha/2, 189} = 152.82\\). Se procede de igual manera para el otro cuantil y se obtiene el intervalo: \\[3.4341145\\times 10^{5} &lt; \\sigma^2 &lt; 5.1451145\\times 10^{5}\\] De forma que el intervalo para la desviación estándar se obtiene sacando raíz cuadrada: \\[586.01 \\text{ g} &lt; \\sigma &lt; 717.29 \\text{ g}\\] Ahora, tratemos de responder la pregunta de la investigación ¿difiere la desviación estándar de los pesos de bebés de madres expuestas a cocaína de la desviación estándar de \\(696\\) g de bebés de madres que no se expusieron a esa droga? El intervalo de confianza construido incluye dentro de su longitud el valor de \\(696\\) g, por lo que no podemos decir que la desviación estándar de los pesos de los bebés de madres expuestas a cocaína difiere del valor de \\(696\\) g de manera significativa. Esto significa que la variabilidad de los pesos de bebés es la misma sea que provengan de madres expuestas a cocaína o no. Ahora veamos un ejemplo aplicado a la comparación de varianzas de dos poblaciones distintas. Se midieron los tamaños de \\(30\\) cráneos de gorilas hembras y \\(29\\) machos (datos de O’Higgins, 1989). La varianza de las hembras es \\(39{,}7\\), mientras que la varianza de los machos es \\(105{,}9\\). ¿Son las varianzas de los tamaños de cráneos de machos y hembras iguales? Solución. Más arriba ya vimos que cuando queremos comparar dos varianzas, podemos usar el cociente entre los estadísticos construidos para estos que siguen una distribución Chi-cuadrado: \\[\\hat{F} = \\frac{\\sigma_H^2 S_M^2}{\\sigma_M^2 S_H^2} \\sim F(n_1 -1, n_2 - 1)\\] de forma que podemos usar esta distribución muestral para construir el intervalo como: \\[P(f_{\\alpha/2, n_1-1, n_2-1} &lt; F &lt; f_{1 - \\alpha/2, n_1-1, n_2-1}) = 1 - \\alpha\\] Si elegimos un nivel de significancia de \\(0{,}05\\), esto indica que \\(1 - 0{,}05 = 0{,}95\\), por lo que el intervalo es de 95% de confianza. Sustituyendo: \\[ \\begin{aligned} P(f_{\\alpha/2, \\nu_1, \\nu_2} &lt; F &lt; f_{1 - \\alpha/2, \\nu_1, \\nu_2}) &amp;= P\\left(f_{0{,}025, 29, 28} &lt; \\frac{105{,}9\\sigma_H^2}{39{,}7\\sigma_M^2} &lt; f_{0{,}975, 29, 28}\\right) = 0{,}95 \\\\ &amp;= P\\left(f_{0{,}025, 29, 28}\\frac{39{,}7}{105{,}9} &lt; \\frac{\\sigma_H^2}{\\sigma_M^2} &lt; f_{0{,}975, 29, 28}\\frac{39{,}7}{105{,}9}\\right) = 0{,}95 \\\\ &amp;= P\\left(\\frac{1}{f_{0{,}975, 29, 28}}\\frac{105{,}9}{39{,}7} &lt; \\frac{\\sigma_M^2}{\\sigma_H^2} &lt; \\frac{1}{f_{0{,}025, 29, 28}}\\frac{105{,}9}{39{,}7}\\right) = 0{,}95 \\end{aligned} \\] Y el intervalo es: \\[\\frac{1}{f_{0{,}975, 29, 28}}\\frac{105{,}9}{39{,}7} &lt; \\frac{\\sigma_M^2}{\\sigma_H^2} &lt; \\frac{1}{f_{0{,}025, 29, 28}}\\frac{105{,}9}{39{,}7}\\] Para la distribución \\(F\\), \\(f_{0{,}025, 29, 28} = 0.47\\). Se procede de igual manera para el otro cuantil y se obtiene el intervalo: \\[1.26 &lt; \\frac{\\sigma_M^2}{\\sigma_H^2} &lt; 5.63\\] ¿Qué nos dice el intervalo sobre las varianzas de los cráneos de gorilas machos y hembras? Si las varianzas fueran iguales, entonces el cociente de las varianzas sería igual a \\(1\\). Notamos que el \\(1\\) no se encuentra dentro del intervalo, por lo que podemos decir, con un \\(95\\)% de confianza, que la varianza de los cráneos de gorilas machos y hembras son distintas. No solo eso, sino que además, como los límites del intervalo son mayores a \\(1\\), entonces la varianza de los cráneos de los gorilas machos es mayor que la varianza de los cráneos de gorilas hembras. "],["ejercicios.-1.html", "11.4 Ejercicios.", " 11.4 Ejercicios. Control del plomo en el aire. A continuación se listan las cantidades de plomo medidas (en microgramos por metro cúbico o \\(\\mu g\\) \\({m}^{-3}\\)) en el aire: \\(5{,}40, 1{,}10, 0{,}42, 0{,}73, 0{,}48\\), y \\(1{,}10\\). La Environmental Protection Agency estableció un estándar de calidad del aire para el plomo de \\(1{,}5\\) \\(\\mu g\\) \\({m}^{-3}\\). Las medidas que se presentan abajo se registraron en el edificio 5 del World Trade Center en diferentes días, inmediatamente después de la destrucción causada por los ataques terroristas del 11 de septiembre de 2001. Después del colapso de los dos edificios hubo una gran preocupación por la calidad del aire. Utilice los valores dados para construir un estimado del intervalo de confianza del 95% para la cantidad media de plomo en el aire. ¿Hay algo en este conjunto de datos que sugiera que el intervalo de confianza tal vez no sea muy bueno? Explique. Se ha realizado un estudio sobre la velocidad en vuelo de diversas especies de pájaros. El propósito era comparar las velocidades del pelícano pardo y el ostrero americano. Se cronometró una muestra de \\(9\\) pájaros pardos y \\(12\\) pájaros ostreros, volando con el viento de costado con una velocidad de viento de \\(5\\) a \\(8\\) millas h\\({}^{-1}\\), y se obtuvo que el pájaro pardo vuela, en promedio, a \\(26{,}05 \\pm 6{,}34\\) millas h\\({}^{-1}\\), y el ostrero a \\(30{,}19 \\pm 3{,}20\\) millas h\\({}^{-1}\\). Construya un intervalo de confianza del 95% para la proporción de varianzas. Un fármaco aún más eficiente contra ectoparásitos ha sido desarrollado. Al momento de realizar el experimento para probar su efectividad en caninos, solo se contaban con \\(14\\) perros. Entonces, en lugar de dividir al conjunto en dos grupos, se decidió que cada perro fuera su propio control, midiendo el número de garrapatas antes de darles el medicamento y después de pasar un tiempo con el mismo, observando el cambio en el número de garrapatas de cada individuo. Los resultados se muestran en la tabla a continuación. Construya un intervalo de confianza apropiado y úselo para determinar si el nuevo medicamento es capaz de disminuir el número de ectoparásitos en caninos. Antes Despues 13 8 14 10 18 6 14 7 13 8 25 9 19 8 15 9 17 6 14 13 10 10 15 8 12 10 12 10 Se tiene un lote de semillas certificadas que se usaran en un ensayo de eficacia de un biocontrolador que tiene propiedades de potenciación del crecimiento. Previo al ensayo, se desea determinar el porcentaje de germinación del lote, para así tener información suficiente para diseñar el ensayo de eficacia. Para esto se decide sembrar 30 semillas en bolsas de vivero con tierra común, regándolas con agua cada día por 15 días, resultando en un porcentaje de germinación del 70%. Construya un intervalo de confianza del 95% para el porcentaje de germinación. Se tiene otro lote de semillas más recientes, cuyo porcentaje de germinación fue del 88% luego de un ensayo con 25 semillas. Realice un intervalo de confianza para la diferencia entre el porcentaje de germinación del nuevo lote con respecto al lote viejo. Concluya sobre qué semillas usaría para el ensayo de eficacia y por qué. Luego de varios meses de investigación y modificaciones al proceso de producción, usted ha logrado aumentar la cantidad de proteína celular obtenida de biorreactores sumergidos de A. oryzae, donde inicialmente se tenía un rendimiento de \\(8 \\pm 3\\) kg L\\({}^{-1}\\) (basado en una muestra de 3 procesos seguidos durante esos meses de pruebas). Medidas obtenidas para el nuevo proceso (\\(n=3\\)) muestran un incremento en la producción a \\(11{,}5 \\pm 5\\) kg L\\({}^{-1}\\) (asuma que las medidas de dispersión reportada corresponden a desviaciones estándar). Construya IC95% para las desviaciones estándar de ambos procesos y comente sobre los resultados ¿Cree usted adecuado el adoptar el nuevo proceso productivo? Construya un IC95% para la proporción de varianzas de ambos procesos y comente los resultados ¿Cuáles cree usted serían los pasos siguientes a seguir para aumentar la producción de proteína celular si se desea adoptar el segundo proceso? Un investigador busca poder inducir la formación de callos a partir de semillas de moringa para la obtención de metabolitos secundarios de interés farmacéutico. El trabajo para poder obtener un primer biorreactor piloto de callos es largo y requiere de la caracterización del proceso de inducción. Uno de los pasos requiere el contabilizar el número de aberraciones cromosómicas por célula que aparecen como consecuencia del tratamiento con los factores de crecimiento (FC) usados. Luego de inducir la callogénesis usando dos concentraciones de FC (una alta y otra baja), se obtuvieron los siguientes resultados: Control: 9,5,4,6,4,3,5,2,4. FC Baja: 12,8,5,7,9,6,9,6,10,6,8. FC Alta: 13,15,7,16,15,11,6,8,15,8. Construya un intervalo de confianza para la diferencia de cada tratamiento de FC con respecto al control. Si se busca controlar el número de aberraciones cromosómicas que aparecen, ¿Qué tratamiento usaría usted? Siguiendo con el ejemplo de los grupos de pinzones de la isla Daphne Major sobrevivientes y muertos durante la sequía de 1977, se registraron las masas corporales de estos, tanto para machos como para hembras. Los resultados, presentados como medias \\(\\pm\\) desviación estándar en gramos, se muestran en la tabla a continuación. Utilice los datos para construir intervalos de confianza apropiados para verificar si hay una diferencia en la masa corporal de los pinzones que sobrevivieron con respecto a los que no sobrevivieron, tanto para machos como para hembras. En caso de encontrar diferencias, ¿por qué cree que las hay? De un significado biológico a sus resultados. Estado Sexo n Media SD Superviviente Macho 27 17.63 1.66 Superviviente Hembra 9 17.06 1.83 Muerto Macho 20 16.12 1.42 Muerto Hembra 10 15.60 1.24 "],["introducción-al-contraste-de-hipótesis..html", "12 Introducción al Contraste de Hipótesis.", " 12 Introducción al Contraste de Hipótesis. En la inferencia estadística, el contraste de hipótesis sirve como método que facilite el proceso de toma de decisión en base a datos recolectados de una población. Su finalidad es producir conclusiones sobre la población partiendo de una hipótesis particular, recordando siempre que las decisiones tomadas y conclusiones alcanzadas son entendidas en un sentido estadístico, es decir, tienen una medida de incertidumbre asociada. Todo proceso de contraste de hipótesis parte de una conjetura inicial sobre el sistema o fenómeno en estudio. Por ejemplo, al estudiar el efecto que tiene la contaminación de un pequeño lago sobre la densidad de peces presentes en el mismo, los investigadores pueden conjeturar que la contaminación causa una disminución de la capacidad de los peces para sobrevivir, y por lo tanto esperar un descenso de la densidad de peces. En este punto, el investigador diseña un experimento y recolecta datos para probar su conjetura. Para ello, el investigador definiría la v. a. que sirve como medida sensible de lo que desea probar (en el caso de nuestro ejemplo, el investigador podría contabilizar el número de peces en el lago) y establecería sus conjeturas en un esquema formal para probar si su conjetura es cierta o no. La forma en la que se formaliza la conjetura inicial y se pone a prueba, cuantificando a la vez la incertidumbre asociada al método, es lo que es objeto de este capítulo. Comenzamos definiendo con propiedad lo que es una hipótesis estadística. La hipótesis estadística es una aseveración o conjetura respecto a una o más poblaciones que se estudian. Esta puede ser verdadera o no y debe ser formalizada en un planteamiento matemático concreto. En este punto, debemos realizar una distinción entre una hipótesis experimental y una hipótesis estadística. La primera es una oración que resume la conjetura del investigador sobre el estado del sistema en estudio. La segunda hace referencia al establecimiento formal (matemático) de la hipótesis experimental. En nuestro ejemplo anterior, la hipótesis experimental sería la densidad de peces en el lago ha disminuido por efecto de la contaminación del cuerpo acuático. Para probar esta conjetura, se debe definir una v. a. que permita corroborar esto: estimando el número de peces y compararlo con un valor de referencia sería una forma, pero también se puede hacer midiendo alguna variable relacionada a la densidad de peces. Por ejemplo, si la densidad de fitoplancton es dependiente de la densidad de peces, el medir la densidad de fitoplancton puede servir como una medida de la cantidad de peces en el lago (aunque claro está, habría que augurarse que el cambio en la densidad de fitoplancton es debido solamente al cambio en la densidad de peces y no, por ejemplo, que esta también se vea afectada por la contaminación del lago). En este caso, si se usa al estimado del número de peces, entonces la hipótesis estadística sería verificar si este valor es menor que el esperado según valores previos medidos para el algo. Al plantear una hipótesis estadística, se plantea en conjunto con esta la negación de la hipótesis: aquella que es la oposición a lo que se piensa está ocurriendo al fenómeno en estudio. Si planteamos la hipótesis de que la densidad de peces ha disminuido, también habremos planteado la hipótesis de que la densidad de peces no ha cambiado o ha aumentado, la cual se opone a nuestra conjetura inicial. Una de estas hipótesis es la correcta, y es el contraste de estas hipótesis los que nos da una medida de que tan probable es que nuestra conjetura sea correcta, y podamos tomar una decisión. Una decisión tomada en base al contraste de hipótesis, está plagada de incertidumbre. La decisión tomada esta enlazada a una medida de incertidumbre, por lo que es posible cometer errores en las conclusiones. Dado que el verdadero estado del fenómeno en estudio está oculto a nosotros (en general, no podemos saber exactamente el valor de un parámetro), y que solo podemos estimar basado en muestras de un tamaño limitado, podemos incurrir a errores en las conclusiones solo por azar. Cualquier afirmación estadística, por tanto, debe ser establecida en conjunto con una medida de que tan seguros estamos de que la decisión tomada es correcta. Ejemplo. Se examinó la influencia del fármaco succinilcolina sobre los niveles de circulación de andrógenos en la sangre. Se obtuvieron muestras de sangre de venados salvajes inmediatamente después de recibir una inyección intramuscular de succinilcolina con dardos de un rifle de caza. Treinta minutos después se obtuvo una segunda muestra de sangre y después los venados fueron liberados. Los niveles de andrógenos de 15 venados al momento de la captura y 30 minutos más tarde, medidos en nanogramos por mililitro (ng \\({\\text{mL}}^{-1}\\)), se presentan en la tabla. Conc. Adrogenos (ng/mL) Venado Al inyectar 30 min después 1 2.76 7.02 2 5.18 3.10 3 2.68 5.44 4 3.05 3.99 5 4.10 5.21 6 7.05 10.26 7 6.60 13.91 8 4.79 18.53 9 7.39 7.91 10 7.30 4.85 11 11.78 11.10 12 3.90 3.74 13 26.00 94.03 14 67.48 94.03 15 17.04 41.70 El enunciado permite establecer una hipótesis experimental sobre la cual podemos deducir trabajaron los investigadores: la succinilcolina tiene un efecto sobre los niveles de andrógenos en la sangre de los venados (noten que, no necesariamente tiene que haber una dirección, esto es, el enunciado no dice nada sobre si el efecto de la succinilcolina es disminuir o aumentar la cantidad de andrógenos en la sangre, solo dice que hay un cambio y ya). De los métodos de estimación sabemos que la cantidad de andrógenos al momento de la inyección es de \\(11.81 \\pm 16.64\\). Mientras que la concentración de andrógenos 30 minutos después de la inyección fue de \\(21.65 \\pm 30.92\\). Podemos construir un intervalo de confianza para las diferencias entre la concentración de andrógenos antes y después, para cada venado. \\[-0.38 &lt; D &lt; 20.08\\] El intervalo de confianza parece indicar que no hay una diferencia significativa en la cantidad de andrógenos en la sangre luego de la inyección de succinilcolina (¿Por qué?). El planteamiento de un sistema de experimentación comienza con una conjetura sobre lo que se desea estudiar: en el ejemplo anterior, se desea saber si la succinilcolina es capaz de disminuir los niveles de andrógenos en la sangre. El intervalo de confianza nos da, con una medida de certeza, una conclusión sobre el efecto de la succinilcolina. Sin embargo, podemos realizar la inferencia de otro modo. Nos preocuparemos por obtener una muestra de tamaño \\(n\\) que esté descrita por los valores \\(x_1, x_2, \\ldots, x_n\\) de una variable aleatoria \\(X\\). Suponemos que cada valor es independiente de los demás. Por tanto, podemos conceptualizar estos valores como una secuencia \\(X_1, X_2, \\ldots, X_n\\) de variables aleatorias independientes e idénticamente distribuidas, cada una de las cuales tiene la misma distribución que \\(X\\). Partiendo de la conjetura inicial, y su negación, las cuales podemos escribir como: \\[ \\begin{aligned} \\text{Conjetura}: &amp; \\text{ La succinilcolina modifica la concentración de andrógeno en venados.} \\\\ \\text{No hay cambio}: &amp; \\text{ La succinilcolina no cambia la concentración de andrógeno en venados.} \\end{aligned} \\] Estas hipótesis experimentales se formalizan usando la variable aleatoria usada para probarlas, en este caso, la cantidad de andrógenos en sangre. Esta variable define un conjunto posible de valores que puede adoptar (las concentraciones, por ejemplo, solo pueden medirse como valores reales positivos). Estos valores posibles corresponden al conjunto de todas las posibles hipótesis estadísticas que se pueden hacer con respecto a esta v. a. La formalización comienza con el establecimiento de este conjunto de hipótesis posibles al que llamamos \\(\\Theta\\); y luego, se seleccionan dos hipótesis \\(\\Theta_0 \\subseteq \\Theta\\) y \\(\\Theta_1 \\subseteq \\Theta\\) tales que: \\[\\Theta_0 \\cup \\Theta_1 = \\Theta, \\qquad \\Theta_0 \\cap \\Theta_1 = \\emptyset\\] Al conjunto \\(\\Theta_0\\) se le conoce como hipótesis nula, \\(H_0\\); y al conjunto \\(\\Theta_1\\) se le conoce como hipótesis alternativa, \\(H_1\\). Las expresiones anteriores lo que indican es que las hipótesis nula y alternativa cumplen dos propiedades: * La de la izquierda dice que no hay otras hipótesis posibles además de la nula y la alternativa. * La de la derecha nos dice que no existen ambigüedades en la toma de decisión basada en hipótesis, ya que ambas, hipótesis nula y alternativa, son mutuamente excluyentes y se escribe formalmente: \\[ \\begin{aligned} H_0: &amp; \\Theta_0 \\subseteq \\Theta \\\\ H_1: &amp; \\Theta_1 \\subseteq \\Theta \\end{aligned} \\] Otra propiedad importante es que si se hace un contraste para obtener una conclusión con respecto a un parámetro \\(\\theta\\), el conjunto \\(\\Theta_0\\) debe contener la conjetura de que no hay un cambio en el valor esperado del parámetro. En el ejemplo sobre el efecto de la succinilcolina sobre la concentración de andrógeno en venados, vimos que la diferencia en la concentración de andrógenos antes y después de la inyección de succinilcolina sirve como variable aleatoria para realizar inferencias. De forma que podemos definir el conjunto de todas las posibles hipótesis como \\(\\Theta = \\{D \\in\\mathbb{R}\\}\\). Entonces: \\[ \\begin{aligned} H_0: &amp; \\{D \\in \\mathbb{R}\\vert D = 0\\} \\\\ H_1: &amp; \\{D \\in \\mathbb{R}\\vert D \\ne 0\\} \\end{aligned} \\] o, de forma más compacta: \\[ \\begin{aligned} H_0: &amp; D = 0 \\\\ H_1: &amp; D \\ne 0 \\end{aligned} \\] La forma de las hipótesis planteadas para un experimento es algo subjetiva, dado que su proposición depende de lo que sabe el investigador y su experiencia. A mayor información se tiene, el contraste es más potente (más adelante veremos a que nos referimos con esto). Una parte importante del contraste de hipótesis es que se asume que la hipótesis nula, la que especifica que no hay un cambio en el parámetro sobre el que se hace inferencia, es cierta. Esta es una suposición del esquema de contraste que facilita la obtención de una distribución muestral para el estadístico con el que se prueba el contraste. En resumen: La hipótesis a probar es la hipótesis nula. Esta es la que se toma como cierta, como la proposición de que no hay cambio alguno, y debemos utilizar la información disponible como evidencia para respaldar nuestra conjetura alternativa. La hipótesis nula nunca se acepta, solo no se rechaza. No es que la condición de igualdad no se mantenga, sino que la información que se tiene no es capaz de refutarla. Ahora, veamos cómo podemos usar un estadístico para probar nuestra hipótesis. "],["estadístico-de-prueba..html", "12.1 Estadístico de Prueba.", " 12.1 Estadístico de Prueba. El estadístico de prueba se construye a partir de la información que se tiene del parámetro poblacional \\(\\theta\\), es decir, usando el estimador \\(\\hat{\\theta}\\) calculado a partir de la muestra de tamaño \\(n\\). Ya hemos construido estadísticos con los cuales hacer inferencias sobre distintos estimadores. Para desviaciones con respecto a un valor promedio: \\[\\hat{Z} = \\frac{\\hat{\\theta} - \\theta}{SE(\\theta)} \\sim N(0,1)\\] o, en caso de no tener un \\(n\\) lo suficientemente grande para tener un buen estimador de \\(SE(\\hat{\\theta})\\), se usa: \\[\\hat{t} = \\frac{\\hat{\\theta} - \\theta}{\\hat{SE(\\hat{\\theta})}}\\] que se distribuye como una \\(t\\)-Student con grafos de libertad que dependen del tamaño de la muestra (véase los ejemplos más adelante). Para contrastes de hipótesis sobre la varianza: \\[\\hat{X}^2 = \\frac{(n-1) Var(\\hat{\\theta})}{Var(\\theta)} \\sim \\chi^2(n-1)\\] o, si se trata de hacer inferencia sobre la proporción de dos varianzas: \\[\\hat{F} = \\frac{Var(\\hat{\\theta_1})Var(\\theta_2)}{Var(\\hat{\\theta_2})Var(\\theta_1)} \\sim F(n_1 - 1, n_2 - 1)\\] Ejemplo. Siguiendo con nuestro ejemplo de la concentración de andrógenos luego de una inyección de succinilcolina, podemos trabajar con las diferencias \\(d_i\\) (para \\(i=1, \\ldots, n\\)) entre la concentración de andrógenos antes y 30 min después de la inyección del metabolito. Cada \\(d_i\\) es la realización de una variable aleatoria \\(D_i\\) independiente, cuya ley de probabilidad es la misma para todo \\(i=1,\\ldots, n\\). Podemos construir un estadístico basado en estas diferencias para verificar si de verdad hay un cambio en la concentración de andrógenos, tal como hicimos antes para construir IC: \\[\\hat{t} = \\frac{\\bar{d} - D}{\\hat{S}_d/\\sqrt{n}}\\] Como dijimos antes, el contraste se hace bajo la suposición de que la hipótesis nula es cierta. En este caso, según la hipótesis nula \\(D = 0\\) (vea la sección anterior), por lo que podemos obtener un valor para el estadístico: \\[\\hat{t} = \\frac{\\bar{d}}{\\hat{S}_d/\\sqrt{n}} = \\frac{9.85}{4.77} = 0.14\\] Este estadístico es un valor observado de los posibles valores que puede adoptar el estadístico de acuerdo a la ley de probabilidad que sigue, que en este caso sabemos es una \\(t\\)-Student con \\(n-1\\) grados de libertad. Y dado que se tiene una distribución muestral, podemos obtener un valor de probabilidad asociado con el cual tomar una decisión sobre si la succinilcolina disminuye o no la cantidad de andrógenos en la sangre de los venados. "],["pz-ge-hatz.html", "12.2 \\(P(Z \\ge \\hat{Z}) = ???\\)", " 12.2 \\(P(Z \\ge \\hat{Z}) = ???\\) En esta sección nos interesa encontrar valores de probabilidad asociados a los estadísticos calculados a partir de los datos. Estas probabilidades nos sirve para tomar decisiones basado en que tan probable es que la hipótesis nula sea cierta (que es la que asumimos es la hipótesis que es cierta). 12.2.1 Región crítica Al definir las hipótesis y calcular un estadístico de prueba, se define un conjunto de valores que puede tomar este último que permiten tomar una decisión sobre la hipótesis que la evidencia está apoyando. Se puede escoger un valor crítico para el cual se puede definir la región crítica (que no es más que un subconjunto de valores del parámetro \\(\\theta\\)), la cual permite rechazar la hipótesis nula si el parámetro \\(\\theta\\) cae en esa región (pertenece al subconjunto). Al igual que hicimos para los intervalos de confianza, se elige un valor umbral \\(\\alpha\\) que determina el criterio de decisión. Esto se logra por medio del uso del \\(\\alpha\\)-cuantil de la distribución muestral del estadístico \\(\\Theta\\) que construimos, que denotamos como \\(\\Theta_\\alpha\\). Por ejemplo, si la distribución del estadístico es una normal estándar, de forma que habremos calculado con la evidencia disponible y nuestra conjetura un valor estimado de \\(\\hat{Z}\\), elegimos entonces el cuantil \\(z_\\alpha\\) para tomar una decisión: Si \\(\\hat{Z} &lt; z_\\alpha\\) entonces la desviación observada (del estimador con respecto a muestra conjetura del valor de \\(\\theta\\)) no es lo suficientemente grande para rechazar la hipótesis nula, y tomamos la decisión de no rechazarla. En este caso, se dice que la desviación observada es una que podríamos esperar por azar solamente. Si \\(\\hat{Z} \\ge z_\\alpha\\) entonces la desviación observada es tan grande o mayor que la que esperaríamos solo por azar, por lo que debe haber un efecto externo (lo que nuestra hipótesis experimental plantea) que está cambiando la ley de probabilidad según la hipótesis nula, por otra expresada en nuestra alternativa, y por lo tanto, debemos rechazar la hipótesis nula en favor de esa alternativa. Notamos que no hay ambigüedades en el criterio de decisión: o rechazamos o no rechazamos la hipótesis nula. El valor de \\(\\alpha\\), como ya mencionamos en el capítulo anterior, se conoce como nivel de significancia. En el contexto del contraste de hipótesis se conoce como la probabilidad de rechazar la hipótesis nula siendo esta cierta, es decir, es una probabilidad de equivocarnos sobre el contraste (más adelante hablaremos de este y otro tipo de error que es importante considerar), y se escribe: \\[\\alpha = P(\\text{Rechazar } H_0 \\vert H_0 \\text{ verdadera})\\] Este valor se elige tan pequeño como se quiera, pero no tanto como para afectar la validez de nuestras conclusiones al realizar un contraste. Al igual que con los intervalos de confianza, se suelen elegir valores de \\(0{,}1\\), de \\(0{,}05\\), o de \\(0{,}01\\), de acuerdo a la importancia de cometer un error en el contraste. En la figura 12.1 se muestra una región crítica para la distribución normal estándar, para un contraste hipotético de la forma \\(H_0 : \\theta = \\theta_0\\) y \\(H_1: \\theta \\ne \\theta_0\\). A este tipo de contraste, en el cual las hipótesis se establecen en términos de igualdad y diferencia, se le conoce como contrastes bilaterales, dado que la desviación observada se espera caiga por encima o por debajo del valor esperado. En estos casos, la región crítica se divide en dos regiones a ambos lados del valor esperado de la desviación. Los contrastes pueden tener dirección cuando esperamos que el parámetro \\(\\theta\\) sea mayor o menor que el valor esperado según la hipótesis nula, \\(\\theta_0\\). Esto es, cuando se plantean contrastes de la forma \\(H_0 : \\theta\\le \\theta_0\\) y \\(H_1: \\theta &gt; \\theta_0\\); o contrastes de la forma \\(H_0 : \\theta \\ge \\theta_0\\) y \\(H_1: \\theta &lt; \\theta_0\\). En estos casos, se dice que el contraste es unilateral. En estos casos, solo se tiene una región crítica a la derecha o izquierda de la distribución (la dirección depende del símbolo usado en la definición de la hipótesis alternativa). Más adelante hablaremos de contrastes unilaterales. Las regiones sombreadas en la figura corresponden a la probabilidad acumulada \\(\\alpha\\). Como la suma del área de ambas regiones es \\(\\alpha\\), y dada la simetría de la distribución normal, eso quiere decir que la probabilidad acumulada de la región crítica a la derecha, la región sombreada a la derecha, es de \\(\\alpha/2\\); y de igual forma, en la región crítica de la izquierda, la probabilidad acumulada, representada por la región sombrada a la izquierda, es de \\(\\alpha/2\\). Figure 12.1: Región crítica para una distribución normal estándar. Note que la región crítica corresponde a los valores de Z de la distribución cuya probabilidad acumulada es \\(lpha\\). De esta forma, se definen dos regiones críticas con dos valores críticos, \\(z_{\\alpha/2}\\) define el valor crítico a la izquierda, mientras que \\(z_{1 - \\alpha/2}\\) define el valor crítico a la derecha de la distribución. En el caso particular de la figura 12.1, si el valor calculado del estadístico cae en alguna de las regiones sombreada, es decir, si \\(\\hat{Z} \\le z_{\\alpha/2}\\) o si \\(\\hat{Z} \\ge z_{1 - \\alpha/2}\\), se decide rechazar la hipótesis nula, de lo contrario, se mantiene. Ejemplo. Siguiendo con nuestro ejemplo del efecto de la succinilcolina sobre la cantidad de andrógenos en venados. Ya antes establecimos las hipótesis en términos del parámetro \\(D\\), la diferencia entre la cantidad de andrógenos antes y después de la inyección de succinilcolina, y calculamos un estadístico para esta diferencia obteniendo el valor de \\(\\hat{t} = 0.14\\) el cual sabemos se distribuye como \\(t\\)-Student con \\(n - 1 = 15 - 1 = 14\\) grados de libertad. Para tomar una decisión seleccionamos un valor adecuado de \\(\\alpha\\) que nos de cierta seguridad sobre nuestra decisión. Podemos elegir \\(\\alpha = 0{,}05\\) y ahora definir la región crítica: para esto, notamos que la hipótesis alternativa no tiene dirección, esto es, no esperamos que la desviación de \\(D\\) con respecto al valor esperado de cero, sea mayor a cero o menor a cero, por lo que la región crítica debe estar a ambos lados del valor esperado, con probabilidades acumuladas que en total sumen \\(\\alpha\\). Se usa entonces como valores críticos los cuantiles \\(t_{0{,}05 / 2; 14} = t_{0{,}025; 14}\\) y \\(t_{1 - 0{,}05 / 2; 14} = t_{0{,}975; 14}\\). Estos cuantiles se pueden obtener en R usando la función qt que da los cuantiles de la distribución. Se obtiene entonces \\(t_{0{,}025; 14}\\) como qt(.025, 14) cuyo resultado es -2.14. Para el otro cuantil, se obtiene como qt(.975, 14) cuyo resultado es 2.14. Notamos que nuestro valor calculado cae fuera de la región critica: es mayor que el cuantil t_{0{,}025; 14}$ y menor que el cuantil t_{0{,}975; 14}$, por lo que no podemos, según nuestro criterio de decisión, rechazar la hipótesis nula en favor de la alternativa. Esto nos lleva a concluir que la inyección de succinilcolina no tiene ningún efecto sobre la concentración de andrógenos en la sangre de los venados, dado que la desviación observada es tan pequeña, que no difiere de la que obtendríamos por azar. 12.2.2 El P-valor como criterio de decisión. También es posible asociar un valor de probabilidad específica a obtener un estadístico tan grande como el calculado usando la función de distribución acumulada. Este valor es: \\[p = P(\\Theta \\ge \\hat{\\Theta})\\] donde \\(p\\) no debe confundirse con una proporción, sino que es la probabilidad de obtener un valor del estadístico \\(\\Theta\\) tan grande o mayor que \\(\\hat{\\Theta}\\) solo por azar. Este valor de probabilidad sirve como medida de que tan cierta es la hipótesis nula Dada la facilidad con la cual es posible calcular valores de probabilidad hoy en día usando paquetes estadísticos, siempre podemos obtener la probabilidad acumulada de cualquier estadístico. Ejemplo. Anteriormente, calculamos que el estadístico \\(\\hat{t}\\) calculado para las diferencias entre las concentraciones de andrógenos al momento y 30 minutos después de la inyección fue 0.14. Podemos obtener la probabilidad acumulada de obtener una desviación tan grande como esa, solo por azar, usando la función pt, en R. Escribimos: \\[1 - P(t \\ge 0.14) = 0.4462\\] donde se usó pt(0{,}14, 14) para calcular la probabilidad acumulada hasta \\(\\hat{t} = 0{,}14\\). Este valor nos dice que la probabilidad de encontrar una desviación en la concentración de andrógenos tan grande o mayor como la observada es bastante grande, por lo que se esperaría por azar. En este caso, tampoco rechazamos la hipótesis nula, pero esta vez lo hacemos usando como criterio el valor de probabilidad acumulada. El problema de los valores marginales. El uso del nivel de significancia como criterio de decisión es bastante útil para tomar decisiones acerca de un contraste que queremos realizar. Sin embargo, debido a la naturaleza estocástica de los experimentos aleatorios, hay casos donde es más difícil llegar a una decisión razonable. Por ejemplo, si realizáramos un contraste hipotético cualquiera a partir de datos recolectados en un experimento, realizaríamos el cálculo del estadístico y lo compararíamos con el valor crítico. Este valor crítico nos dice que la probabilidad acumulada desde este valor hasta infinito es igual a \\(\\alpha\\), esto es: \\[P(Z \\ge Z_{crtitico}) = \\alpha\\] Entonces, si en nuestro experimento hipotético, nuestro estadístico calculado cae en la región crítica a una distancia considerable del valor crítico, no tendríamos problema en rechazar la hipótesis nula. Desde el punto de vista del \\(p\\) valor como criterio de decisión, dicho estadístico tendría una probabilidad asociada mucho menor al valor de \\(\\alpha\\). Ahora, suponga que el estadístico no dista mucho del valor crítico, de forma que su probabilidad no es muy diferente de \\(\\alpha\\). Seamos más prácticos: digamos que en nuestro experimento hipotético usted está trabajando con un nivel de significancia de \\(0{,}05\\) para un contraste bilateral en el que piensa usar la distribución normal estándar para comparar du estadístico calculado, cuyo valor encuentra es de \\(\\hat{Z} = 2{,}00\\) que tiene una probabilidad asociada de \\(p=0{,}0228\\). Para un contraste de este tipo usted sabe, por lo discutido antes, que el valor crítico es \\(1{,}96\\), que tiene una probabilidad asociada de \\(\\alpha/2 = 0{,}025\\). Bajo este caso hipotético particular (que suele ocurrir en la práctica) podríamos pensar en rechazar la hipótesis nula. Después de todo, tanto el valor calculado como la probabilidad del mismo son menores a los valores críticos. Sin embargo, debemos recordar que nuestro valor estimado del estadístico es solo una observación aleatoria del verdadero valor del estadístico, lo cual implica que tendríamos que pensar en que tan diferente es nuestro valor del estadístico del valor crítico: estando muy cerca del valor crítico no nos da mucha seguridad de que sean distintos. El argumento dado en el ejemplo anterior nos hace darnos cuenta de la dificultad de realizar inferencia usando valores marginales, valores que caen cerca del margen de la región crítica. En estas situaciones, se debe sopesar la necesidad de concluir en una u otra dirección contra las consecuencias de cometer un error de decisión. Si las consecuencia de la decisión son muy relevantes, como lo puede ser el gasto de dinero y/o esfuerzo de investigación, o más importante aún, la salud y supervivencia de algún ser vivo, se ha de optar por una decisión cautelosa que minimice los costos, materiales o humanos, de equivocarse. El último párrafo pone de manifiesto un problema importante que tiene que ver con el control de la tasa de errores que cometemos. En este sentido, necesitamos precisar estos errores con mayor exactitud. "],["beta-ptextrechazar-h_0-vert-h_1text-cierta.html", "12.3 \\(1 - \\beta = P(\\text{Rechazar }H_0 \\vert H_1\\text{ cierta})\\)", " 12.3 \\(1 - \\beta = P(\\text{Rechazar }H_0 \\vert H_1\\text{ cierta})\\) 12.3.1 Errores de Decisión. Como ya hemos mencionado, desde que iniciamos el estudio de la inferencia estadística, hemos dicho que en un contexto estadístico no hay verdades absolutas. Todas nuestras conclusiones acerca de un fenómeno de estudio se hacen tomando en cuenta cierta incertidumbre inherente al experimento y que debemos cuantificar de alguna manera. Al reportar estas conclusiones, y los resultados de nuestros análisis, debemos estar conscientes y controlar, en la medida de lo posible, dos tipos de errores en la toma de decisiones, los cuales se resumen bien en la tabla ??. Estado Real \\(H_0\\text{ cierta.}\\) \\(H_0\\text{ falsa.}\\) No rechazar \\(H_0\\) Decisión Correcta Error Tipo II Rechazar \\(H_0\\) Error Tipo I Decisión Correcta Como podemos ver en la tabla, si el verdadero estado del experimento es que la hipótesis nula es cierta y, con la evidencia recolectada, decidimos no rechazar \\(H_0\\), entonces no tenemos mayor problema ya que habremos llegado a la decisión correcta. De la misma forma ocurre si el estado del experimento es que \\(H_1\\) es cierta y decidimos rechazar \\(H_0\\). El problema surge cuando tomamos decisiones equivocadas, las cuales son de dos tipos. Decimos que cometemos error tipo I cuando decidimos rechazar la hipótesis nula siendo la hipótesis nula cierta, la cual ocurre con una probabilidad: \\[P(\\text{Rechazar }H_0 \\vert H_0 \\text{ cierta}) = \\alpha\\] En este caso, habremos recolectado una muestra muy desviada de lo esperado solo por azar. Como observamos, esta corresponde al nivel de significancia que establecemos al diseñar el experimento. Decimos que cometemos error tipo II cuando decidimos mantener la hipótesis nula siendo la alternativa verdadera, lo cual ocurre con probabilidad: \\[P(\\text{No rechazar }H_0 \\vert H_1 \\text{ cierta}) = \\beta\\] En la figura 12.2 se representan ambos errores gráficamente. Figure 12.2: Representación gráfica de los errores de decisión asociados al contraste de hipótesis. A la izquierda el error tipo I y a la derecha el error tipo II. Estos dos tipos de errores son inherentes a cualquier contraste de hipótesis, y los experimentos deben diseñarse de tal forma que pueda controlarse la probabilidad de cometer estos errores, dada la relevancia de las consecuencias que resultan de cometer un error de estos tipos. Por ejemplo, antiguos estudios muestran que el germicida DDT puede acumularse en el cuerpo. En 1965 la concentración media de DDT en las partes grasas del cuerpo de las personas en Estados Unidos fue de \\(9\\) ppm. Se espera que, como resultado de estrictos controles, esta concentración haya decrecido. \\[ \\begin{aligned} H_0: &amp; \\mu \\ge 9\\text{ ppm} \\\\ H_1: &amp; \\mu &lt; 9 \\text{ ppm} \\end{aligned} \\] Si rechazamos \\(H_0\\) siendo esta cierta, eso quiere decir que la concentración de DDT es de 9 ppm o mayor pero decidimos que no lo es, por lo que cometemos error tipo I. Debido a esto, pensaríamos que los programas de control han sido efectivo y se tomaría la decisión de seguir gastando dinero en estos, aun cuando no son efectivos en absoluto, retardando la aplicación de nuevos controles que si pudiesen ser efectivos. Si fallamos en rechazar \\(H_0\\), siendo esta falsa, cometemos error tipo II, y concluiríamos que los controles no están siendo efectivos cuando en realidad si lo son. En este caso se desecharía un programa de control exitoso controlando las concentraciones de DDT, en el cual se ha invertido recursos importantes para su aplicación. El ejemplo anterior pone de manifiesto las consecuencias que pueden resultar de cometer un error. Podemos ver que si cometemos error tipo I, podríamos tomar la decisión de seguir con el programa de control, pero como no es efectivo, resultaría en dinero gastado y posiblemente los casos de envenenamiento con DTT seguirían aumentando. Si cometemos error tipo II, resultaría en la eliminación de un programa que ha sido exitoso en manejar las cantidades de DDT en el ambiente. Notamos que la equivocación por error tipo I es mucho más grave dado que resulta en consecuencias directas a la salud, mientras que la eliminación del programa por error tipo II solo resultaría en la pérdida del dinero invertido. En este caso, al diseñar el experimento, el investigador debe estar más interesado en controlar el error tipo I y mantenerlo a un valor de \\(\\alpha\\), y no preocuparse tanto por la probabilidad cometer error tipo II. Aunque en teoría esto es cierto, la probabilidad de cometer alguno de los dos errores están relacionados, de tal forma que al disminuir la probabilidad de cometer uno, aumenta la probabilidad de cometer el otro, como se muestra en figura 12.3. Figure 12.3: Relación entre la probabilidad de cometer error tipo I y error tipo II. Estos errores se controlan al momento de diseñar el experimento (no se elige el valor de \\(\\alpha\\) luego de recolectados los datos, sino mucho antes). Dado que, en general, las consecuencias de cometer error tipo I son mucho más graves que las de cometer error tipo II, lo que se suele hacer es prestablecer un valor de \\(\\alpha\\) pequeño, y se diseña un experimento calculando el tamaño que debe tener la muestra para un valor de \\(\\beta\\) dado, manteniendo el \\(\\alpha\\) preestablecido. Más adelante veremos cómo realizar estos cálculos, pero primero veamos unos ejemplos. 12.3.2 Pruebas unilaterales. Las condiciones de los conjuntos asociados a cada hipótesis pueden ser más precisos, dando lugar a pruebas unilaterales. Por ejemplo: \\[ \\begin{aligned} H_0: &amp; D \\le 0 \\\\ H_1: &amp; D &gt; 0 \\end{aligned} \\] "],["otros-ejemplos..html", "12.4 Otros ejemplos.", " 12.4 Otros ejemplos. Una pequeña empresa busca saber si existe alguna preferencia en la elección entre dos posibles variaciones de una bebida probiótica que se quiere lanzar al mercado. Los investigadores piensan que la primera versión tendría una mejor aceptación que la segunda. Para ello, se seleccionaron al azar 223 individuos, de los cuales 118 eran hombres y el resto mujeres. Los resultados recolectados muestran que de los varones, 68 de estos prefirieron una bebida sobre la otra, mientras que 79 de las mujeres prefirieron esa misma bebida sobre la otra. ¿Prueban los datos recolectados que existe una preferencia en la elección de una bebida sobre la otra? Basado en su respuesta, ¿qué decisiones tomaría sobre las bebidas probióticas probadas? Solución. Si hubiese alguna preferencia por una bebida u otra, entonces uno esperaría que la proporción de personas que prefieren la bebida uno serían más de la mitad de los encuestados (esta es nuestra hipótesis experimental). Para expresar esto formalmente, definimos primero la proporción de personas que eligieron la primera bebida como \\(\\pi\\), cuyo estimador es \\(p = (68 + 79) / 223 = 147 / 223 = 0{,}659\\). Esto nos permite expresar la hipótesis experimental como \\(\\pi &gt; 0{,}5\\), y lo que se busca probar es el contraste unilateral a la derecha: \\[\\begin{aligned} H_0: &amp; \\pi \\le 0{,}5 \\\\ H_1: &amp; \\pi &gt; 0{,}5 \\end{aligned}\\] Construimos ahora un estadístico para probar el contraste, el cual mide que tanto se desvía el valor estimado del valor que según la hipótesis nula tomamos como cierto: \\[\\begin{aligned} \\hat{Z} &amp;= \\frac{p - \\pi}{\\sqrt{\\pi(1-\\pi)/n}} \\\\ &amp;= \\frac{0{,}659 - 0{,}5}{\\sqrt{0{,}5(1-0{,}5)/223}} \\\\ &amp;= 4.75 \\end{aligned}\\] Por el TLC, sabemos que el estadístico se distribuye como una normal estándar. Si usamos un valor de \\(\\alpha\\) de \\(0{,}05\\), entonces, como el contraste es unilateral, el valor crítico es \\(z_\\alpha=1{,}64\\) y por lo tanto, dado que \\(\\hat{Z}\\) es mucho mayor, decidimos rechazar la hipótesis nula en favor de la alternativa. De hecho, la probabilidad asociada es \\(P(Z \\ge \\hat{Z}) = 0{,}000001\\), la cual es muy baja para que sea una desviación esperada por azar. Esto nos lleva a concluir que de hecho hay una preferencia por la primera bebida por parte de los encuestados. En el ejemplo anterior, notamos que el contraste se establece en terminos del parámetro \\(\\pi\\), que es una proporción que puede tomar valores en el intervalo \\([0,1]\\). Aunque definimos las hipótesis usando una notación más breve para los conjuntos \\(\\Theta_0\\) y \\(\\Theta_1\\), no debe olvidar que la elección del parámetro sobre el que se hace inferencia define el conjunto de posibles hipótesis como \\(\\Theta = \\{\\pi \\in [0,1]\\}\\), de forma que el contraste es: \\[\\begin{aligned} H_0: &amp; \\Theta_0 = \\{\\pi \\in [0,1] \\vert \\pi \\le 0{,}5 \\} \\\\ H_1: &amp; \\Theta_1 = \\{\\pi \\in [0,1] \\vert \\pi &gt; 0{,}5 \\} \\end{aligned}\\] No olvide que las hipótesis son conjuntos mutuamente excluyentes que definen los posibles valores que puede tomar el parámetro según la conjetura inicial. Otra cosa que notar es que, para el cálculo de \\(SE(\\pi)\\) usamos el valor de \\(\\pi\\) y no la proporción \\(p\\). Esto es así porque suponemos que la hipótesis nula es cierta, y como conocemos el valor de \\(\\pi\\), podemos calcular basado en este valor. Siguiendo con el ejemplo anterior, nos damos cuenta que el estudio esta segmentado de acuerdo al sexo, por lo que los investigadores podrían estar interesados en saber si la preferencia observada es igual en ambos hombres y muejeres, o si solo es debido a un sexo. Por lo que debemos probar dos contrastes más, similares al anterior; pero antes debemos aclarar la notación. Definimos primero la proporción poblacional de machos y hembras como \\(\\pi_M\\) y \\(\\pi_H\\), respectivamente, los cuales se estiman por \\(p_M\\) y \\(p_H\\). Los contrastes son iguales al anterior, escribimos: \\[\\begin{aligned} H_0: &amp; \\pi_j \\le 0{,}5 \\\\ H_1: &amp; \\pi_j &gt; 0{,}5 \\end{aligned}\\] donde \\(j = M, H\\). Construimos estadísticos similares al anterior para probar los contrastes, obteniendo para los hombres: \\[\\hat{Z}_M = \\frac{0.576 - 0{,}5}{\\sqrt{0{,}5(1-0{,}5)/118}} = 1.66\\] y para las mujeres: \\[\\hat{Z}_H = \\frac{0.752 - 0{,}5}{\\sqrt{0{,}5(1-0{,}5)/118}} = 5.17\\] Estos dos valores son mayores que el valor crítico de \\(1{,}64\\), aunque el valor calculado para la muestra de hombres es marginal. La probabilidad asociada a estos estadísticos son: \\[P(Z \\ge 1.66) = 0{,}0488\\] y \\[P(Z \\ge 5.17) = 0{,}0000001\\] Con estos resultados, según los criterios de decisión dados, rechazamos la hipótesis nula en ambos casos y concluimos que hay una preferencia por la primera bebida en hombres y mujeres. Sin embargo, la evidencia parece ser más confiable en la muestra de mujeres encuestadas que en la de varones. Al final del ejercicio anterior, rechazamos la hipótesis nula en ambas muestras de varones y mujeres. Si se nos pidiera dar una opinión sobre la decisión de venta de las bebidas, basado solo en los resultados de esa encuesta, podríamos decidir vender la bebida siguiendo una estrategia de ventas dirigida a las mujeres para la primera bebida probiótica. Se sabe que los topos de cierta especie salen a la superficie para alimentarse de insectos que se encuentran en la superficie, exponiéndose a aves y mamíferos cazadores que se alimentan de estos. Los topos, como estrategia de supervivencia, se exponen a la superficie por un tiempo y luego vuelven a sus hoyos para moverse a uno distinto donde repiten el comportamiento, buscando alimento. Ciertas praderas se han convertido en zonas de estudio de las comunidades de estas especies de topo. Dos zonas atraen mayor atención debido a que una de las zonas, la de más al sur, es bastante heterogénea espacialmente, con irregularidades y cambios en la planicie que resultan en una gran cantidad de sitios para que los topos puedan protegerse de los depredadores. La otra zona es más regular con menos sitios que sirvan de escondite. Se han observado los topos y se han calculado los tiempos de exposición de los topos en la superficie. En la pradera del sur, se obtuvo un tiempo de \\(33 \\pm 8{,}3\\) segundos basado en la observación de \\(14\\) topos; mientras que en la otra pradera, la más homogénea, la observación de \\(12\\) topos mostró un tiempo de exposición de \\(29 \\pm 4{,}1\\) segundos. Los investigadores buscan saber si hay una diferencia en los tiempos de exposición de los topos en ambas zonas. Solución. Un cálculo rápido de un intervalo de confianza del 95% para la diferencia en el tiempo promedio de exposición entre ambas zonas se muestra a continuación: \\[-0.12 &lt; \\mu_{S} - \\mu_{N} &lt; 8.12\\] Notamos claramente que no hay una diferencia significativa en el tiempo promedio de exposición de ambas zonas, aun cuando en la zona Sur es más largo. Los investigadores, esperando este resultado, conjeturan que debido a la mayor heterogeneidad en el nicho del sur, el tiempo de exposición sería más variable que en la otra región, dada la menor presión que imponen los depredadores en zonas de mayor dificultad de encuentro de presas. Se decide entonces realizar un contraste para verificar esta hipótesis. Se desea saber entonces si la varianza en el tiempo de reacción en la zona sur es mayor que en la zona de más al norte, y los contrastes de interés son: \\[\\begin{aligned} H_0: &amp; \\sigma^2_S / \\sigma^2_N \\le 1 \\\\ H_1: &amp; \\sigma^2_S / \\sigma^2_N &gt; 1 \\end{aligned}\\] Como ya vimos antes, con respecto a las varianzas, podemos construir estadísticos basado en la proporción entre las varianza de ambas poblaciones: \\[\\begin{aligned} \\hat{F} &amp;= \\frac{\\sigma^2_N S^2_S}{\\sigma^2_S S^2_N} \\\\ &amp;= \\frac{(8{,}3)^2\\sigma^2_N}{(4{,}1)^2\\sigma^2_S } \\\\ &amp;= 4{,}098 \\frac{\\sigma^2_N}{\\sigma^2_S} \\end{aligned}\\] Bajo el supuesto de que la hipótesis nula es cierta, el cociente \\(\\sigma^2_N / \\sigma^2_S = 1\\), por lo que el valor estimado del estadístico es \\(\\hat{F} = 4{,}098\\), el cual es un valor observado de la distribución \\(F\\) con \\(n_S - 1 = 13\\) y \\(n_N - 1 = 11\\) grados de libertad. El valor crítico para esta distribución, para un nivel de significancia de \\(0{,}05\\), es de $F_{crítico} = 2.76 (la prueba es unilateral asi que solo escogemos el cuantil \\(F_{0{,}05}\\)). Notamos que el valor observado cae en la región crítica, ya que \\(\\hat{F} &gt; F_{crítico}\\) y por lo tanto, debemos rechazar la hipótesis nula en favor de la alternativa. De hecho, \\(P(F \\ge \\hat{F}) = 0.0125\\), la cual es bastante baja, y concluimos que una proporción de varianza de aproximadamente \\(4\\) es demasiado grande como para haber ocurrido solo por azar. Se concluye entonces que el tiempo de exposición de los topos en la zona sur es más variable, lo cual tiene sentido dado que la topografía más heterogénea hace que en ciertos lugares los topos puedan estar más tiempo expuestos por estar más protegidos. El ejemplo anterior sirve para ver que en ciertas ocasiones, aún si los valores pormedios de una variable aleatoria no son distintos, las poblaciones pueden ser distintas en términos de la variabilidad de estas. Veamos un ejemplo para un diseño experimental de dos muestras independientes, en la cual se mide una variable aleatoria continua. "],["ejercicios.-2.html", "12.5 Ejercicios.", " 12.5 Ejercicios. Se midieron los tamaños de \\(30\\) cráneos de gorilas hembras y \\(29\\) machos (datos de O’Higgins, 1989). La varianza de las hembras es \\(39{,}7\\), mientras que la varianza de los machos es \\(105{,}9\\). ¿Son las varianzas de los tamaños de cráneos de machos y hembras iguales? Bruton y Owen (1988) midieron varias muestras de trilobites ilénidos del Ordovícico de Noruega y Suecia. Veremos la longitud del cefalón, medida en \\(43\\) especímenes de Stenopareia glaber de Noruega y \\(17\\) especímenes de S. linnarssoni de Noruega y Suecia. Los valores medios son \\(16{,}8\\) (varianza \\(15{,}2\\)) y \\(25{,}8\\) (varianza \\(64{,}3\\)) respectivamente, lo que indica que S. glaber es más pequeña que S. linnarssoni. ¿La diferencia es estadísticamente significativa o podría ser solo un efecto de muestreo aleatorio? Se investiga sobre la distancia máxima a la que es audible una llamada de alerta en ardillas. Se cree que la media de las distancias máximas es de más de 87 metros. ¿Apoyan el argumento los datos recolectados de una muestra aleatoria? Explicar la respuesta basándose en el valor P del contraste. los datos, en metros, son los siguientes: \\(90{,}8\\); \\(79{,}4\\); \\(94{,}4\\); \\(96{,}7\\); \\(85{,}2\\); \\(89{,}7\\); \\(82{,}0\\); \\(88{,}2\\); \\(91{,}9\\); \\(94{,}3\\); \\(95{,}1\\); \\(84{,}5\\); \\(90{,}8\\); \\(79{,}4\\); \\(96{,}7\\); \\(96{,}7\\); \\(85{,}2\\); \\(89{,}7\\); \\(82{,}0\\); \\(88{,}2\\); \\(91{,}9\\); \\(94{,}3\\); \\(95{,}1\\); \\(84{,}5\\); \\(88{,}6\\); \\(95{,}6\\); \\(89{,}4\\); \\(87{,}3\\); \\(98{,}5\\); \\(87{,}1\\); \\(82{,}1\\); \\(86{,}7\\); \\(98{,}5\\); \\(87{,}1\\); \\(82{,}1\\); \\(86{,}7\\); \\(88{,}6\\); \\(95{,}6\\); \\(89{,}4;\\) y \\(87{,}3\\). Un programa de entrenamiento busca evaluar el desempeño físico de una muestra aleatoria de individuos sometidos al programa. Para ello colocan bajo observación a 10 hombres, para ser sometidos a un programa típico de entrenamiento militar. Se anotaron sus pesos antes y después de dicho entrenamiento, mostrados a continuación (el primer dato de antes y después corresponde al primer individuo, el segundo dato de antes y después al segundo individuo, y así sucesivamente): Antes(g): \\(45,110,92,57,59,110,82,86,104,49\\) Después(g): \\(40,105,75,85,53,113,78,85,107,49\\) Pruebe si el programa de entrenamiento tiene efecto en la disminución del peso. Cualquiera sea su resultado, construya un intervalo de confianza para la diferencia promedio del peso de los individuos luego del programa de entrenamiento. Los bifenilos policlorados (PCB) son contaminantes de origen industrial relacionados con el DDT, los cuales afectan ambientes de todo el planeta. Aunque se eliminan progresivamente, permanecen en el medio por muchos años. Supongamos que Ud. está interesado en estudiar los efectos del PCB en la capacidad reproductiva de lechuzas, midiendo el espesor de la cáscara (en mm) en huevos de aves expuestas al PCB. Al final del estudio se obtuvo la muestra siguiente (en mm): \\(0{,}21; 0{,}185; 0{,}22; 0{,}215; 0{,}21; 0{,}265; 0{,}148; 0{,}136; 0{,}257; 0{,}136\\) y \\(0{,}249\\). Estudios previos describen el grosor normal de los huevos de la lechuza en estudio, con un valor de \\(0{,}226\\) mm. a) Aplique la prueba estadística apropiada y determine si puede demostrarse estadísticamente la hipótesis de daño con un nivel de significación de \\(0{,}05\\). b) Construya un intervalo de confianza del 95% y redacte su conclusión, argumentando cuantitativamente en base al \\(p\\) del contraste y al intervalo. Se desea estudiar si todavía la revolución industrial tiene efecto sobre la sobrevivencia de la polilla del abedul (Biston betularia), ya que sus depredadores las detectan por el contraste de su coloración con respecto a la del árbol donde se posan. Para tal fin se comparan muestras colectadas antes de 1845 (principios de la revolución industrial), preservadas en un museo de la ciudad de Manchester, con muestras en la actualidad provenientes de zonas contaminadas de la misma ciudad. En el primer grupo, de \\(180\\) ejemplares, \\(25\\) son oscuras, mientras que en el segundo, de un total de \\(380\\) polillas, \\(90\\) presentan coloración oscura. ¿Habrá diferencia temporal? Construya un intervalo del \\(99\\)% de confianza para la diferencia. Se realizó un estudio previo a la planificación de un programa de explotación por caza de subsistencia sobre el conejo sabanero Silvilagus floridanus en una población de Panaquire, Edo Miranda. La tasa de extracción debía conservar la proporción natural hembra:macho de la zona y se deseaba saber si la proporción era igual para ambos sexos. Se capturaron \\(350\\) conejos, de los cuales \\(105\\) resultaron ser hembras. a) Realice una prueba para evaluar la veracidad de la propuesta de los manejadores de fauna. b) Construya un intervalo de confianza del \\(95\\)% para la proporción poblacional. c) Usando argumentos cuantitativos (\\(p\\)-valor; valores de la estimación por intervalos), explique sus conclusiones. Cinco años después el grupo es contratado por el estado para aplicar el mismo programa de explotación del conejo de monte en Boca de Paguey, al sur-este de Panaquire. Se colectaron \\(248\\) conejos, de los cuales \\(120\\) fueron machos. d) Realice una prueba para comparar ambos grupos de datos y explique sus conclusiones. e) independientemente de sus resultados, construya e interprete un intervalo de confianza del 95% para la diferencia de las proporciones poblacionales. f) Explique si considera Ud que puede establecerse la misma tasa de extracción de hembras y machos fijada para la primera localidad hace 5 años. Un investigador busca poder inducir la formación de callos a partir de semillas de moringa para la obtención de metabolitos secundarios de interés farmacéutico. El trabajo para poder obtener un primer biorreactor piloto de callos es largo y requiere de la caracterización del proceso de inducción. Uno de los pasos requiere el contabilizar el número de aberraciones cromosómicas por célula que aparecen como consecuencia del tratamiento con los factores de crecimiento (FC) usados. Luego de inducir la callogénesis usando dos concentraciones de FC (una alta y otra baja), se obtuvieron los siguientes resultados: Control: \\(4,10,4,10,3,6,3,5,5\\). FC Baja: \\(3,14,12,6,9,7,9,10,6,4,11\\). FC Alta: \\(12,18,11,11,11,11,18,8,12,16\\). Antes (en los ejercicios del capítulo anterior), se le pidió construir intervalos de confianza para verificar diferencias entre cada una de las muestras tratadas con FC con respecto al control. Ahora, usted debe realizar un contraste de hipótesis para verificar si existen diferencias entre los tratamientos con FC y concluya sobre los resultados. Sea cual sea el resultado, realice un contraste entre la tasa promedio de aberraciones por célula que hay entre las muestras tratadas con FC (esto es, uniendo las muestras con FC baja y alta) y el control. ¿Qué puede decir sobre el proceso de inducción y las aberraciones cromosómicas encontradas en los tratamientos? "],["análisis-de-datos-categóricos..html", "13 Análisis de datos categóricos. ", " 13 Análisis de datos categóricos. "],["elección-de-una-prueba-estadística..html", "13.1 Elección de una prueba estadística.", " 13.1 Elección de una prueba estadística. 13.1.1 Pruebas paramétricas y no paramétricas. Las pruebas paramétricas definen claramente la escala de medida de la variable aleatoria, asi como también hacen suposiciones sobre la distribución poblacional y los parámetros de la misma (generalmente, una distribución normal). Las pruebas no paramétricas son aquellas que hacen suposiciones menos restrictivas sobre la escala sobre la cual es posible medir la variable aleatoria, y sobre la distribución poblacional, de la cual “no se asume nada sobre los parámetros”. La elección de la prueba adecuada es aquella que use mejor la información contenida en la muestra. Pero el desconocimiento del fenómeno o la distribución subyacente tiene un papel importante en la decisión de peso al elegir entre información y creencia. 13.1.2 Elección de la prueba estadistica. La elección de una prueba no viene determinada solo por la potencia de la misma: La manera como se obtuvo la muestra, la naturaleza de la población de la que se obtuvo la muestra, las hipotesis a probar, y el tipo de medición o escala que se empleó para la variable implicada. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
